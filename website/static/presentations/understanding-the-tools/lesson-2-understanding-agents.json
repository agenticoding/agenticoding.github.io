{
  "metadata": {
    "title": "Understanding Agents",
    "lessonId": "lesson-2-understanding-agents",
    "estimatedDuration": "35-45 minutes",
    "learningObjectives": [
      "Understand agent feedback loops",
      "Recognize agents as text-based systems",
      "Master context engineering principles",
      "Apply stateless design advantages"
    ]
  },
  "slides": [
    {
      "type": "title",
      "title": "Understanding Agents",
      "subtitle": "From token prediction to autonomous execution",
      "content": [],
      "speakerNotes": {
        "talkingPoints": "Welcome to Lesson 2. We've established that LLMs are brains (token prediction) and agent frameworks are bodies (execution). Now we're diving into how they work together. This lesson will fundamentally change how you think about AI-assisted coding—moving from chat interface thinking to agent-based thinking.",
        "timing": "1 minute",
        "discussion": "Before we start, think: what's the difference between asking ChatGPT for code and having an agent implement a feature? Hold that thought.",
        "context": "This lesson is the conceptual foundation for everything that follows. Understanding how agents think and operate is critical for effective prompting.",
        "transition": "Let's start with the core loop that drives all agent behavior."
      }
    },
    {
      "type": "concept",
      "title": "The Agent Execution Loop",
      "content": [
        "Perceive: Agent observes current state",
        "Reason: LLM predicts next action",
        "Act: Agent executes tools (Read, Edit, Bash)",
        "Observe: Results return to context",
        "Iterate: Loop continues until goal reached"
      ],
      "speakerNotes": {
        "talkingPoints": "An agent isn't just an LLM. It's a feedback loop combining reasoning with action. Each cycle drives the agent closer to the goal. The key difference from chat: the agent closes the loop autonomously without waiting for you.",
        "timing": "3-4 minutes",
        "discussion": "Have you manually executed steps between AI prompts? That's what agents automate. Ask: what happens at each step? Why does order matter?",
        "context": "A chat interface breaks this loop—you're the manual actuator between steps. An agent maintains the loop itself.",
        "transition": "Let's see this loop in action with a concrete example."
      }
    },
    {
      "type": "comparison",
      "title": "Chat Interface vs Agent Workflow",
      "left": {
        "label": "Chat Interface",
        "content": [
          "You ask: 'How add auth?'",
          "LLM responds with code",
          "You manually edit files",
          "You ask: 'Got an error...'",
          "You manually execute again"
        ]
      },
      "right": {
        "label": "Agent Workflow",
        "content": [
          "You specify: 'Add auth to API'",
          "Agent reads existing patterns",
          "Agent edits files automatically",
          "Agent runs tests automatically",
          "Agent fixes errors automatically"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "The chat interface requires manual intervention at each step. You're the execution layer. With agents, the entire loop runs autonomously. You provide the goal; the agent handles perceive-reason-act-observe-iterate without waiting for you.",
        "timing": "2-3 minutes",
        "discussion": "Which approach scales better? Which has more context consistency? Why can agents do more work in less time?",
        "context": "In production, agents reduce iteration cycles from 5+ manual steps to 1-2 autonomous runs. You can parallelize: open multiple terminals, start multiple agents on different tasks.",
        "transition": "Now here's the critical insight: all of this is happening at the text level."
      }
    },
    {
      "type": "concept",
      "title": "It's All Just Text",
      "content": [
        "No hidden state or internal 'thinking'",
        "System prompts, task, tools, results—all text",
        "Context window is the agent's entire world",
        "LLM is completely stateless",
        "Each token prediction uses only current context"
      ],
      "speakerNotes": {
        "talkingPoints": "This is the demystifying moment. There's no magic. No separate reasoning engine. No hidden state. When an agent 'reasons,' it's generating text that appears in the context. That text becomes input to the next prediction. Everything—absolutely everything—is text flowing through a context window.",
        "timing": "2-3 minutes",
        "discussion": "What does 'stateless' mean here? Why can't the LLM remember previous conversations? How does that change your strategy?",
        "context": "Extended thinking modes complicate this slightly—the model generates hidden reasoning tokens before visible output. But the principle remains: it's all computation on text. The visible reasoning you see is a summary, not the full chain-of-thought.",
        "transition": "Let's look at what actually flows through that context window."
      }
    },
    {
      "type": "codeExecution",
      "title": "Textual Flow Through Context",
      "steps": [
        {
          "line": "SYSTEM: You are Claude, an AI assistant...",
          "highlightType": "human",
          "annotation": "System instructions define the agent's role and constraints"
        },
        {
          "line": "USER: Add email validation to the registration\nendpoint",
          "highlightType": "human",
          "annotation": "Your task specification enters the context"
        },
        {
          "line": "ASSISTANT: I'll start by reading the registration\ncode to understand the current structure.",
          "highlightType": "prediction",
          "annotation": "LLM predicts what to do next"
        },
        {
          "line": "Agent executes: Read(src/auth/register.ts)",
          "highlightType": "execution",
          "annotation": "Tool call becomes visible text in context"
        },
        {
          "line": "TOOL_RESULT: export async function register(email,\npassword) { ... 42 lines of code ...",
          "highlightType": "feedback",
          "annotation": "File content returns as text; context grows"
        },
        {
          "line": "ASSISTANT: I see the registration lacks email\nvalidation. I'll check for existing validators first.",
          "highlightType": "prediction",
          "annotation": "LLM analyzes and predicts next step"
        },
        {
          "line": "Agent executes: Glob(src/**/*validator*)",
          "highlightType": "execution",
          "annotation": "Search tool adds more text to context"
        },
        {
          "line": "TOOL_RESULT: Found: src/validators/email.ts",
          "highlightType": "feedback",
          "annotation": "Search results become context input"
        },
        {
          "line": "ASSISTANT: I'll integrate the existing email\nvalidator into the registration flow.",
          "highlightType": "prediction",
          "annotation": "LLM synthesizes plan from available context"
        },
        {
          "line": "Agent executes: Edit(src/auth/register.ts, old,\nnew)",
          "highlightType": "execution",
          "annotation": "Code modification as text-based operation"
        }
      ],
      "speakerNotes": {
        "talkingPoints": "This is the full picture. Every interaction—your task, the agent's reasoning, tool calls, results—exists as continuous text in the context window. The agent doesn't have secret thoughts. When you see it 'reason,' that reasoning is text being generated token-by-token. The next prediction consumes that text. This is the foundation: you're orchestrating text flows, not commanding a system with hidden state.",
        "timing": "4-5 minutes",
        "discussion": "Why does this matter? What happens if the context gets too long? What can't the agent see if it's scrolled out? How do you design prompts knowing this?",
        "context": "In complex tasks, context management becomes critical. Tool results stack up. Old context scrolls out. The agent loses context about early decisions. This is why grounding and active context management matter.",
        "transition": "Let's look at a concrete example of what a full context actually contains."
      }
    },
    {
      "type": "code",
      "title": "What the Context Actually Contains",
      "language": "text",
      "code": "SYSTEM: You are Claude, expert at secure authentication.\nYour task: review this code critically.\n\nUSER: Add email validation to registration.\nHandle: duplicate emails, invalid formats,\ndisposable addresses.\n\nASSISTANT: I'll read the current registration code.\n\nTOOL: Read(src/auth/register.ts)\nRESULT: [42 lines of registration logic]\n\nASSISTANT: Critical issue: no email validation.\nI'll integrate validators.\n\nTOOL: Read(src/validators/email.ts)\nRESULT: [email validation code]\n\nTOOL: Edit(src/auth/register.ts, ...)\nRESULT: Applied changes\n\nTOOL: Bash(npm test -- auth.test.ts)\nRESULT: 12 tests passing\n\nASSISTANT: Email validation integrated. All\ntests passing.",
      "caption": "Single continuous text stream: instructions, task, reasoning, tool calls, results, feedback",
      "speakerNotes": {
        "talkingPoints": "This is the complete picture. System instructions, your task, the agent's responses, tool calls, results—all exist as one continuous text stream. There's no separation. The agent doesn't 'see' context and 'hidden reasoning' separately. It's all one buffer. This changes everything about how you work with agents.",
        "timing": "3 minutes",
        "discussion": "If all of this is text, what happens if your initial task description is vague? What happens if tool results are verbose and overflow the context? What information does the agent lose?",
        "context": "This is why quality matters at every level. Vague prompts produce vague reasoning. Verbose tool output wastes tokens. The context window is finite. Every interaction shapes what the agent can see and reason about next.",
        "transition": "Understanding this textual nature reveals a powerful advantage: the agent's statelessness."
      }
    },
    {
      "type": "concept",
      "title": "The Stateless Advantage",
      "content": [
        "LLM has no memory between conversations",
        "Each response uses only current context",
        "Clean-slate exploration: evaluate options unbiased",
        "Unbiased code review: audit with no defensive bias",
        "Context engineering controls agent behavior"
      ],
      "speakerNotes": {
        "talkingPoints": "The LLM is completely stateless. It doesn't 'remember' previous conversations. It has no hidden internal state. This sounds like a limitation—it's actually a massive advantage. You control behavior by controlling context. Want JWT authentication reviewed? One context. Want sessions reviewed? Different context. Same code gets different analysis because the agent isn't defending earlier choices. This is extraordinarily powerful.",
        "timing": "3-4 minutes",
        "discussion": "How is this different from a traditional system? If the agent can't remember, how do you build complex workflows? What opportunities does this open?",
        "context": "In production, statelessness means you can architect Generate → Review → Iterate workflows. The agent writes code in one context, then reviews it unbiasedly in another. Or run multi-perspective analysis: security review, performance review, maintainability review, each in separate contexts with focused instructions.",
        "transition": "This brings us to the visual concept of how context isolation and fresh perspectives enable unbiased analysis."
      }
    },
    {
      "type": "visual",
      "component": "AbstractShapesVisualization",
      "caption": "Stateless context isolation enables objective code review",
      "speakerNotes": {
        "talkingPoints": "This visualization shows the power of context isolation. The same code that receives 'looks good overall' in one context triggers critical security findings in a fresh context. Why? Because the agent isn't defending earlier decisions. It's analyzing from scratch. This is how you build objective code review into your workflows.",
        "timing": "2 minutes",
        "discussion": "How does this change code review? What happens if you review your own code versus having someone else review it? How does an agent differ?",
        "context": "Practical application: When you implement a feature and want objective feedback, don't ask the same agent context. Start fresh. Give the fresh context the code plus a specific review prompt: 'Audit this for security vulnerabilities.' The agent will find issues the generating context missed.",
        "transition": "Now let's look at the tools that enable agents to interact with the world."
      }
    },
    {
      "type": "concept",
      "title": "Tools: The Agent's Interface to Reality",
      "content": [
        "Built-in tools: Read, Edit, Bash, Grep, Write, Glob",
        "Engineered for LLM safety and efficiency",
        "External tools via MCP (Model Context Protocol)",
        "Tools expand agent capabilities: databases, APIs, cloud",
        "Discoveries happen at runtime, not configuration"
      ],
      "speakerNotes": {
        "talkingPoints": "Agents interact with the world through tools. Built-in tools (Read, Edit, Bash, Grep) are optimized for coding tasks with safety guardrails and LLM-friendly output. External tools connect to external systems via MCP—a standardized plugin protocol. When you configure an MCP server, the agent discovers its tools at runtime.",
        "timing": "2-3 minutes",
        "discussion": "Why not just let the agent run arbitrary shell commands? What safety issues come up? Why is MCP better than embedding credentials in prompts?",
        "context": "Built-in tools aren't simple shell wrappers. Grep returns results formatted for LLM readability. Read truncates large files intelligently. Bash captures output safely. These designs matter—they determine how well the agent can reason about results.",
        "transition": "Let's see how MCP configuration works in practice."
      }
    },
    {
      "type": "code",
      "title": "MCP: Connecting External Systems",
      "language": "json",
      "code": "{\n  \"mcpServers\": {\n    \"postgres\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"mcp_postgres\"],\n      \"env\": {\"DB_URL\":\n        \"postgresql://user:pass@localhost/db\"\n      }\n    },\n    \"github\": {\n      \"command\": \"node\",\n      \"args\": [\"path/to/mcp-github.js\"],\n      \"env\": {\"GITHUB_TOKEN\":\n        \"${secrets.github_token}\"\n      }\n    }\n  }\n}",
      "caption": "MCP servers expose tools at runtime; agent discovers capabilities dynamically",
      "speakerNotes": {
        "talkingPoints": "This configuration file connects your agent to external systems. The agent discovers available tools when it starts. It can then use 'query-database' or 'create-github-issue' naturally, as if they were built-in tools. MCP handles the protocol complexity. You focus on tasks.",
        "timing": "2 minutes",
        "discussion": "What systems would you want your agent to access? Why is discovering tools at runtime safer than hardcoding credentials?",
        "context": "MCP is the standard for extending agents. The ecosystem is growing—PostgreSQL, MongoDB, Stripe, Figma, AWS, GCP, Azure connectors exist. You can write custom MCP servers for internal systems.",
        "transition": "Now let's compare different agent deployment models."
      }
    },
    {
      "type": "comparison",
      "title": "Agent Deployment Models: Advantages",
      "left": {
        "label": "Chat & IDE Agents",
        "content": [
          "Single-window UI coupling",
          "Context resets per conversation",
          "Manual copy-paste workflows",
          "Blocked until completion",
          "Limited parallelism"
        ]
      },
      "right": {
        "label": "CLI Agents",
        "content": [
          "Terminal-based independence",
          "Persistent context per project",
          "Native file operations",
          "Non-blocking execution",
          "Multi-terminal parallelism"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "CLI agents deliver superior developer experience for implementation work. Why? They unlock parallelism. Open three terminals, run agents on three projects simultaneously. Each agent works independently. Context-switch freely. Chat interfaces and IDE agents are tightly coupled to single windows. You're blocked until one agent completes.",
        "timing": "3 minutes",
        "discussion": "Have you used chat interfaces for coding? What frustrated you? How does CLI solve that? When would you still use chat?",
        "context": "Chat excels at answering questions and brainstorming. Agents excel at executing tasks. The combination is powerful: use chat for exploration and learning, agents for implementation.",
        "transition": "This brings us to the core principle underlying all of this: context engineering."
      }
    },
    {
      "type": "concept",
      "title": "Context Engineering: The Core Principle",
      "content": [
        "Context is the agent's entire world",
        "Engineer context to steer behavior upfront",
        "Scope tasks: focused context → focused work",
        "Adjust dynamically: steer mid-conversation",
        "Objectivity: fresh context for unbiased review"
      ],
      "speakerNotes": {
        "talkingPoints": "This is the through-line of the entire course. Effective AI-assisted coding is about engineering context. The agent only knows what's in its context window. You control what's in the context: system prompts, task descriptions, which files you show it, what results you include. Vague context produces wandering behavior. Precise, scoped context steers the agent exactly where you need it. This is system design thinking applied to text.",
        "timing": "3-4 minutes",
        "discussion": "How is context engineering like API design? What's the contract between you and the agent? How do you make that contract clear?",
        "context": "The stateless nature enables this. You're not fighting with hidden state or previous biases. You engineer a fresh context and the agent operates within it. This is powerful beyond chat interfaces because context isn't lost between turns.",
        "transition": "Let's summarize what you've learned and how it applies to the rest of this course."
      }
    },
    {
      "type": "takeaway",
      "title": "Key Takeaways",
      "content": [
        "Agents close feedback loops autonomously using tools",
        "Everything is text: prompts, reasoning, results, context",
        "Stateless design enables objective analysis and iteration",
        "Context engineering controls agent behavior—that's your leverage"
      ],
      "speakerNotes": {
        "talkingPoints": "You now understand agents at a fundamental level. They're not magical. They're text-based systems that combine token prediction with tool execution in feedback loops. The context window is everything. That's where your power lies—engineering that context. The rest of this course teaches how to apply this understanding to real scenarios.",
        "timing": "2 minutes",
        "discussion": "What surprised you about how agents work? What changes in your thinking now that you know it's all text? What's the first thing you want to try?",
        "context": "Next lesson, we move to methodology. We'll establish a production-ready framework for working with agents. You'll learn to design tasks that agents can execute reliably.",
        "transition": "Let's move to Lesson 3: High-Level Methodology, where we establish the framework for everything that follows."
      }
    }
  ]
}
