{
  "metadata": {
    "title": "Understanding Agents",
    "lessonId": "lesson-2-understanding-agents",
    "estimatedDuration": "35-45 minutes",
    "learningObjectives": [
      "Understand agent execution loops",
      "Recognize context as text",
      "Leverage stateless LLM advantages",
      "Engineer context to steer"
    ]
  },
  "slides": [
    {
      "type": "title",
      "title": "Understanding Agents",
      "subtitle": "How LLMs and Agent Frameworks Work Together",
      "content": [],
      "speakerNotes": {
        "talkingPoints": "In Lesson 1, we established that LLMs are brains (token prediction engines) and agent frameworks are bodies (execution layers). Today we'll understand how these components work together to create autonomous coding agents that can complete complex tasks. We'll demystify what actually happens under the hood - it's all just text flowing through a context window.",
        "timing": "1 minute",
        "discussion": "Quick check: Who has used an AI coding agent before? What was your experience with how it worked?",
        "context": "This lesson provides the mental model students need to understand everything else in the course. Without this foundation, agent behavior seems magical and unpredictable.",
        "transition": "Let's start by understanding the core execution loop that makes agents different from chat interfaces..."
      }
    },
    {
      "type": "concept",
      "title": "The Agent Execution Loop",
      "content": [
        "Perceive → Read context (files, state, history)",
        "Reason → LLM generates plan and next action",
        "Act → Execute tool (Read, Edit, Bash, etc.)",
        "Observe → Process tool output",
        "Verify → Goal achieved? If no, iterate"
      ],
      "speakerNotes": {
        "talkingPoints": "An agent isn't just an LLM responding to prompts. It's a feedback loop that combines reasoning with action, allowing the LLM to iteratively work toward a goal. The key distinction: a chat interface requires you to manually execute actions between prompts. An agent autonomously loops through this cycle.",
        "timing": "3 minutes",
        "discussion": "Think about the last time you used ChatGPT for coding. How many times did you have to manually copy code, run it, copy errors back? That manual loop is what agents automate.",
        "context": "This loop structure is fundamental to all modern agent frameworks - whether it's Claude Code, AutoGPT, or custom implementations. The specific tools and verification strategies differ, but the perceive-reason-act-observe cycle is universal.",
        "transition": "Let's see this in action with a concrete example..."
      }
    },
    {
      "type": "comparison",
      "title": "Chat Interface vs Agent Workflow",
      "left": {
        "label": "Chat Interface",
        "content": [
          "You: 'How should I add auth to this API?'",
          "LLM: 'Here's the code...'",
          "You manually edit files",
          "You: 'I got this error...'",
          "You manually edit again"
        ]
      },
      "right": {
        "label": "Agent Workflow",
        "content": [
          "You: 'Add auth to this API'",
          "Agent reads API files automatically",
          "Agent edits files automatically",
          "Agent runs tests, analyzes errors",
          "Agent fixes code until tests pass"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "The chat interface forces you into the loop manually at every step. The agent closes the loop automatically, executing the full perceive-reason-act-verify cycle without requiring manual intervention. This is why agents can complete complex multi-step tasks while chat interfaces require constant human participation.",
        "timing": "3-4 minutes",
        "discussion": "When would you prefer a chat interface over an agent? When would the agent be clearly superior? Consider scenarios like brainstorming vs implementation.",
        "context": "In production environments, agents shine for implementation tasks with clear success criteria (tests passing, builds succeeding, specific functionality working). Chat interfaces remain better for exploratory work, learning new concepts, or brainstorming architecture decisions.",
        "transition": "Now let's demystify what's actually happening under the hood. It's simpler than you might think..."
      }
    },
    {
      "type": "concept",
      "title": "Under the Hood: It's All Just Text",
      "content": [
        "Everything flows through a single context window",
        "No magic, no separate reasoning engine",
        "System prompts, user input, tool calls, results - all text",
        "The agent only knows what's in the context",
        "Extended thinking modes hide reasoning tokens"
      ],
      "speakerNotes": {
        "talkingPoints": "Here's the fundamental truth that demystifies AI coding agents: everything is just text flowing through a context window. When you interact with an agent, you're watching a conversation unfold in a single, large text buffer. The agent doesn't 'think' separately from its output - when you see reasoning like 'I should check the validation logic...', that's not internal thought, it's text being generated in the context. Extended thinking modes from providers like Anthropic and OpenAI complicate this by generating hidden reasoning tokens before visible output, but fundamentally it's still text manipulation.",
        "timing": "3 minutes",
        "discussion": "This might seem anticlimactic - there's no hidden intelligence, just text prediction. But understanding this is crucial: why does knowing it's 'just text' help you use agents more effectively?",
        "context": "This mental model explains agent limitations (context window size), debugging strategies (inspect what's in context), and optimization techniques (minimize token usage). Senior engineers who grasp this can engineer prompts and workflows that non-technical users struggle with.",
        "transition": "Let's look at exactly what this textual flow looks like in a real agent interaction..."
      }
    },
    {
      "type": "codeExecution",
      "title": "Concrete Example: What the Context Contains",
      "steps": [
        {
          "line": "SYSTEM: You are Claude Code, an AI coding assistant. You have access to these tools: Read(file_path), Edit(file_path, old_string, new_string), Bash(command), Grep(pattern, path)...",
          "highlightType": "human",
          "annotation": "System instructions define agent behavior and available tools"
        },
        {
          "line": "USER: Add email validation to the registration endpoint",
          "highlightType": "human",
          "annotation": "User task specification enters context as text"
        },
        {
          "line": "ASSISTANT: I'll search for the registration endpoint first.",
          "highlightType": "prediction",
          "annotation": "LLM predicts next action based on context"
        },
        {
          "line": "TOOL_USE: Grep(\"register\", \"**/*.ts\")",
          "highlightType": "execution",
          "annotation": "Agent framework executes the tool call"
        },
        {
          "line": "TOOL_RESULT: Found in: src/routes/auth.ts:12, src/handlers/user.ts:45",
          "highlightType": "feedback",
          "annotation": "Tool results added to context as text"
        },
        {
          "line": "ASSISTANT: Found the endpoint. Let me read the handler to see current validation.",
          "highlightType": "prediction",
          "annotation": "LLM receives previous results and predicts next step"
        },
        {
          "line": "TOOL_USE: Read(\"src/handlers/user.ts\")",
          "highlightType": "execution",
          "annotation": "Framework executes Read tool"
        },
        {
          "line": "TOOL_RESULT: [file contents showing no email validation]",
          "highlightType": "feedback",
          "annotation": "File contents now part of context"
        },
        {
          "line": "ASSISTANT: No validation present. I'll add email validation using Zod.",
          "highlightType": "prediction",
          "annotation": "LLM analyzes context and decides on implementation"
        },
        {
          "line": "TOOL_USE: Edit(\"src/handlers/user.ts\", old_code, new_code)",
          "highlightType": "execution",
          "annotation": "Framework executes code modification"
        },
        {
          "line": "TOOL_RESULT: Edit successful",
          "highlightType": "feedback",
          "annotation": "Success confirmation added to context"
        },
        {
          "line": "ASSISTANT: Validation added. Running tests to verify.",
          "highlightType": "prediction",
          "annotation": "LLM predicts verification step"
        },
        {
          "line": "TOOL_USE: Bash(\"npm test user.test.ts\")",
          "highlightType": "execution",
          "annotation": "Framework runs test command"
        },
        {
          "line": "TOOL_RESULT: PASS - ✓ rejects invalid email, ✓ rejects short password",
          "highlightType": "feedback",
          "annotation": "Test results confirm success"
        },
        {
          "line": "ASSISTANT: Email validation successfully added to the registration endpoint.",
          "highlightType": "summary",
          "annotation": "Task completion - loop terminates"
        }
      ],
      "speakerNotes": {
        "talkingPoints": "This entire conversation - system instructions, your task, tool calls, results, and responses - exists as one continuous text stream in the LLM's context window. Notice how each tool result becomes input to the next prediction. The agent doesn't have a separate memory or state - everything it knows is literally visible in this text flow. This is pedagogically critical: understanding this demystifies how agents work and why they sometimes forget things (context overflow) or get confused (ambiguous context).",
        "timing": "5-6 minutes - this is the most important conceptual slide",
        "discussion": "Pause here and ask: What happens if this context gets too long? What information might scroll out? How would that affect agent behavior? Let students reason about context window limits.",
        "context": "In production, this textual flow is why agents can lose track of early requirements in long sessions, why starting fresh conversations can yield better results, and why explicit grounding (Lesson 5) is critical. The context window is finite - current models range from 32K to 200K tokens.",
        "transition": "Now that you understand the textual nature, let's explore a crucial insight about LLM statefulness..."
      }
    },
    {
      "type": "concept",
      "title": "The Stateless Advantage",
      "content": [
        "LLM has no memory between conversations",
        "Each response generated solely from current context",
        "This is an advantage, not a limitation",
        "Clean-slate exploration: no bias from previous decisions",
        "Unbiased code review: agent critiques its own work"
      ],
      "speakerNotes": {
        "talkingPoints": "Here's a crucial insight that transforms how you work with AI coding agents: the LLM is completely stateless. Its only 'world' is the current context window. It doesn't 'remember' previous conversations or have hidden internal state. This is a massive advantage - you control what the agent knows by controlling what's in the context. Start a new conversation and the agent has no bias from previous decisions. The same code that gets 'looks sound overall' in one context triggers 'Critical security vulnerabilities' in a fresh context.",
        "timing": "4 minutes",
        "discussion": "How can you exploit this statelessness? Think about Generate → Review → Iterate workflows, or multi-perspective analysis (security review in one context, performance in another).",
        "context": "This enables powerful patterns: write code in one agent session, then open a fresh session and ask for a security audit without revealing you wrote it. The agent applies full scrutiny with no defensive bias. Or explore multiple architectural approaches in parallel sessions without the agent defending its first choice.",
        "transition": "Let's visualize how clean context prevents agent hallucinations..."
      }
    },
    {
      "type": "visual",
      "title": "Context and Agent Behavior",
      "component": "AbstractShapesVisualization",
      "caption": "Clean context prevents agent hallucinations",
      "speakerNotes": {
        "talkingPoints": "This visualization demonstrates how context engineering affects agent behavior. On the left, polluted or ambiguous context leads to hallucinations and unpredictable outputs. On the right, clean, well-structured context produces reliable, grounded responses. The agent can only work with what's in its context - garbage in, garbage out. How you engineer context determines agent behavior.",
        "timing": "2 minutes",
        "discussion": "Think about times you've seen an agent 'hallucinate' or make up facts. What was likely wrong with the context at that moment?",
        "context": "This connects directly to Lesson 5 (Grounding) and Lesson 6 (Context Management). The quality of your context engineering directly determines agent reliability in production. Senior engineers who master this produce dramatically better results than those who treat agents as magic boxes.",
        "transition": "Now let's discuss the tools that agents use to manipulate context..."
      }
    },
    {
      "type": "concept",
      "title": "Tools: Built-In vs External",
      "content": [
        "Built-in tools: Read, Edit, Bash, Grep, Write, Glob",
        "Optimized for speed, safety, token efficiency",
        "MCP (Model Context Protocol): standardized plugin system",
        "External tools: databases, APIs, cloud platforms",
        "Configure MCP servers in settings"
      ],
      "speakerNotes": {
        "talkingPoints": "Agents become useful through tools - functions the LLM can call to interact with the world. CLI coding agents ship with purpose-built tools for common workflows. These aren't just wrappers around shell commands - they're engineered with edge case handling, LLM-friendly output formats, safety guardrails, and token efficiency. MCP (Model Context Protocol) is a standardized plugin system for adding custom tools to connect your agent to external systems like databases, APIs, or cloud platforms.",
        "timing": "3 minutes",
        "discussion": "What external systems would you want to connect to an agent in your daily work? Think about your specific workflows and infrastructure.",
        "context": "In production, the built-in tools handle 90% of coding tasks. MCP becomes valuable when you need domain-specific integrations (querying your production database to debug issues, calling internal APIs, interacting with cloud resources). The standardization means you can share MCP server configurations across teams.",
        "transition": "Let's look at why CLI coding agents are superior to IDE-based or chat-based alternatives..."
      }
    },
    {
      "type": "comparison",
      "title": "CLI Coding Agents vs Alternatives",
      "left": {
        "label": "Chat/IDE Agents",
        "content": [
          "Tightly coupled to single window/project",
          "Blocked until agent completes or you cancel",
          "Context resets between conversations",
          "Manual copy-paste for code changes",
          "No concurrent work on multiple projects"
        ]
      },
      "right": {
        "label": "CLI Agents",
        "content": [
          "Multiple terminal tabs = multiple agents",
          "Work on different projects simultaneously",
          "Context-switch freely between agents",
          "Each agent works independently",
          "Unlock parallelism without thread management"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "While chat interfaces like ChatGPT and Copilot Chat excel at answering questions and brainstorming, CLI coding agents deliver superior developer experience for actual implementation work. The key advantage: concurrent work. Multiple terminal tabs equals multiple agents working on different projects simultaneously. IDE agents are tightly coupled to a single window and project - you're blocked until the agent completes or you cancel and lose context. CLI agents unlock parallelism without managing conversation threads or multiple IDE instances.",
        "timing": "3-4 minutes",
        "discussion": "How many projects do you typically work on concurrently? How often do you context-switch? CLI agents let you maintain agent context across all of them simultaneously.",
        "context": "In production environments with microservices, this is transformative. Run agents on frontend, backend, and infrastructure repos concurrently. Each maintains its own context. You're not blocked waiting for one agent to finish before switching projects. Note: Lesson 7 covers Planning & Execution strategies including when to parallelize vs serialize tasks.",
        "transition": "Let's synthesize everything we've learned about context engineering..."
      }
    },
    {
      "type": "takeaway",
      "title": "Key Takeaways",
      "content": [
        "Agents autonomously loop execution cycles",
        "Context window is text flow",
        "Statelessness enables unbiased review workflows",
        "Engineer context to steer behavior",
        "CLI agents enable concurrent work"
      ],
      "speakerNotes": {
        "talkingPoints": "Let's synthesize: Agents are autonomous execution loops that perceive, reason, act, and verify until goals are met. Everything flows as text through a context window - no hidden state or magic. The LLM is stateless, which is an advantage: fresh contexts enable unbiased reviews and clean-slate exploration. Effective AI-assisted coding is about engineering context to steer behavior. You're already good at designing interfaces and contracts - apply those skills to engineer context. CLI agents unlock concurrent work across multiple projects. The rest of this course teaches how to apply these insights across real coding scenarios.",
        "timing": "3 minutes",
        "discussion": "Before we move on: which of these insights was most surprising? Which changes how you'll use agents going forward?",
        "context": "These concepts are foundational for the entire course. Lesson 3 introduces high-level methodology, Lesson 4 covers prompting, Lesson 5 tackles grounding, and Lesson 6 addresses context management - all building on the agent execution loop and textual context model we've established here.",
        "transition": "Next lesson, we'll explore high-level methodology for structuring your work with AI coding agents..."
      }
    }
  ]
}
