{
  "metadata": {
    "title": "Understanding Agents",
    "lessonId": "lesson-2-understanding-agents",
    "estimatedDuration": "35-45 minutes",
    "learningObjectives": [
      "Understand agent execution loops",
      "Recognize context as stateless text flow",
      "Master context engineering principles",
      "Choose right tool architecture"
    ]
  },
  "slides": [
    {
      "type": "title",
      "title": "Understanding Agents",
      "subtitle": "Feedback loops, stateless context, and tool architecture",
      "content": [],
      "speakerNotes": {
        "talkingPoints": "This lesson builds on Lesson 1 where we established LLMs as token prediction engines and agents as execution layers. Now we're diving into HOW these systems work together. We'll demystify the agent execution loop, understand what actually happens in the context window, and learn how to engineer context to control behavior.",
        "timing": "1 minute",
        "discussion": "Ask students: Has anyone used Claude Code, Cursor, or ChatGPT to write code? What was different about the experience compared to asking questions?",
        "context": "Agents feel like magic, but they're predictable feedback systems. Understanding the mechanics transforms how you work with them.",
        "transition": "Let's start by understanding what separates an agent from a chat interface."
      }
    },
    {
      "type": "codeExecution",
      "title": "The Agent Execution Loop",
      "steps": [
        {
          "line": "Human specifies: 'Add authentication to this API'",
          "highlightType": "human",
          "annotation": "Clear task definition"
        },
        {
          "line": "Agent perceives: Read API files and existing patterns",
          "highlightType": "execution",
          "annotation": "Gather context about current state"
        },
        {
          "line": "LLM reasons: 'I should implement JWT approach'",
          "highlightType": "prediction",
          "annotation": "Token prediction drives strategy"
        },
        {
          "line": "Agent acts: Edit files to add JWT middleware",
          "highlightType": "execution",
          "annotation": "Deterministic tool execution"
        },
        {
          "line": "Agent observes: Run tests - see failures",
          "highlightType": "feedback",
          "annotation": "Results returned to context"
        },
        {
          "line": "LLM analyzes: 'Token validation is missing'",
          "highlightType": "prediction",
          "annotation": "Reasoning incorporates feedback"
        },
        {
          "line": "Agent acts: Edit validation logic",
          "highlightType": "execution",
          "annotation": "Second iteration"
        },
        {
          "line": "Loop continues until: Tests pass",
          "highlightType": "summary",
          "annotation": "Autonomous iteration without manual intervention"
        }
      ],
      "speakerNotes": {
        "talkingPoints": "This is the fundamental difference between an agent and a chat interface. A chat interface requires YOU to execute actions between prompts. An agent closes the loop automatically. The LLM doesn't have to wait for you to run tests and report errors - it does it, sees the result, and iterates. This is what makes agents autonomous.",
        "timing": "4-5 minutes",
        "discussion": "Compare this to your own debugging workflow. How many times do you manually execute the same cycle when debugging a single issue? Agents eliminate that friction.",
        "context": "In production, this loop can run for hours on complex tasks. The agent manages its own iteration budget and stops when done.",
        "transition": "Now, let's understand what's actually happening inside that loop. The secret is that everything is just text."
      }
    },
    {
      "type": "comparison",
      "title": "Manual vs Autonomous Workflow",
      "left": {
        "label": "Chat Interface",
        "content": [
          "You read API files manually",
          "You manually edit code",
          "You run tests in terminal",
          "You report error to LLM",
          "You manually iterate 5+ times"
        ]
      },
      "right": {
        "label": "Agent Workflow",
        "content": [
          "Agent reads files automatically",
          "Agent edits code directly",
          "Agent runs tests automatically",
          "Agent observes results immediately",
          "Agent iterates until complete"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "The left side is what you experience with ChatGPT or Copilot Chat - you're the bottleneck in the execution loop. The right side is the agent workflow. The agent handles the entire cycle. This isn't just convenience; it's a fundamentally different execution model.",
        "timing": "2-3 minutes",
        "discussion": "Ask: Which approach scales better for complex, multi-step tasks? Why can't chat interfaces parallelize work?",
        "context": "Chat interfaces CAN be used in scripts with copy-paste, but you lose the context window management and tight feedback loop.",
        "transition": "So how does the agent actually know what to do? The answer is: everything is just text flowing through a context window."
      }
    },
    {
      "type": "concept",
      "title": "Everything Is Text",
      "content": [
        "No hidden state, no internal memory",
        "Context window = agent's entire world",
        "All communication is text: prompts, results, reasoning",
        "Same text buffer: system instructions, task, tool calls, outputs",
        "LLM sees its own previous responses as text, not memories"
      ],
      "speakerNotes": {
        "talkingPoints": "This is the foundational insight that demystifies agents. When you use an AI coding agent, you're watching a conversation unfold in a single, large text buffer. The agent doesn't have secret internal state or a reasoning engine hidden from you. Everything it 'thinks' is text being generated in the context. This seems limiting, but it's actually your leverage point.",
        "timing": "3 minutes",
        "discussion": "Ask: If there's no hidden state, what does the agent 'remember'? (Only what's in the current context.) What happens when something scrolls out of the context window?",
        "context": "Extended thinking modes complicate this - Anthropic and OpenAI now offer hidden reasoning tokens that you're billed for but can't see. But for our purposes, understand the core model: text in, text out, visible context window.",
        "transition": "Let's look at a concrete example of what actually flows through that context window."
      }
    },
    {
      "type": "code",
      "title": "Context Window Textual Flow",
      "language": "text",
      "code": "SYSTEM: You are Claude Code. Complete\ncoding tasks using provided tools.\n\nUSER: Add email validation to\nregistration endpoint. Handle edge cases\nlike + addressing and multiple @ symbols.\n\nASSISTANT: I'll add validation.\nLet me first examine the endpoint.\n\nTOOL_CALL: Read('src/auth/register.ts')\n\nTOOL_RESULT: [File contents with\ncurrent registration logic]\n\nASSISTANT: I see the endpoint.\nNow I'll add comprehensive validation.\n\nTOOL_CALL: Edit('src/auth/register.ts',\nold, new)\n\nTOOL_RESULT: Edit successful",
      "caption": "Single text stream: system instructions, task, reasoning, tool calls, and results",
      "speakerNotes": {
        "talkingPoints": "This is exactly what flows through the agent's context window. System instructions tell the agent what tools it has. Your task tells it what to do. The agent generates reasoning and tool calls (which are also text). Tool results are inserted back as text. The entire conversation is one continuous stream. The agent isn't 'thinking separately' - when you see the reasoning, that's text being generated in the same buffer you'll later see the code in.",
        "timing": "3 minutes",
        "discussion": "Ask: Where does the agent 'remember' the file contents it read? (In the context, as tool result text.) If we added 10 more tool calls to this conversation, would the earliest ones still be visible? (Maybe not - context windows are finite.)",
        "context": "This is why context window size matters. Large windows mean agents can handle longer tasks with full conversation history. Small windows force the agent to lose early context.",
        "transition": "Understanding this textual flow reveals something powerful: the LLM is completely stateless."
      }
    },
    {
      "type": "concept",
      "title": "The Stateless Advantage",
      "content": [
        "No internal state = clean-slate exploration",
        "New context, new decisions with no bias",
        "Same code gets different reviews in fresh contexts",
        "Enable: Generate → Review → Iterate workflows",
        "You control what agent knows through context"
      ],
      "speakerNotes": {
        "talkingPoints": "Here's where this gets powerful. Because the LLM has no memory between contexts, you have absolute control. Ask it to implement authentication with JWT in one context, sessions in another - each gets evaluated on pure merit without defending earlier choices. Code that gets 'looks good overall' in one context might trigger 'security vulnerabilities' in a fresh context. This isn't inconsistency - it's the lack of defensive bias. In production, this enables multi-perspective code review: security review in one context, performance in another, maintainability in another.",
        "timing": "3-4 minutes",
        "discussion": "Ask: How does this differ from asking a human engineer to review their own code? (Humans have ego investment. Stateless LLM has none.) How could you abuse this - ask for ineffective code intentionally?",
        "context": "This is why many teams use separate agents for generation and review - they get unbiased feedback because the reviewer sees no history of the author's decisions.",
        "transition": "Now let's talk about the tools that agents use to interact with the world. Tools are how agents perceive, act, and observe."
      }
    },
    {
      "type": "visual",
      "title": "Stateless Context as Advantage",
      "component": "AbstractShapesVisualization",
      "caption": "Fresh contexts remove defensive bias in code review",
      "speakerNotes": {
        "talkingPoints": "The visualization shows multiple context windows - each isolated, each evaluating code independently. The same code can be critiqued differently in different contexts. This isn't a bug; it's a feature. Use it strategically to get multiple perspectives on critical decisions.",
        "timing": "2 minutes",
        "discussion": "Real-world scenario: You write a caching strategy. Context A reviews for performance. Context B reviews for correctness. Context C reviews for maintainability. Each gives different feedback. How would you prioritize conflicting advice?",
        "context": "Enterprise teams running agentic systems often have context pools: validation contexts, security contexts, performance contexts, all running in parallel.",
        "transition": "Agents interact with their world through tools. Let's understand the two categories of tools."
      }
    },
    {
      "type": "concept",
      "title": "Tools: Built-In vs External",
      "content": [
        "Built-in: Read, Edit, Bash, Grep, Write, Glob",
        "Engineered for LLM: edge cases, safety, efficiency",
        "External: MCP (Model Context Protocol)",
        "Connect to: databases, APIs, cloud platforms",
        "Agent discovers MCP tools at runtime"
      ],
      "speakerNotes": {
        "talkingPoints": "Built-in tools aren't just thin wrappers around shell commands. They're engineered specifically for LLM use. Read handles massive files gracefully. Edit prevents accidental overwrites. Bash captures output in token-efficient formats. External tools come through MCP - a standardized plugin system. Configuration happens once, agent discovers all available tools automatically at startup.",
        "timing": "2-3 minutes",
        "discussion": "Ask: Why would you want an MCP tool instead of just using Bash to call an API? (Type safety, error handling, consistent output format, LLM-friendly documentation.)",
        "context": "Popular MCP servers: postgres-mcp (database queries), github-mcp (GitHub API), figma-mcp (design tools). Companies build internal MCPs for business logic.",
        "transition": "Let's see what an MCP configuration looks like."
      }
    },
    {
      "type": "code",
      "title": "MCP Server Configuration",
      "language": "json",
      "code": "{\n  \"mcpServers\": {\n    \"postgres\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"mcp-postgres/index.js\"\n      ],\n      \"env\": {\n        \"DATABASE_URL\":\n          \"postgresql://user:pass@\n          localhost/db\"\n      }\n    }\n  }\n}",
      "caption": "Agent discovers database queries at runtime via MCP",
      "speakerNotes": {
        "talkingPoints": "This is how you connect agents to your infrastructure. The agent sees MCP tools just like built-in tools - same interface, same execution model. The agent can now generate database queries, execute them, see results, and adapt. No copy-pasting results back to the LLM. No manual execution. Pure agent autonomy.",
        "timing": "2 minutes",
        "discussion": "Ask: If an agent has database access, what could go wrong? (Unvalidated queries, data deletion, infinite loops.) How do you guard against this?",
        "context": "In production, MCP servers often run with restricted credentials - read-only for exploratory agents, schema-aware validation for write operations.",
        "transition": "Now that we understand agent architecture, let's compare different agent platforms."
      }
    },
    {
      "type": "comparison",
      "title": "CLI Agents vs Chat/IDE Agents",
      "left": {
        "label": "Chat (ChatGPT, Copilot Chat)",
        "content": [
          "Single window, single project",
          "Manual copy-paste of code",
          "Each conversation resets context",
          "Sequential interaction required",
          "Limited tool access"
        ]
      },
      "right": {
        "label": "CLI Agents (Claude Code, Aider)",
        "content": [
          "Multiple terminal tabs in parallel",
          "Direct file access and execution",
          "Context persists across interaction",
          "Concurrent work on different tasks",
          "Full tool ecosystem: Read, Edit, Bash, MCP"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "CLI agents unlock parallelism. Open three terminal tabs, run three agents on three projects simultaneously. Each agent keeps its context and tools. Compare this to chat interfaces where you're blocked until interaction completes. IDE agents like Cursor are better than chat but still context-switched to a single window. CLI agents are the production choice for serious multi-project work.",
        "timing": "3 minutes",
        "discussion": "Real scenario: You're debugging three services simultaneously. How would each approach handle this? Chat: context switch manually. IDE: switch windows manually. CLI: three running agents, context preserved, check back when ready.",
        "context": "Enterprise teams often run persistent agent clusters - background agents on scheduled tasks (daily audits, dependency scanning) while foreground agents handle active development.",
        "transition": "With all these capabilities, the key question becomes: how do you control agent behavior? The answer is context engineering."
      }
    },
    {
      "type": "concept",
      "title": "Context Engineering: Steering Behavior",
      "content": [
        "Context window = agent's entire world",
        "Engineer context upfront with focused prompts",
        "Steer dynamically mid-conversation if needed",
        "Vague context = wandering, exploratory behavior",
        "Precise context = targeted, scoped execution"
      ],
      "speakerNotes": {
        "talkingPoints": "This is system design thinking applied to text. You already understand interfaces, contracts, and bounded contexts in code. Context engineering is the same principle: define clear boundaries on what the agent should know and do. Vague instructions like 'improve the code' trigger scattered exploration across multiple files. Precise instructions like 'Add input validation to register.ts:45-78, following zod schema pattern from auth.ts' target exactly what the agent should do. You control this through your prompts, file content you show the agent, and tool results you feed back.",
        "timing": "3-4 minutes",
        "discussion": "Ask: If you wanted an agent to add a feature but you wanted it conservative (minimal changes), how would you engineer context differently than if you wanted it aggressive (refactor broadly)?",
        "context": "Advanced teams use 'context scaffolding' - they pre-populate context with relevant examples, patterns, constraints, and even previous solutions in similar domains.",
        "transition": "Let's summarize what we've learned about agents."
      }
    },
    {
      "type": "takeaway",
      "title": "Key Takeaways",
      "content": [
        "Agents are feedback loops: perceive, reason, act, verify, iterate",
        "Context is text: system instructions, task, tools, results in one stream",
        "Stateless LLMs enable clean-context exploration and unbiased review",
        "CLI agents with MCP tools unlock parallelism and production autonomy"
      ],
      "speakerNotes": {
        "talkingPoints": "These four concepts are the foundation of everything that follows. Agents aren't magic - they're predictable systems that follow the feedback loop. Their only world is the context window, which you control. Leverage the stateless nature for multi-perspective analysis. And use CLI agents with proper tool architecture when you need production-grade autonomy. Lesson 3 builds directly on this foundation with methodologies for using agents in real coding workflows.",
        "timing": "2 minutes",
        "discussion": "Ask students: Which concept was most surprising or changed how you think about agents?",
        "context": "These principles apply whether you're using Claude Code, Cursor, Aider, or any other agent system.",
        "transition": "Next lesson: High-Level Methodology - how to structure work for agents to actually succeed."
      }
    }
  ]
}