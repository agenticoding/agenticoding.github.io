{
  "metadata": {
    "title": "Introduction to AI Agents",
    "lessonId": "lesson-1-intro",
    "estimatedDuration": "30-45 minutes",
    "learningObjectives": [
      "Understand agents as tool systems",
      "Recognize LLM token prediction mechanics",
      "Avoid anthropomorphizing AI behavior",
      "Establish operator mindset foundation"
    ]
  },
  "slides": [
    {
      "type": "title",
      "title": "Introduction to AI Agents",
      "subtitle": "Operating Precision Instruments, Not Managing Teammates",
      "content": [],
      "speakerNotes": {
        "talkingPoints": "Welcome to the course. We're here to teach you how to operate AI agents that autonomously execute complex development tasks. But before we jump into what agents can do, we need to establish what they actually are - because marketing terminology makes them sound far more magical than the reality.",
        "timing": "1-2 minutes",
        "discussion": "Ask students: How many of you have already used AI coding assistants? What surprised you most about how they work?",
        "context": "This is the foundation lesson. Everything that follows depends on understanding that agents are tools, not teammates.",
        "transition": "Let's start by understanding the paradigm shift happening in software engineering."
      }
    },
    {
      "type": "concept",
      "title": "The Paradigm Shift",
      "content": [
        "Manufacturing: Manual craftsmanship → Automated machine programming",
        "CNC machines transformed operator role, not eliminated it",
        "Software: Line-by-line coding → Agent orchestration",
        "Same transformation: bandwidth, repeatability, precision",
        "Gain in creativity through configuration, not loss of control"
      ],
      "speakerNotes": {
        "talkingPoints": "This course is about a fundamental transformation in how we build software. It's not unprecedented - manufacturing went through this 50 years ago. CNC machines didn't put machinists out of work; they transformed their role from manual implementation to design, programming, and verification. We're seeing the exact same pattern in software engineering right now.",
        "timing": "2-3 minutes",
        "discussion": "Ask: What do you think changed about a machinist's job after CNC? How is that similar to your job evolving with AI agents?",
        "context": "This analogy is crucial for reframing the relationship with AI. You're not losing control; you're gaining leverage. A skilled CNC operator can produce more parts, with better precision, because they focus on specification and verification instead of manual execution.",
        "transition": "Let's look at what this transformation means specifically for software engineers."
      }
    },
    {
      "type": "comparison",
      "title": "Traditional vs Agent-Driven Development",
      "left": {
        "label": "Traditional",
        "content": [
          "Engineers write code line-by-line",
          "Focus on syntax and implementation",
          "Bandwidth limited by typing speed",
          "Manual testing and verification"
        ]
      },
      "right": {
        "label": "Agent-Driven",
        "content": [
          "Engineers orchestrate AI agents",
          "Focus on architecture and specification",
          "Bandwidth expanded through delegation",
          "Systematic testing as architectural guardrail"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "In traditional development, you're the doer. You write code, you run tests, you debug. With agents, you become the director. You specify what needs to happen, you verify the output, you maintain control through architecture and constraints. This isn't lazy - it's a fundamental shift in where your expertise creates value.",
        "timing": "2-3 minutes",
        "discussion": "Ask: Which role are you already doing? When do you catch yourself typing out obvious boilerplate instead of designing systems?",
        "context": "Many engineers feel uncomfortable with this shift initially. They worry they're 'not doing the work.' The reality is that the work has changed. Configuration and verification are harder problems than implementation.",
        "transition": "Now let's understand what AI agents actually are, starting with the brain: the LLM."
      }
    },
    {
      "type": "concept",
      "title": "LLM: The Brain (Token Prediction Engine)",
      "content": [
        "Predicts next probable token in sequence",
        "Processes ~200K tokens of context (working memory)",
        "Samples from learned probability distributions",
        "Zero consciousness, intent, or comprehension"
      ],
      "speakerNotes": {
        "talkingPoints": "A Large Language Model is a statistical pattern matcher. It's built on transformer architecture and trained to predict the next most probable token given a sequence of previous tokens. That's it. No magic. It's incredibly sophisticated autocomplete - one that's read most of the internet and can generate convincing continuations of any pattern it's learned.",
        "timing": "3-4 minutes",
        "discussion": "Ask: Can someone explain what 'token' means? (Sub-word unit, roughly 4 characters). Ask: If it just predicts the next token, how does it generate complex code? (Sequential predictions that build on each other, guided by context and constraints.)",
        "context": "This is where most misunderstandings come from. People see the output - coherent, sometimes brilliant code - and assume understanding happened. What actually happened is probability calculation. The LLM saw similar patterns in training data and produced statistically likely continuations.",
        "transition": "But the LLM alone can't do anything. It only generates text. We need the execution layer."
      }
    },
    {
      "type": "marketingReality",
      "title": "Marketing vs Reality: LLM Capabilities",
      "metaphor": {
        "label": "Marketing Speak",
        "content": [
          "\"The agent thinks\"",
          "\"The agent understands code\"",
          "\"The agent learns from you\"",
          "\"The agent reasons through problems\""
        ]
      },
      "reality": {
        "label": "Technical Reality",
        "content": [
          "LLM generates token predictions via attention layers",
          "Pattern matching produces contextually probable output",
          "Statistical weights fixed at training time (not updated in conversation)",
          "Sequential predictions build on each other"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "This is critical. Marketing language makes LLMs sound conscious and learning. They're not. They're sophisticated probability machines. The LLM can't learn from you during a conversation - it learned patterns from training data, and now it's pattern-matching against your input. It can't 'understand' code the way a human does. It can produce contextually probable continuations based on patterns it's seen.",
        "timing": "2-3 minutes",
        "discussion": "Ask: Why does the LLM sometimes write invalid code if it 'understands'? (It's optimizing for probability, not correctness. It's generating what's likely, not what's right.)",
        "context": "This distinction is operationally important. It explains why agents hallucinate, why they sometimes miss obvious bugs, and why YOUR verification systems matter. You're the intelligence. The agent is a sophisticated execution tool.",
        "transition": "Now let's look at the execution layer - the body that makes the agent actually do things."
      }
    },
    {
      "type": "concept",
      "title": "Agent Software: The Body (Execution Layer)",
      "content": [
        "File operations: Read, Write, Edit",
        "Command execution: Bash, git, npm, pytest",
        "Code search: Grep, Glob",
        "API calls: Fetch docs, external resources",
        "LLM predicts → Agent executes deterministically"
      ],
      "speakerNotes": {
        "talkingPoints": "The LLM brain alone can only generate text. The agent framework is the body - it wraps the LLM and enables action. When an agent 'implements a feature,' it's really a loop: LLM predicts what should happen next, agent executes that prediction deterministically, agent returns the result to the LLM. Rinse, repeat.",
        "timing": "2-3 minutes",
        "discussion": "Ask: What happens if the LLM predicts an invalid command? (The agent executes it anyway - you need validation and error handling.)",
        "context": "This is where control lives. The agent software enforces constraints, handles errors, and verifies outputs. You configure these guardrails. The LLM is just the planning layer.",
        "transition": "Let's visualize how this loop actually works in practice."
      }
    },
    {
      "type": "codeExecution",
      "title": "Agent Execution Loop: How It Actually Works",
      "steps": [
        {
          "line": "Engineer specifies:\n'Add authentication middleware\nto src/app.ts'",
          "highlightType": "human",
          "annotation": "Explicit task specification with location"
        },
        {
          "line": "LLM predicts:\n'I should read existing auth\npatterns'",
          "highlightType": "prediction",
          "annotation": "Token prediction drives next action"
        },
        {
          "line": "Agent executes:\nRead(src/middleware/auth.ts)",
          "highlightType": "execution",
          "annotation": "Deterministic tool execution"
        },
        {
          "line": "File content returned to context:\n[200 lines of existing auth code]",
          "highlightType": "feedback",
          "annotation": "Operation result available to LLM"
        },
        {
          "line": "LLM analyzes patterns and\npredicts:\n'I'll use JWT approach\nwith middleware pattern'",
          "highlightType": "prediction",
          "annotation": "Prediction incorporates new context"
        },
        {
          "line": "Agent executes:\nEdit(src/app.ts, old, new)",
          "highlightType": "execution",
          "annotation": "Code modification based on prediction"
        },
        {
          "line": "Agent executes:\nBash('npm test')",
          "highlightType": "execution",
          "annotation": "Verification step"
        },
        {
          "line": "Test results returned to context",
          "highlightType": "feedback",
          "annotation": "LLM receives verification signal"
        },
        {
          "line": "LLM predicts:\n'Tests pass, task complete'",
          "highlightType": "prediction",
          "annotation": "Or predicts fixes if tests fail"
        },
        {
          "line": "Loop continues until task\nsucceeds or max iterations",
          "highlightType": "summary",
          "annotation": "Engineer maintains control through iteration limits"
        }
      ],
      "speakerNotes": {
        "talkingPoints": "This is the core mental model. The LLM doesn't have agency - it just predicts what should happen next. The agent software executes that prediction, returns the result, and the LLM makes the next prediction. Notice how verification (tests) are part of the loop. The LLM doesn't know if the code is correct until we run tests and feed the results back.",
        "timing": "4-5 minutes",
        "discussion": "Ask: What happens if we skip the test step? (The agent can't verify correctness, so it might keep iterating on broken code.) Ask: What happens if the LLM predicts something impossible? (The agent executes it and returns an error, the LLM predicts a fix.)",
        "context": "This loop is why context management matters. The LLM only sees ~200K tokens. If your task is longer than that, you need to provide it incrementally. It's also why constraints matter - if you don't specify what 'done' means, the loop might run until max iterations.",
        "transition": "Now let's talk about the three critical errors operators make when they forget this model."
      }
    },
    {
      "type": "concept",
      "title": "Three Critical Operator Errors",
      "content": [
        "Error 1: Assuming agent 'knows' things (it only sees current context)",
        "Error 2: Expecting agent to 'care' about outcomes (executes literal instructions)",
        "Error 3: Treating it like a teammate (it's a precision instrument)",
        "Fix: Provide explicit context, precise constraints, verification systems"
      ],
      "speakerNotes": {
        "talkingPoints": "Most problems with agents stem from three misconceptions. First: assuming the agent has knowledge beyond what's in the current context. It doesn't. Second: expecting the agent to interpret your loose instructions charitably. It won't - it executes what you literally ask. Third: treating it like a junior developer who should 'get it.' It's not a person, it's a tool that speaks English.",
        "timing": "3-4 minutes",
        "discussion": "Ask: Can anyone share a time when an agent did something that seemed obviously wrong? (Usually it's because they made one of these three assumptions.)",
        "context": "These errors are normal. We're wired to anthropomorphize intelligent-seeming systems. But this anthropomorphization kills your effectiveness as an operator. The moment you accept that it's a tool, not a teammate, you start using it better.",
        "transition": "Understanding these limitations actually reveals the real power of AI agents."
      }
    },
    {
      "type": "concept",
      "title": "Power and Limitation of \"Fancy Autocomplete\"",
      "content": [
        "Power: Incredible at generating patterns it's seen in training",
        "Limitation: No model of correctness, only probability",
        "Implication: You must build verification systems (tests, types, lints)",
        "You're operating a code generation tool, not managing a developer"
      ],
      "speakerNotes": {
        "talkingPoints": "This reframes the entire relationship. Yes, LLMs are 'just fancy autocomplete,' but that's incredibly powerful. They're good at generating code patterns because they've learned from billions of examples. The limitation is that they optimize for probability, not correctness. They'll generate code that looks right even when it's subtly wrong. That's why your job shifts: you're not managing a developer, you're maintaining architectural guardrails.",
        "timing": "2-3 minutes",
        "discussion": "Ask: What's an example of code that's syntactically correct but functionally wrong? (Off-by-one errors, race conditions, etc.) That's where agents excel at generating wrong code.",
        "context": "This is liberating. It means you don't need to worry about the agent's 'motivation' or 'understanding.' You need to worry about your test coverage, your type system, your linting rules. Those are your control mechanisms.",
        "transition": "Let's wrap up with the key takeaways from this lesson."
      }
    },
    {
      "type": "takeaway",
      "title": "Key Takeaways",
      "content": [
        "Agents = LLM (token prediction engine) + Software (execution layer)",
        "LLM has no comprehension, consciousness, or learning during conversation",
        "Your role shifts from coder to operator: specify, verify, maintain guardrails",
        "Three errors kill effectiveness: assuming knowledge, expecting intent, anthropomorphizing"
      ],
      "speakerNotes": {
        "talkingPoints": "You're learning to operate precision tools. The mental model matters more than the specific tools. If you understand that agents are token prediction engines wrapped in execution software, you'll stop making the three operator errors. You'll start building better specifications, you'll anticipate hallucinations, and you'll architect systems that verify correctness instead of trusting the agent.",
        "timing": "2 minutes",
        "discussion": "Ask: Which of the three errors have you made? How would you fix it?",
        "context": "This is the foundation. Everything in the next lessons builds on this understanding. You can't optimize prompts effectively until you understand what the LLM actually does. You can't architect agent workflows until you understand the execution loop.",
        "transition": "Next lesson we'll dive into agent architecture and how to structure workflows for complex tasks."
      }
    }
  ]
}
