{
  "metadata": {
    "title": "Lesson 9: Reviewing Code",
    "lessonId": "lesson-9-reviewing-code",
    "estimatedDuration": "35-45 minutes",
    "learningObjectives": [
      "Review code in fresh context",
      "Apply systematic review methodology",
      "Optimize for dual audiences",
      "Recognize diminishing returns"
    ]
  },
  "slides": [
    {
      "type": "title",
      "title": "Reviewing Code",
      "subtitle": "The Validate phase: systematic quality gates before shipping",
      "content": [],
      "speakerNotes": {
        "talkingPoints": "This lesson addresses the critical final step before committing code: validation through review. Your tests pass. The implementation works. But is it actually correct? Code review is the quality gate that catches probabilistic errors agents introduce—subtle bugs, architectural mismatches, edge cases. This is where your operator judgment becomes essential.",
        "timing": "1 minute",
        "discussion": "Ask: 'How often has code passed tests but still had production issues?' This sets up why code review is non-negotiable despite automated testing.",
        "context": "This is the 'Validate' phase from Lesson 3's four-phase workflow. We're completing the feedback loop: plan → execute → test → validate.",
        "transition": "Let's start with the core principle that makes review effective: context separation."
      }
    },
    {
      "type": "concept",
      "title": "Fresh Context is Critical",
      "content": [
        "Reviewing in same conversation → agent defends prior decisions",
        "Fresh context prevents confirmation bias",
        "Agent analyzes objectively without attachment to prior choices",
        "Parallels Lesson 1 & 2: leverage stateless nature of agents",
        "Separate review conversation is non-negotiable"
      ],
      "speakerNotes": {
        "talkingPoints": "This is the fundamental insight: an agent reviewing its own work in the same conversation where it made decisions will be biased toward defending those decisions. It has context of its reasoning. Fresh context means no memory of prior choices—pure objective analysis. This parallels the stateless agent principle from Lessons 1-2. We're intentionally throwing away prior context to gain objectivity.",
        "timing": "2-3 minutes",
        "discussion": "Ask: 'When you review your own code immediately after writing it, what bias creeps in? How does that compare to reviewing someone else's code?' Connect this to agent behavior.",
        "context": "In production: if your code review process is 'generate code, run tests, commit immediately,' you're skipping validation. If you have multiple rounds of code generation, each review should be in a fresh context.",
        "transition": "Now that we understand why fresh context matters, let's look at the systematic review methodology."
      }
    },
    {
      "type": "concept",
      "title": "Review is a Four-Phase Process",
      "content": [
        "Research: understand intent and context",
        "Plan: structure the review approach",
        "Execute: perform systematic analysis",
        "Validate: decide ship, fix, or regenerate"
      ],
      "speakerNotes": {
        "talkingPoints": "Code review isn't a free-form activity—apply the same four-phase methodology from Lesson 3 to reviewing. First, research: understand what the implementation should accomplish and what constraints apply. Second, plan: decide what to check and in what order. Third, execute: perform the analysis systematically using a review template. Fourth, validate: apply operator judgment to the findings. This structure prevents ad-hoc reviews and ensures comprehensive coverage.",
        "timing": "2 minutes",
        "discussion": "Ask: 'What goes wrong when you skip the planning phase of a review?' (Answer: you miss categories of issues, contradictory feedback.)",
        "context": "This four-phase approach works for security audits, performance reviews, and architectural validations—it's a general pattern.",
        "transition": "The execute phase is where the review prompt template comes in. Let's examine it."
      }
    },
    {
      "type": "code",
      "title": "The Review Prompt Template",
      "language": "text",
      "code": "You are an expert code reviewer for this\ncodebase.\n\nContext:\n- Project: [name]\n- Language: [TypeScript/Python/etc]\n- Standards: [DRY, YAGNI, typed, tested]\n\nReview this implementation:\n```\n[CODE BLOCK]\n```\n\nAnalyze:\n1. Correctness (logic, edge cases)\n2. Architecture (patterns, fit)\n3. Maintainability (clarity, style)\n4. Test coverage (what's untested?)\n\nProvide file:line references for\nall findings.",
      "caption": "Structured review with persona, constraints, and evidence requirements",
      "speakerNotes": {
        "talkingPoints": "This template integrates techniques from Lesson 4 (Prompting 101): it establishes persona ('expert code reviewer'), sets communication constraints (focus on 4 categories), provides grounding (project context), and requires evidence ('file:line references'). Notice what's NOT here: vague critique. Notice what IS here: specific dimensions to analyze. This template is adaptable—swap analysis dimensions for security review, performance review, or architectural review.",
        "timing": "3 minutes",
        "discussion": "Ask: 'What happens if you skip the context section?' (Answer: agent hallucinates patterns that don't fit your codebase). 'Why require file:line references?' (Answer: it forces grounding in actual code, not opinions.)",
        "context": "In production: this template is your baseline. Customize the 'Analyze' section for your team's specific concerns. If you care about logging, add it. If concurrency is critical, add it.",
        "transition": "The template assumes one-pass review. In reality, review is iterative. Let's look at the cycle."
      }
    },
    {
      "type": "concept",
      "title": "Iterative Review Cycle",
      "content": [
        "Review in fresh context → identify issues",
        "Fix issues → run tests (Lesson 8)",
        "Re-review in fresh context (not same conversation)",
        "Stop at: green light OR diminishing returns",
        "Tests are arbiter: review failure = review wrong"
      ],
      "speakerNotes": {
        "talkingPoints": "Review is rarely one-pass. After first review, you fix identified issues and run tests. Those tests might fail—meaning the review suggestion was wrong. Then re-review in a fresh context. The key: don't continue the same conversation where the agent will start defending its prior suggestions. Keep iterating until either all issues are resolved (green light) or findings become trivial (diminishing returns). When you hit diminishing returns—nitpicking, hallucinated problems, or excessive iteration cost—stop and ship. Your test suite is the objective arbiter.",
        "timing": "3 minutes",
        "discussion": "Ask: 'When is it time to stop reviewing and ship the code?' (Answer: when tests pass AND findings are green OR diminishing returns set in). 'What's a sign the review is hallucinating?' (Answer: suggested 'fix' breaks previously passing tests.)",
        "context": "Real scenario: you do 3 rounds of review and regeneration. Round 4 identifies 'issues' that don't exist. Stop. Ship. Diminishing returns.",
        "transition": "Reviewing code is one context. But pull requests have a second audience: AI review assistants. That requires a different approach."
      }
    },
    {
      "type": "comparison",
      "title": "Two Audiences, One PR",
      "left": {
        "label": "Human Reviewers",
        "content": [
          "Scan quickly, infer from context",
          "Value concise summaries (1-3 paragraphs)",
          "Want business value and 'why'",
          "Need high-level understanding at a glance"
        ]
      },
      "right": {
        "label": "AI Review Assistants",
        "content": [
          "Parse chunk-by-chunk, need explicit structure",
          "Require detailed technical context",
          "Want specific file paths, line numbers, patterns",
          "Need enumerated breaking changes, dependencies"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "Pull requests serve two fundamentally different audiences with different information processing styles. Humans scan and infer. AI parses and requires explicit structure. Traditional PR descriptions optimize for one or the other. The solution: generate dual-optimized descriptions—a human-readable summary in the PR description, and a comprehensive technical context in a separate artifact (like PR_REVIEW_CONTEXT.md) that feeds your AI review assistant. This requires a coordinated workflow using sub-agents.",
        "timing": "2-3 minutes",
        "discussion": "Ask: 'What do you put in a PR description when your reviewer is an AI assistant?' (Answer: explicit structure, not prose). 'Why not just use one description?' (Answer: humans need brevity, AI needs comprehensiveness—incompatible optimization targets.)",
        "context": "In practice: your GitHub PR has the human summary. You attach PR_REVIEW_CONTEXT.md (or feed it to your AI assistant separately). The assistant analyzes the technical context, the human reads the summary.",
        "transition": "Creating these dual descriptions requires a specific prompt pattern. Let's examine it."
      }
    },
    {
      "type": "code",
      "title": "Dual-Optimized PR Template",
      "language": "text",
      "code": "Generate two outputs:\n\n1. HUMAN DESCRIPTION\n   (1-3 paragraphs, scannable)\n   - What changed (business impact)\n   - Breaking changes (if any)\n   - Key files affected\n\n2. AI CONTEXT (PR_REVIEW_CONTEXT.md)\n   - Detailed architecture changes\n   - File:line affected areas\n   - Explicit pattern changes\n   - Dependencies and regressions\n   - Suggested review focus areas\n\nUse sub-agent tool to explore\ngit history and codebase patterns\nbefore drafting to conserve context.",
      "caption": "Separate outputs for human brevity and AI comprehensiveness",
      "speakerNotes": {
        "talkingPoints": "This prompt demonstrates multiple Lesson 4-7 techniques: sub-agents for context conservation (Lesson 5—don't let git diffs fill your context window), multi-source grounding (research PR best practices, ground in actual codebase), structured prompting (two distinct output formats), and evidence requirements (actual file paths from git history). The sub-agent capability is unique to Claude Code CLI—it lets you explore 20-30 changed files in a sub-agent without bloating the main conversation context.",
        "timing": "2-3 minutes",
        "discussion": "Ask: 'Why use a sub-agent instead of doing everything in one prompt?' (Answer: git history exploration can consume 40K+ tokens, pushing critical reasoning into the U-shaped curve's ignored middle. Sub-agents keep main context focused.)",
        "context": "Production workflow: run this prompt once per PR. Generates both human and AI-optimized descriptions. Post human description to GitHub, store AI context for assistant review.",
        "transition": "Once a PR is open with dual descriptions, how do you leverage them as a reviewer? Let's look at that."
      }
    },
    {
      "type": "code",
      "title": "Consuming Dual PR Descriptions",
      "language": "text",
      "code": "Step 1: Read human description\n  → Understand 'why' and business value\n  → Note breaking changes\n  → Form initial mental model\n\nStep 2: Feed AI-optimized context\n        to your review assistant\n  → Provides technical grounding\n  → Explicit terminology and patterns\n  → Reduces hallucinations\n\nStep 3: Use structured review prompt\n  with evidence requirements\n  → file:line references\n  → Severity-tagged findings\n  → APPROVE/REQUEST CHANGES/REJECT",
      "caption": "Leverage both descriptions for comprehensive, grounded review",
      "speakerNotes": {
        "talkingPoints": "As a reviewer receiving a PR with dual descriptions, consume them in sequence. First, the human summary forms your mental model—you understand intent and scope. Second, feed the AI-optimized context to your review assistant (Copilot, Claude Code, etc.). This provides the comprehensive grounding needed for accurate analysis. The AI assistant has architecture, patterns, and dependencies—it can analyze without hallucinating. Third, use a structured review prompt with evidence requirements to produce findings organized by severity with file references.",
        "timing": "2 minutes",
        "discussion": "Ask: 'What happens if you skip the AI-optimized context?' (Answer: your AI assistant hallucinates or makes shallow observations). 'Why start with the human summary?' (Answer: context matters—you need intent before diving into code details.)",
        "context": "Practical: this workflow improves review quality 60%+ by giving AI assistants the context they need. Hallucinations drop, coverage improves.",
        "transition": "We've covered the systematic process. Now let's look at a specific technique that makes the analysis more efficient."
      }
    },
    {
      "type": "concept",
      "title": "Chain of Draft: Efficient Analysis",
      "content": [
        "Think step-by-step, but keep drafts minimal",
        "5 words maximum per thinking step",
        "Return final assessment after separator (####)",
        "Maintains CoT reasoning without token bloat",
        "Faster responses, same analysis quality"
      ],
      "speakerNotes": {
        "talkingPoints": "Chain of Draft (CoD) is an optimization of Chain of Thought prompting. Instead of verbose step-by-step explanations, CoD instructs the LLM to think through analysis logically but keep intermediate drafts to 5 words maximum. Then return the final assessment after a separator. This provides the same reasoning benefits as CoT but with reduced token consumption and faster response times. You get systematic analysis without the cost. This is particularly effective for code review, security audits, and architectural analysis.",
        "timing": "1-2 minutes",
        "discussion": "Ask: 'Why limit drafts to 5 words if you want detailed reasoning?' (Answer: the detailed reasoning happens in the model's forward pass—the brief draft is just a checkpoint. You only pay tokens for the final assessment.)",
        "context": "In practice: using CoD in review prompts cuts token consumption 30-40% while maintaining analysis quality. Critical for cost-conscious teams.",
        "transition": "Now let's synthesize everything into key takeaways."
      }
    },
    {
      "type": "takeaway",
      "title": "Key Takeaways",
      "content": [
        "Review in fresh context—separate conversation prevents confirmation bias",
        "Use four-phase methodology (Research → Plan → Execute → Validate)",
        "Iterate until green light or diminishing returns, not perfection",
        "Generate dual-optimized PR descriptions for human and AI reviewers",
        "Evidence requirements force grounding—always require file:line references",
        "Tests are the arbiter—if a review suggestion breaks tests, the review was wrong"
      ],
      "speakerNotes": {
        "talkingPoints": "Pull these six principles together as the foundation for effective code review. Fresh context prevents bias. Four-phase methodology ensures systematic coverage. Knowing when to stop iterating prevents over-engineering. Dual descriptions serve both audiences. Evidence requirements prevent hallucinations. Tests provide objective grounding. These principles apply whether you're reviewing your own agent-generated code or auditing PRs from team members.",
        "timing": "2 minutes",
        "discussion": "Ask: 'Which of these principles would have the biggest impact in your current code review process?' Have students identify one to implement immediately.",
        "context": "Production application: these are not optional practices. They're the difference between shipping defective code and maintaining quality standards at scale with AI assistance.",
        "transition": "This completes the review phase. Next lesson: Debugging—how to identify and fix problems when things go wrong."
      }
    }
  ]
}
