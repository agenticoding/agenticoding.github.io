{
  "metadata": {
    "title": "Lesson 9: Reviewing Code",
    "lessonId": "lesson-9-reviewing-code",
    "estimatedDuration": "35-45 minutes",
    "learningObjectives": [
      "Review code in fresh context",
      "Identify agent probabilistic errors",
      "Generate dual-optimized PR descriptions",
      "Know when to stop iterating"
    ]
  },
  "slides": [
    {
      "type": "title",
      "title": "Lesson 9: Reviewing Code",
      "subtitle": "The critical Validate phase—catching\nagent errors before shipping",
      "content": [],
      "speakerNotes": {
        "talkingPoints": "Tests pass. The agent completed your plan. But is it actually correct? This lesson covers the Validate phase from Lesson 3—the systematic quality gate that catches the probabilistic errors agents inevitably introduce. We'll cover fresh-context review, iterative refinement, and generating PR descriptions that work for both humans and AI reviewers.",
        "timing": "1 minute",
        "discussion": "Ask: Who's reviewed agent-generated code that looked good but had subtle bugs?",
        "context": "By this point in Lesson 3's workflow (Research, Plan, Execute), code quality isn't guaranteed. Agents make statistical predictions—sometimes wrong. Review catches architectural mismatches, edge case handling, and style violations the iterative process missed.",
        "transition": "Let's start with the core insight: why fresh context matters for objective review."
      }
    },
    {
      "type": "concept",
      "title": "Fresh Context: The Key Insight",
      "content": [
        "Review in separate conversation from authoring",
        "Prevents confirmation bias and code defensiveness",
        "Leverages agent statelessness (Lessons 1-2)",
        "Same conversation: agent defends prior choices",
        "Fresh context: agent analyzes objectively"
      ],
      "speakerNotes": {
        "talkingPoints": "The most important rule: never ask the agent that wrote the code to review it in the same conversation. In fresh context, an agent reviews code without attachment to prior decisions. It asks 'Is this correct?' not 'How do I defend this choice?' This is counterintuitive but essential—it's the difference between defensive and objective analysis.",
        "timing": "2 minutes",
        "discussion": "Compare to code review: we ask colleagues, not the original author, to catch logic errors. Same principle applies to agents.",
        "context": "This relies on understanding from Lessons 1-2: agents are stateless. A fresh prompt with the code and review instructions provides a clean slate for analysis, without prior context influencing judgment.",
        "transition": "Now let's understand the different code contexts we work in."
      }
    },
    {
      "type": "comparison",
      "title": "Agent-Only vs Mixed Codebases",
      "left": {
        "label": "Agent-Only",
        "content": [
          "Maintained exclusively by AI",
          "Optimize for AI clarity",
          "More explicit type annotations",
          "Detailed architectural context files"
        ]
      },
      "right": {
        "label": "Mixed (Most Production)",
        "content": [
          "Humans and AI collaborate on code",
          "Optimize for human readability",
          "AI must follow team style",
          "Mandatory human audit before commit"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "Most of you work in mixed codebases where both humans and agents write code. The critical difference: in mixed codebases, you must add an explicit manual review step where you fully read AI-generated code before committing. Without this, agents generate code following training patterns that may not match your team's readability standards. This is non-negotiable.",
        "timing": "2 minutes",
        "discussion": "Ask: Who here works with AI-generated code in a human-collaborated project? What style mismatches have you seen?",
        "context": "The same engineering principles (DRY, YAGNI, architecture) apply to both. What changes is the review process and style optimization. In agent-only systems, you can tolerate more verbose documentation and explicit type annotations because the audience is deterministic. In mixed systems, humans are in the loop—clarity for humans becomes a hard requirement.",
        "transition": "Let's look at the actual review process—the prompt template that structures systematic review."
      }
    },
    {
      "type": "code",
      "title": "Review Prompt Template",
      "language": "bash",
      "code": "# After tests pass, review in fresh context\n\necho \"You are a code reviewer. Analyze\nthis implementation for correctness,\narchitecture, and readability.\n\nAccept the following context:\n- Domain: [specific domain]\n- Architecture: [key patterns]\n- Review standards: [team guidelines]\n\nProvide:\n1. Logic errors or edge cases\n2. Architecture mismatches\n3. Code readability issues\n4. Actionable suggestions\n\nProvide file paths and line numbers.\" | \\n  agent --review src/feature/",
      "caption": "Structure review with persona, constraints, and grounding",
      "speakerNotes": {
        "talkingPoints": "This template integrates techniques from Lesson 4: Prompting 101. The persona (code reviewer) sets perspective. Context constraints (domain, architecture) prevent hallucinations. Structured output format (numbered list) ensures consistency. Critically: 'Provide file paths and line numbers' forces grounding in actual code, not statistical guesses.",
        "timing": "3 minutes",
        "discussion": "Point out each element: Why persona? (Sets review tone) Why context? (Prevents suggesting patterns that don't fit your codebase) Why file paths? (Makes findings verifiable)",
        "context": "This is the review equivalent of the Lesson 7 execution prompt. Both emphasize evidence requirements. A review without file:line is just opinion. With grounding, findings are testable—you can verify them immediately.",
        "transition": "But review is rarely one-pass. Let's see how to iterate effectively."
      }
    },
    {
      "type": "concept",
      "title": "Iterative Review: The Cycle",
      "content": [
        "Review in fresh context → find issues",
        "Fix issues, re-run tests (catch regressions)",
        "Review again in fresh conversation",
        "Stop at green light or diminishing returns",
        "Don't ship with unresolved test failures"
      ],
      "speakerNotes": {
        "talkingPoints": "Code review is a cycle: analyze, fix, validate, repeat. Each iteration must run tests after fixes to catch regressions—the review 'fix' might break working code. The agent can be wrong. This is where operator judgment becomes essential: do the fixes actually improve quality?",
        "timing": "2-3 minutes",
        "discussion": "Ask: What's an example of a review 'suggestion' that actually broke working code? (Happens more often than people think.)",
        "context": "This aligns with Lesson 8 (testing): tests are your objective arbiter. If a review suggestion breaks tests, the review was wrong. Trust the tests more than the review agent's explanation.",
        "transition": "When do you stop iterating? There are clear signals."
      }
    },
    {
      "type": "comparison",
      "title": "Shipping Decisions: When to Stop",
      "left": {
        "label": "Keep Iterating",
        "content": [
          "Test failures after review suggestions",
          "Critical security or logic errors",
          "Architectural mismatches",
          "Pattern violations in codebase"
        ]
      },
      "right": {
        "label": "Ship the Code",
        "content": [
          "Tests passing + review green",
          "Nitpicking on style preferences",
          "Agent hallucinating non-existent issues",
          "4+ iterations with diminishing returns"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "Diminishing returns is the key concept. If review iterations are finding increasingly trivial issues—variable naming, spacing, style preferences—you've crossed the threshold. Further iteration costs more than it provides. Trust your tests as the objective arbiter. Ship when tests pass and substantive issues are resolved.",
        "timing": "2 minutes",
        "discussion": "Point out: Review itself is probabilistic. The agent making suggestions is also an LLM making predictions. It can be wrong. Tests don't lie.",
        "context": "This ties back to Lesson 7's execution principle: verification stops guessing. Tests verify correctness. If tests pass and review finds only nitpicks, ship it.",
        "transition": "Now we move from reviewing code to documenting it for review. This is where you optimize for two audiences."
      }
    },
    {
      "type": "concept",
      "title": "Two Audiences, Different Needs",
      "content": [
        "Humans: scan quickly, infer from context",
        "Humans want: concise why + business value",
        "AI reviewers: parse chunk-by-chunk sequentially",
        "AI needs: explicit structure + technical detail",
        "Traditional PRs optimize for one, not both"
      ],
      "speakerNotes": {
        "talkingPoints": "Pull requests need to serve two completely different audiences. Humans read summaries and infer meaning. AI agents parse systematically and need explicit structure. A 1-paragraph summary works for humans but leaves AI reviewers guessing about architectural intent. Detailed technical specs help AI but overwhelm humans scrolling GitHub.",
        "timing": "2-3 minutes",
        "discussion": "Ask: Has an AI review tool ever suggested something that made no sense for your architecture? Likely because the PR description was too vague for systematic parsing.",
        "context": "This connects to Lesson 5 (Grounding) and Lesson 4 (Prompting): AI agents need explicit structure. Vague pronouns and semantic drift cause hallucinations. Humans handle ambiguity naturally—AI doesn't.",
        "transition": "The solution: generate dual-optimized descriptions using sub-agents."
      }
    },
    {
      "type": "code",
      "title": "Dual-Optimized PR Description Generation",
      "language": "bash",
      "code": "# Generate human + AI optimized descriptions\n\necho \"Generate two PR descriptions:\n\n1. HUMAN-OPTIMIZED (GitHub PR body)\n- 1-3 paragraphs max\n- Explain business value\n- List breaking changes\n- Reference related issues\n\n2. AI-OPTIMIZED (PR_REVIEW_CONTEXT.md)\n- Complete technical context\n- File-by-file changes\n- Architectural patterns used\n- Explicit terminology\n\nExplore git history and codebase,\nthen generate both.\" | \\n  agent --task pr-description",
      "caption": "Generate descriptions optimized for each audience",
      "speakerNotes": {
        "talkingPoints": "This demonstrates multiple Lesson 4-5 techniques: sub-agents for context conservation (don't pollute main orchestrator's context), multi-source grounding (explore git history + codebase architecture), and structured output formats. The human description focuses on 'why' and business impact. The AI description provides technical scaffolding: file paths, patterns, explicit constraints.",
        "timing": "3 minutes",
        "discussion": "Walk through the dual-generation approach: Why separate sub-agents? (Token conservation, cleaner separation of concerns) Why both descriptions? (Humans and AI parse differently)",
        "context": "This is unique to Claude Code CLI—spawning specialized agents for specific tasks. Other tools require sequential prompts. Here, the orchestrator can delegate context-intensive work to sub-agents.",
        "transition": "Once you have these dual descriptions, how do you actually review incoming PRs?"
      }
    },
    {
      "type": "concept",
      "title": "Consuming Dual-Optimized PR Descriptions",
      "content": [
        "Read human description first for context",
        "Understand why and business value",
        "Scan for breaking changes upfront",
        "Feed AI description to review assistant",
        "AI-optimized context prevents hallucinations"
      ],
      "speakerNotes": {
        "talkingPoints": "When reviewing a PR with dual descriptions, consume them strategically. Start with the human version—it frames the intent and business context. Then give your AI review assistant the AI-optimized description, which provides comprehensive technical scaffolding. This combination gives AI reviewers the grounding they need for accurate analysis, significantly reducing hallucinations.",
        "timing": "2 minutes",
        "discussion": "Compare to reviewing without AI-optimized context: more hallucinations, more false positives, more time wasted on suggestions that don't fit your architecture.",
        "context": "This ties back to Lesson 5 (Grounding): explicit context prevents errors. The AI-optimized description is your grounding document—it's the difference between 'AI reviewer hallucinates' and 'AI reviewer analyzes accurately.'",
        "transition": "Let's look at the review process itself—how to structure your analysis."
      }
    },
    {
      "type": "code",
      "title": "Review Analysis Pattern",
      "language": "markdown",
      "code": "Think step by step, keep each\nthinking step to 5 words max.\n\nAnalyze:\n1. Logic correctness\n2. Architecture fit\n3. Edge case handling\n4. Code style consistency\n5. Performance implications\n\nReturn final assessment:\n\n#### REVIEW DECISION\n**Summary**: [One sentence verdict]\n**Issues**: [Severity: file:line]\n**Reusability**: [Refactoring opportunities]\n**Decision**: [APPROVE/CHANGES/REJECT]",
      "caption": "Chain of Draft: efficient reasoning with minimal token cost",
      "speakerNotes": {
        "talkingPoints": "This demonstrates Chain of Draft (CoD) from the lesson notes—an optimization of Chain of Thought that maintains structured reasoning while being more efficient. Instead of verbose step-by-step explanations, CoD tells the LLM to think through each step but keep the draft to 5 words max, then return the final assessment. You get the reasoning benefits of CoT with faster response times and lower token consumption.",
        "timing": "2-3 minutes",
        "discussion": "Compare CoD to verbose chain-of-thought explanations: same reasoning quality, half the tokens. This matters when you're reviewing 50+ files.",
        "context": "This is an advanced prompting technique (Lesson 4 optimization) specifically designed for efficiency. The separator (####) signals the final assessment section—clear structure prevents the LLM from burying the verdict in verbose reasoning.",
        "transition": "Let's see the complete flow from code completion to shipping."
      }
    },
    {
      "type": "codeExecution",
      "title": "Complete Code Review Workflow",
      "steps": [
        {
          "line": "Agent completes implementation\nper Lesson 7 plan",
          "highlightType": "human",
          "annotation": "Original planning and execution"
        },
        {
          "line": "LLM predicts: run tests\nto validate execution",
          "highlightType": "prediction",
          "annotation": "Agent recognizes testing is essential"
        },
        {
          "line": "Agent executes: pytest src/\n&& check for failures",
          "highlightType": "execution",
          "annotation": "Deterministic validation"
        },
        {
          "line": "Test results returned:\nall passing vs failures",
          "highlightType": "feedback",
          "annotation": "Objective pass/fail signal"
        },
        {
          "line": "If tests fail, fix and rerun\n(Lesson 8 cycle)",
          "highlightType": "prediction",
          "annotation": "Recovery path"
        },
        {
          "line": "Tests passing: copy code to\nfresh conversation",
          "highlightType": "execution",
          "annotation": "Context isolation for review"
        },
        {
          "line": "Fresh agent analyzes:\nlogic, architecture, style",
          "highlightType": "prediction",
          "annotation": "Objective review without\ndefensiveness"
        },
        {
          "line": "Review findings returned:\nfile:line + severity",
          "highlightType": "feedback",
          "annotation": "Grounded, testable suggestions"
        },
        {
          "line": "Apply fixes and retest\n(catch regressions)",
          "highlightType": "execution",
          "annotation": "Validate that fixes work"
        },
        {
          "line": "Review again in fresh context\nor declare green",
          "highlightType": "prediction",
          "annotation": "Iterate until diminishing\nreturns"
        },
        {
          "line": "Ship when: tests pass +\nno substantive issues",
          "highlightType": "summary",
          "annotation": "Final quality gate"
        }
      ],
      "speakerNotes": {
        "talkingPoints": "This is the complete validate phase from Lesson 3. After execution (Lesson 7) completes and tests (Lesson 8) pass, review happens in isolation. Fresh context prevents bias. Review findings must be grounded (file:line). Fixes are validated with tests to catch regressions. The cycle continues until you reach a green light (tests pass, no substantive issues) or diminishing returns (mostly style nitpicks).",
        "timing": "4-5 minutes",
        "discussion": "Walk through each step and emphasize the decision points: Why fresh context? Why test after every fix? When do we stop iterating?",
        "context": "This connects all of Lesson 3 (four phases) and demonstrates how they interact: Plan → Execute → Test → Validate. Each phase is essential and prevents the next from being skipped.",
        "transition": "Let's wrap up with the core principles that make code review effective."
      }
    },
    {
      "type": "takeaway",
      "title": "Key Takeaways",
      "content": [
        "Review in fresh context to eliminate bias",
        "Tests verify correctness; review catches subtlety",
        "Stop iterating at green light or diminishing returns",
        "Dual-optimize PR descriptions for humans and AI",
        "Grounding (file:line refs) makes review findings testable"
      ],
      "speakerNotes": {
        "talkingPoints": "These five principles capture the essence of effective code review with agents. Fresh context eliminates the defensiveness that makes review worthless. Tests are your objective arbiter—if a suggestion breaks tests, it's wrong. Diminishing returns tells you when further iteration wastes time. Dual-optimized descriptions serve both audiences simultaneously. And grounding ensures review findings are based on actual code, not hallucinations.",
        "timing": "2 minutes",
        "discussion": "Ask: Which of these will change how you review AI-generated code?",
        "context": "These principles apply whether you're using Claude Code, Copilot, or any other AI tool. The core patterns—fresh context, evidence requirements, iteration discipline—are universal.",
        "transition": "Next lesson: Lesson 10: Debugging with AI. We'll cover how to systematically diagnose failures when code doesn't work as expected."
      }
    }
  ]
}