{
  "metadata": {
    "title": "Reviewing Code",
    "lessonId": "lesson-9-reviewing-code",
    "estimatedDuration": "35-45 minutes",
    "learningObjectives": [
      "Review in fresh contexts",
      "Apply iterative review cycles",
      "Generate dual-audience PR descriptions",
      "Leverage AI-assisted PR review"
    ]
  },
  "slides": [
    {
      "type": "title",
      "title": "Reviewing Code",
      "subtitle": "The Validate Phase: Quality Gates Before Shipping",
      "content": [],
      "speakerNotes": {
        "talkingPoints": "This lesson covers the Validate phase from the four-phase workflow. Code review catches probabilistic errors that agents inevitably introduce: subtle logic bugs, architectural mismatches, and edge cases handled incorrectly.",
        "timing": "1 minute",
        "discussion": "Ask students about their current code review practices with AI-generated code.",
        "context": "This follows implementation (Lesson 7) and testing (Lesson 8) in the workflow.",
        "transition": "Let's start with the most critical insight about reviewing AI-generated code."
      }
    },
    {
      "type": "concept",
      "title": "The Fresh Context Principle",
      "content": [
        "Review in a separate context from where code was written",
        "Agents defend decisions in the same conversation",
        "Fresh context enables objective analysis",
        "Prevents confirmation bias from prior choices"
      ],
      "speakerNotes": {
        "talkingPoints": "This is the key insight: LLMs are stateless, but within a conversation they build attachment to their decisions. An agent reviewing its own work will rationalize and defend choices rather than critically analyze them.",
        "timing": "2-3 minutes",
        "discussion": "Have you noticed AI assistants being defensive about code they just wrote? How did you handle it?",
        "context": "This connects to Lessons 1-2 on LLM and agent fundamentals—stateless nature can be leveraged for objectivity.",
        "transition": "Now let's look at the structured review prompt template."
      }
    },
    {
      "type": "code",
      "title": "The Review Prompt Template",
      "language": "markdown",
      "code": "You are an expert code reviewer. Analyze the current changeset and provide a critical review.\n\nThe changes in the working tree were meant to: $DESCRIBE_CHANGES\n\nThink step-by-step through each aspect below, focusing solely on the changes in the working tree.\n\n1. **Architecture & Design**\n   - Verify conformance to project architecture\n   - Check module responsibilities are respected\n   - Ensure changes align with the original intent\n\n2. **Code Quality**\n   - Code must be self-explanatory and readable\n   - Style must match surrounding code patterns\n   - Changes must be minimal - nothing unneeded\n   - Follow KISS principle\n\n3. **Maintainability**\n   - Optimize for future LLM agents working on the codebase\n   - Ensure intent is clear and unambiguous\n   - Verify comments and docs remain in sync with code\n\n4. **User Experience**\n   - Identify areas where extra effort would significantly improve UX\n   - Balance simplicity with meaningful enhancements\n\nReview the changes critically. Focus on issues that matter.\nUse ChunkHound's code research.\nDO NOT EDIT ANYTHING - only review.",
      "caption": "Structured review prompt applying Lesson 4 prompting principles",
      "speakerNotes": {
        "talkingPoints": "This template integrates persona, Chain-of-Thought, structure, and grounding requirements. Each element serves a purpose: persona establishes expertise, numbered sections enforce systematic analysis, and the final constraint prevents the agent from 'fixing' things during review.",
        "timing": "3-4 minutes",
        "discussion": "Which elements from Lesson 4 do you recognize in this prompt? Why is the 'DO NOT EDIT' constraint important?",
        "context": "The $DESCRIBE_CHANGES placeholder connects review to original intent, preventing drift.",
        "transition": "Review is rarely one-pass—let's discuss the iterative cycle."
      }
    },
    {
      "type": "concept",
      "title": "Iterative Review Cycle",
      "content": [
        "Review finds issues → fix → re-run tests → review again",
        "Always review in fresh context (not same conversation)",
        "Review itself is probabilistic—agent can be wrong",
        "Tests are the objective arbiter"
      ],
      "speakerNotes": {
        "talkingPoints": "The review cycle continues until you reach a green light or diminishing returns. Critical insight: the review agent is also making statistical predictions—it might suggest 'fixes' that break working code. Your test suite catches these regressions.",
        "timing": "2-3 minutes",
        "discussion": "Has an AI review ever suggested a 'fix' that broke something? How did you catch it?",
        "context": "This is where Lesson 8's test-first approach pays off—tests validate review suggestions.",
        "transition": "How do we know when to stop iterating?"
      }
    },
    {
      "type": "comparison",
      "title": "When to Ship vs When to Iterate",
      "left": {
        "label": "Stop and Ship",
        "content": [
          "Tests passing + review green",
          "Tests passing + review nitpicking",
          "4+ iterations with minor remaining issues",
          "Agent hallucinating non-existent problems"
        ]
      },
      "right": {
        "label": "Continue Iterating",
        "content": [
          "Tests failing (fix issues first)",
          "Critical architectural violations found",
          "Security vulnerabilities identified",
          "Breaking changes not documented"
        ]
      },
      "neutral": true,
      "speakerNotes": {
        "talkingPoints": "Diminishing returns manifest as nitpicking, hallucinations, review-induced test failures, or excessive iteration cost. At that point, trust your tests as the objective arbiter. The key question: does this issue actually matter in production?",
        "timing": "2-3 minutes",
        "discussion": "How do you distinguish substantive feedback from nitpicking in your current workflow?",
        "context": "This balances perfectionism against shipping velocity—both are valid concerns.",
        "transition": "Now let's move to pull requests—serving two very different audiences."
      }
    },
    {
      "type": "comparison",
      "title": "PR Audiences: Human vs AI Reviewers",
      "left": {
        "label": "Human Reviewers",
        "content": [
          "Scan quickly, infer from context",
          "Value concise summaries (1-3 paragraphs)",
          "Want to understand 'why' and business value",
          "Process holistically at a glance"
        ]
      },
      "right": {
        "label": "AI Review Assistants",
        "content": [
          "Parse content chunk-by-chunk",
          "Struggle with vague pronouns",
          "Need explicit structure and terminology",
          "Require detailed technical context"
        ]
      },
      "neutral": true,
      "speakerNotes": {
        "talkingPoints": "Traditional PR descriptions optimize for one audience or the other. The solution: generate both in a coordinated workflow using sub-agents. This applies the dual-audience principle throughout the review process.",
        "timing": "2-3 minutes",
        "discussion": "Think about your last PR description. Which audience was it optimized for?",
        "context": "This connects to Lesson 5's grounding principles—different consumers need different context formats.",
        "transition": "Let's look at the prompt pattern for generating dual-optimized descriptions."
      }
    },
    {
      "type": "code",
      "title": "PR Description Generator Prompt",
      "language": "markdown",
      "code": "You are a contributor to {PROJECT_NAME} creating a GitHub pull request for the current branch. Using the sub task tool to conserve context, explore the changes in the git history relative to mainstream. Summarize and explain like you would to a fellow co-worker:\n- Direct and concise\n- Professional but conversational\n- Assume competence and intelligence\n- Skip obvious explanations\n\nThe intent of the changes is:\n```\n{CHANGES_DESC}\n```\n\nBuilding upon this, draft two markdown files: one for a human review / maintainer for the project and another complementary that's optimized for the reviewer's agent. Explain:\n- What was done and the reasoning behind it\n- Breaking changes, if any exist\n- What value it adds to the project\n\n**CONSTRAINTS:**\n- The human optimized markdown file should be 1-3 paragraphs max\n- Do NOT state what was done, instead praise where due and focus on what needs to be done\n- Agent optimized markdown should focus on explaining the changes efficiently\n\n**DELIVERABLES:**\n- HUMAN_REVIEW.md - Human optimized review\n- AGENT_REVIEW.md - Agent optimized review",
      "caption": "Generates coordinated PR descriptions for both human and AI reviewers",
      "speakerNotes": {
        "talkingPoints": "Notice the sub-agent instruction for context conservation—exploring 20-30 changed files consumes 40K+ tokens. The sub-agent returns only synthesized findings. Multi-source grounding (ArguSeek + ChunkHound) ensures descriptions reflect actual codebase patterns.",
        "timing": "3-4 minutes",
        "discussion": "Why does the prompt specify 'Using the sub task tool'? What problem does this solve?",
        "context": "This combines techniques from Lessons 4, 5, and 7: persona, grounding, evidence requirements.",
        "transition": "Now let's flip perspectives—how do we review PRs that come with dual descriptions?"
      }
    },
    {
      "type": "concept",
      "title": "Consuming Dual PR Descriptions",
      "content": [
        "Read human description first for 'why' and business value",
        "Form initial mental model of the changeset",
        "Feed AI-optimized description to your review assistant",
        "Validates findings against comprehensive technical context"
      ],
      "speakerNotes": {
        "talkingPoints": "When you're on the receiving end of a PR with dual descriptions, use them strategically. Human-optimized for your quick scan, AI-optimized to feed your AI assistant for accurate analysis with proper grounding.",
        "timing": "2 minutes",
        "discussion": "How would this change your current PR review workflow?",
        "context": "This creates a virtuous cycle: contributors generate structured context, reviewers leverage it efficiently.",
        "transition": "Let's look at the AI-assisted review prompt pattern."
      }
    },
    {
      "type": "code",
      "title": "AI-Assisted PR Review Prompt",
      "language": "markdown",
      "code": "You are ChunkHound's maintainer reviewing {PR_LINK}. Ensure code quality, prevent technical debt, and maintain architectural consistency.\n\n# Review Process\n1. Use `gh` CLI to read the complete PR - all files, commits, comments, related issues.\n2. Think step by step, but only keep a minimum draft for each thinking step, with 5 words at most. End the assessment with a separator ####.\n3. Never speculate about code you haven't read - investigate files before commenting.\n\n# Critical Checks\nBefore approving, verify:\n- Can existing code be extended instead of creating new (DRY)?\n- Does this respect module boundaries and responsibilities?\n- Are there similar patterns elsewhere? Search the codebase.\n- Is this introducing duplication?\n\n# Output Format\n```markdown\n**Summary**: [One sentence verdict]\n**Strengths**: [2-3 items]\n**Issues**: [By severity: Critical/Major/Minor with file:line refs]\n**Reusability**: [Specific refactoring opportunities]\n**Decision**: [APPROVE/REQUEST CHANGES/REJECT]\n```\n\nBelow is the contributor's description of the changes. Do NOT trust it, validate it yourself.\n```\n{AGENT_DESC}\n```",
      "caption": "Two-step review: generate analysis, validate findings, then create output",
      "speakerNotes": {
        "talkingPoints": "Notice step 2 uses Chain of Draft—thinking step by step but keeping drafts to 5 words max. This maintains reasoning benefits while reducing token consumption. The 'Do NOT trust it' instruction forces validation over assumption.",
        "timing": "3-4 minutes",
        "discussion": "Why does the prompt say 'Do NOT trust' the contributor's description? How does this prevent review shortcuts?",
        "context": "After Step 1, review the output for hallucinations before Step 2 generates final files.",
        "transition": "Let's understand Chain of Draft—the technique used in step 2."
      }
    },
    {
      "type": "codeComparison",
      "title": "Chain of Thought vs Chain of Draft",
      "leftCode": {
        "label": "Chain of Thought (CoT)",
        "language": "text",
        "code": "Think step by step through\neach aspect of the code.\nExplain your reasoning\nin detail for each step."
      },
      "rightCode": {
        "label": "Chain of Draft (CoD)",
        "language": "text",
        "code": "Think step by step, but\nonly keep a minimum draft\nfor each thinking step,\nwith 5 words at most.\nEnd with separator ####."
      },
      "neutral": true,
      "speakerNotes": {
        "talkingPoints": "Chain of Draft maintains structured reasoning benefits while being more efficient. Instead of verbose explanations, the LLM keeps concise drafts (5 words max per step) then returns final assessment. Same quality, reduced tokens, faster responses.",
        "timing": "2-3 minutes",
        "discussion": "When would you prefer verbose CoT over efficient CoD? What trade-offs exist?",
        "context": "CoD is particularly useful for reviews where you want reasoning without consuming context space.",
        "transition": "Let's discuss the critical distinction between codebase types."
      }
    },
    {
      "type": "comparison",
      "title": "Agent-Only vs Mixed Codebases",
      "left": {
        "label": "Agent-Only Codebases",
        "content": [
          "Maintained exclusively by AI",
          "Optimize for AI clarity",
          "More explicit type annotations",
          "Review: 'Will an agent understand this later?'"
        ]
      },
      "right": {
        "label": "Mixed Codebases (Most Common)",
        "content": [
          "Human and AI collaboration",
          "Optimize for human brevity",
          "Maintain AI navigability",
          "Add manual review step before committing"
        ]
      },
      "neutral": true,
      "speakerNotes": {
        "talkingPoints": "Most production codebases are mixed. The critical difference: add a manual review step where you fully read and audit AI-generated code before committing. Without explicit project rules, agents generate code following training patterns that may not match your team's style.",
        "timing": "2-3 minutes",
        "discussion": "Is your codebase agent-only or mixed? How does this affect your review process?",
        "context": "Tune your project rules (Lesson 6) to guide agents toward expected writing style.",
        "transition": "Let's wrap up with the key takeaways."
      }
    },
    {
      "type": "takeaway",
      "title": "Key Takeaways",
      "content": [
        "Fresh context prevents confirmation bias",
        "Iterate until green or diminishing",
        "Tests are the objective arbiter",
        "Generate dual-audience PR descriptions",
        "Validate AI suggestions with evidence"
      ],
      "speakerNotes": {
        "talkingPoints": "Review in fresh context for objectivity. Iterate the review cycle but stop at diminishing returns. Trust tests over AI opinions. Generate separate descriptions for human and AI reviewers. Always validate suggestions against actual code.",
        "timing": "2 minutes",
        "discussion": "Which of these practices will you implement first in your workflow?",
        "context": "These practices integrate with the four-phase workflow: Research → Plan → Execute → Validate.",
        "transition": "Next lesson covers debugging with AI—when code doesn't work as expected."
      }
    }
  ]
}