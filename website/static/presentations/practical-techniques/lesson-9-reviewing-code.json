{
  "metadata": {
    "title": "Lesson 9: Reviewing Code",
    "lessonId": "lesson-9-reviewing-code",
    "estimatedDuration": "45-60 minutes",
    "learningObjectives": [
      "Review code in fresh context objectively",
      "Iterate until green or diminishing returns",
      "Optimize PR descriptions for dual audiences",
      "Apply four-phase methodology to validation"
    ]
  },
  "slides": [
    {
      "type": "title",
      "title": "Lesson 9: Reviewing Code",
      "subtitle": "The Validate phase—systematic quality gates before shipping",
      "content": [],
      "speakerNotes": {
        "talkingPoints": "We've implemented, tested, and the agent succeeded. But is the code actually correct? This lesson covers the systematic review phase that catches probabilistic errors agents introduce. We'll focus on why fresh context prevents confirmation bias, how to structure effective reviews, and when to stop iterating.",
        "timing": "1 minute",
        "discussion": "Ask students: Have you ever found subtle bugs weeks after shipping? What made them hard to catch?",
        "context": "This is the Validate phase from Lesson 3's four-phase workflow—the final quality gate.",
        "transition": "Let's start by understanding what code review catches that testing misses."
      }
    },
    {
      "type": "concept",
      "title": "Review: The Validate Phase",
      "content": [
        "Catches probabilistic errors agents introduce",
        "Detects subtle logic bugs and edge cases",
        "Identifies architectural mismatches",
        "Prevents confirmation bias through fresh context",
        "Completes the four-phase workflow (Research→Plan→Execute→Validate)"
      ],
      "speakerNotes": {
        "talkingPoints": "Tests verify that code runs. Reviews verify that code is correct. These are fundamentally different. An agent can write code that compiles, passes tests, and still contains subtle logic errors, incorrect edge case handling, or patterns that don't fit your architecture. The key difference from previous phases: review happens outside the conversation where code was written.",
        "timing": "2-3 minutes",
        "discussion": "What's the difference between code that passes tests and code that's correct? In your experience, what bugs slip through testing?",
        "context": "In production systems, subtle bugs often cost more than catastrophic failures because they're harder to detect. A logic bug in a cache invalidation routine might silently corrupt user data across months.",
        "transition": "The critical insight is where the review happens. Let's explore why fresh context matters."
      }
    },
    {
      "type": "concept",
      "title": "Fresh Context Prevents Confirmation Bias",
      "content": [
        "Agent reviewing own work defends prior decisions",
        "Attachment to previous choices prevents objectivity",
        "Fresh conversation provides stateless analysis",
        "Separates writing context from review context",
        "Same quality of reasoning, zero confirmation bias"
      ],
      "speakerNotes": {
        "talkingPoints": "This is subtle but critical. An agent that reviews its own code in the same conversation will rationalize and defend what it already wrote. It invested tokens explaining why it chose that pattern. Starting fresh—in a new conversation with only the code, not the reasoning—forces the agent to analyze objectively. This mirrors how human code review works: fresh eyes catch what the author misses.",
        "timing": "2-3 minutes",
        "discussion": "Have you ever defended a code choice in review because you wrote it? How did that affect your judgment?",
        "context": "This is the 'stateless nature' concept from Lessons 1-2 applied to quality assurance. The agent has no prior conversation context to defend, so it analyzes purely on evidence.",
        "transition": "Now, the review process differs slightly based on your codebase type. Let's examine agent-only versus mixed codebases."
      }
    },
    {
      "type": "comparison",
      "title": "Codebase Type Affects Review Focus",
      "left": {
        "label": "Agent-Only Codebases",
        "content": [
          "Minimal human code intervention",
          "Optimize style for agent clarity",
          "More explicit type annotations",
          "Verbose architectural documentation",
          "Review: Can an agent understand this later?"
        ]
      },
      "right": {
        "label": "Mixed Codebases (Most Production)",
        "content": [
          "Balance human and AI collaboration",
          "Optimize style for human readability",
          "Maintain agent navigability",
          "Add manual audit step before commit",
          "Review: Does this match team standards?"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "The core engineering standards—DRY, YAGNI, architecture, maintainability—apply everywhere. What differs is style optimization. In agent-only systems, you can be more explicit because agents benefit from clarity. In mixed codebases—which is most of production—you need to tune your project rules (Lesson 6) to guide agents toward human-readable style, then explicitly verify the output before committing. This manual step is non-negotiable in mixed codebases without explicit project rules.",
        "timing": "2-3 minutes",
        "discussion": "Is your codebase agent-only or mixed? What style rules do you currently enforce?",
        "context": "Many teams skip the manual verification step and end up with codebases that gradually shift toward agent preferences rather than human readability standards. This technical debt compounds over quarters.",
        "transition": "Now let's look at the practical review process. I'll show you a prompt template that integrates techniques from Lesson 4."
      }
    },
    {
      "type": "code",
      "title": "The Review Prompt Template",
      "language": "text",
      "code": "You are a code reviewer for our team.\nReview this code for:\n- Logic correctness\n- Edge cases\n- Architectural fit\n- Readability\n- Security concerns\n\nProvide findings with file:line\nreferences. Separate critical issues\nfrom style notes.\n\nStop when you reach confidence\nthat the code is production-ready.",
      "caption": "Applies Lesson 4 techniques: persona, structure, constraints, evidence requirements",
      "speakerNotes": {
        "talkingPoints": "This template integrates multiple prompting principles: it establishes a clear persona ('code reviewer for our team'), specifies what to review (logic, edge cases, architecture, readability, security), demands evidence (file:line references), and sets a stop condition (when confident it's production-ready). Notice what's NOT here—we don't say 'think step by step' because reviews benefit from Chain of Draft (we'll cover that in a moment).",
        "timing": "3-4 minutes",
        "discussion": "Compare this to prompts you currently use. What's missing from your reviews?",
        "context": "This template is adapted from Lesson 4's prompting principles. Each element serves a specific purpose: constraints guide what aspects matter, persona sets tone, evidence requirements ground findings in actual code.",
        "transition": "The review process isn't one-pass. Let's look at the iterative cycle."
      }
    },
    {
      "type": "concept",
      "title": "Iterative Review Cycle",
      "content": [
        "Review in fresh context → identifies issues",
        "Fix issues → run tests (Lesson 8)",
        "Re-review in new conversation",
        "Continue until green or diminishing returns",
        "Tests are the objective arbiter of correctness"
      ],
      "speakerNotes": {
        "talkingPoints": "Code review is probabilistic—the agent can be wrong. It might suggest 'fixes' that break working code. That's why tests are non-negotiable after each review iteration. The pattern is: review finds issues, you fix them, tests must still pass, then re-review in fresh context. The agent won't defend prior choices because it's a new conversation.",
        "timing": "2-3 minutes",
        "discussion": "Have you had a reviewer suggest a 'fix' that broke something? How did you handle it?",
        "context": "In production, regression testing catches these AI-induced bugs, but it's expensive. Better to be disciplined: tests must still pass after any AI-suggested fix.",
        "transition": "But you don't iterate forever. There's a pattern for knowing when to stop."
      }
    },
    {
      "type": "concept",
      "title": "Recognizing Diminishing Returns",
      "content": [
        "Green light: No substantive issues, tests pass → ship",
        "Nitpicking: Trivial style preferences",
        "Hallucinations: Inventing issues that don't exist",
        "Test failures after 'fixes' → review was wrong",
        "Stop when cost of iteration > value of findings"
      ],
      "speakerNotes": {
        "talkingPoints": "There are two end states for iteration: a genuine green light (no substantive issues, tests pass, you're confident shipping) or diminishing returns (further review costs more than it provides and risks quality degradation). Nitpicking is a warning sign—if the agent is suggesting variable renames or minor style tweaks in the 4th iteration, you've hit diminishing returns. Hallucinations are worse: the agent invents architectural issues or patterns that don't actually exist in your codebase. When you see these patterns, trust your tests and ship.",
        "timing": "2-3 minutes",
        "discussion": "Share a story: when have you spent time on code review and gotten diminishing value?",
        "context": "This is operator judgment—the 'art' of the process. There's no algorithm for perfect code, so eventually you stop refining and ship the good-enough version.",
        "transition": "Now let's shift focus. Code review isn't just internal—pull requests serve two very different audiences."
      }
    },
    {
      "type": "comparison",
      "title": "Pull Requests: Two Audiences",
      "left": {
        "label": "Human Reviewers",
        "content": [
          "Scan quickly, infer meaning",
          "Value concise summaries (1-3 paragraphs)",
          "Want to understand the 'why'",
          "Understand vague pronouns and context drift",
          "Decision-making focus"
        ]
      },
      "right": {
        "label": "AI Review Assistants",
        "content": [
          "Parse content chunk-by-chunk",
          "Struggle with vague pronouns",
          "Need explicit structure (Lesson 5)",
          "Require detailed technical context",
          "Evidence-based analysis focus"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "Traditional PR descriptions optimize for one audience or the other. A verbose PR helps an AI assistant but overwhelms human reviewers. A concise PR is easy for humans but gives AI assistants insufficient context for accurate analysis. The solution: generate both in a coordinated workflow. This is where sub-agents (Lesson 5) become powerful—a sub-agent explores git history while the main orchestrator drafts dual-optimized descriptions.",
        "timing": "3-4 minutes",
        "discussion": "Do your PRs work for both human and AI reviewers? Which audience do you currently optimize for?",
        "context": "This is most relevant if you're using AI assistants for code review. Many teams still think PRs are human-only, which limits AI effectiveness.",
        "transition": "Let me show you the advanced prompt pattern that generates these dual-optimized descriptions."
      }
    },
    {
      "type": "codeExecution",
      "title": "PR Generation Workflow: Dual Audiences",
      "steps": [
        {
          "line": "Engineer specifies: 'Generate PR description for this branch'",
          "highlightType": "human",
          "annotation": "Task with dual-audience requirement"
        },
        {
          "line": "LLM predicts: 'I'll use sub-agents for context efficiency'",
          "highlightType": "prediction",
          "annotation": "Decision to preserve context (Lesson 5)"
        },
        {
          "line": "Agent executes: Sub-agent explores git history and architecture",
          "highlightType": "execution",
          "annotation": "Separate context for research (Lesson 5 grounding)"
        },
        {
          "line": "Sub-agent returns: Synthesized findings (commits, patterns, files changed)",
          "highlightType": "feedback",
          "annotation": "Compressed context for main orchestrator"
        },
        {
          "line": "LLM receives findings and predicts: 'I'll generate human summary first'",
          "highlightType": "prediction",
          "annotation": "Structured output planning"
        },
        {
          "line": "Agent executes: Draft 1-3 paragraph human summary (why + business value)",
          "highlightType": "execution",
          "annotation": "Human-optimized output"
        },
        {
          "line": "Agent executes: Draft technical context document (files, patterns, breaking changes)",
          "highlightType": "execution",
          "annotation": "AI-optimized output with explicit structure"
        },
        {
          "line": "Both outputs complete PR review dual-audience requirement",
          "highlightType": "summary",
          "annotation": "Workflow achieves objectives with context conservation"
        }
      ],
      "speakerNotes": {
        "talkingPoints": "Notice the sub-agent technique from Lesson 5: instead of filling the main conversation context with 50 commit diffs, a separate agent explores git history and returns only synthesized findings. This preserves context for the actual description generation. The main orchestrator then uses those findings to draft two versions: one optimized for humans (concise, narrative, business-focused) and one optimized for AI (explicit structure, file:line references, architectural patterns). Both versions inform each other—they're not independent—but they serve different consumers.",
        "timing": "4-5 minutes",
        "discussion": "Have you generated PR descriptions with AI? What happened when you tried to optimize for both audiences?",
        "context": "Sub-agent capability is unique to Claude Code CLI. Other tools require sequential prompts, which breaks the feedback loop between exploring context and drafting descriptions.",
        "transition": "Now let's zoom out to the meta-level: when you're reviewing a PR created with these dual descriptions, how do you consume both?"
      }
    },
    {
      "type": "concept",
      "title": "Reviewing Dual-Optimized PRs",
      "content": [
        "Read human summary first for business context",
        "Feed AI-optimized description to review assistant",
        "AI assistant has full technical grounding for accurate analysis",
        "Reduces hallucinations through explicit structure",
        "Both descriptions inform your decision"
      ],
      "speakerNotes": {
        "talkingPoints": "When you receive a PR with dual descriptions, consume them in order: human summary first to understand the 'why', then feed the AI-optimized context to your review assistant (Copilot, Codex, Claude Code). The AI now has comprehensive, unambiguous technical grounding instead of trying to infer context from vague pronouns. This dramatically improves review accuracy. You're leveraging the same grounding principles from Lesson 5 to improve AI-assisted code review.",
        "timing": "2-3 minutes",
        "discussion": "Do you use AI for code review? How does explicit structure affect its accuracy?",
        "context": "This is practical: teams that skip the explicit technical context step often find their AI review assistants hallucinate architectural issues or suggest inappropriate patterns.",
        "transition": "Let's wrap up with the key takeaways from this lesson."
      }
    },
    {
      "type": "takeaway",
      "title": "Key Takeaways",
      "content": [
        "Review in fresh context to eliminate confirmation bias—stateless analysis beats defensive reasoning",
        "Iterate until green or diminishing returns, not indefinitely—tests are the objective arbiter",
        "Apply the four-phase methodology to review itself—research code intent, plan review structure, execute analysis, validate via tests",
        "Generate dual-optimized PR descriptions for human and AI audiences—concise narrative plus explicit technical structure"
      ],
      "speakerNotes": {
        "talkingPoints": "These four concepts form the essence of systematic code review with AI. Fresh context prevents the agent from defending poor choices. Knowing when to stop prevents costly over-iteration. Applying the four-phase methodology to review itself (not just development) ensures you approach validation systematically. And dual-optimized PRs serve the full spectrum of reviewers. Together, these practices ensure code is correct before shipping.",
        "timing": "2-3 minutes",
        "discussion": "Which of these will have the biggest impact on your team's review process?",
        "context": "The next lesson covers debugging—when review catches issues but you need deeper investigation to understand root causes.",
        "transition": "Let's move to Lesson 10: Debugging with AI. You'll learn how to systematically investigate failures when review or tests surface problems."
      }
    }
  ]
}
