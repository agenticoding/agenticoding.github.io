{
  "metadata": {
    "title": "Reviewing Code",
    "lessonId": "lesson-9-reviewing-code",
    "estimatedDuration": "35-45 minutes",
    "learningObjectives": [
      "Review in fresh context",
      "Apply iterative review cycles",
      "Generate dual-audience PR descriptions",
      "Recognize diminishing review returns"
    ]
  },
  "slides": [
    {
      "type": "title",
      "title": "Reviewing Code",
      "subtitle": "The Validate Phase: Quality Gates Before Shipping",
      "content": [],
      "speakerNotes": {
        "talkingPoints": "This lesson covers the Validate phase from our four-phase workflow. You've implemented code, tests pass, the agent executed your plan—now we need to verify it's actually correct. Code review catches probabilistic errors that agents inevitably introduce.",
        "timing": "1 minute",
        "discussion": "How many of you have shipped agent-generated code that later revealed subtle bugs?",
        "context": "This is the final quality gate before committing. Without systematic review, you're trusting statistical predictions blindly.",
        "transition": "Let's start with the fundamental principle that makes AI code review effective."
      }
    },
    {
      "type": "concept",
      "title": "The Fresh Context Principle",
      "content": [
        "Review in a fresh context, separate from implementation",
        "Agents defend decisions in the same conversation",
        "Stateless agents analyze objectively without attachment",
        "Prevents confirmation bias from prior choices"
      ],
      "speakerNotes": {
        "talkingPoints": "The key insight for effective AI code review: always review in a fresh context. This leverages the stateless nature of LLMs from Lessons 1 and 2. An agent reviewing its own work in the same conversation will rationalize and defend its decisions. A fresh context provides objective analysis.",
        "timing": "2-3 minutes",
        "discussion": "Why do you think agents defend their decisions in the same context? Think about how context window affects predictions.",
        "context": "This is similar to why human developers benefit from code review by others—fresh eyes catch what you've become blind to.",
        "transition": "With this principle in mind, let's look at the review prompt template."
      }
    },
    {
      "type": "code",
      "title": "The Review Prompt Template",
      "language": "markdown",
      "code": "You are an expert code reviewer. Analyze the current changeset and provide a critical review.\n\nThe changes in the working tree were meant to: $DESCRIBE_CHANGES\n\nThink step-by-step through each aspect below, focusing solely on the changes in the working tree.\n\n1. **Architecture & Design**\n   - Verify conformance to project architecture\n   - Check module responsibilities are respected\n   - Ensure changes align with the original intent\n\n2. **Code Quality**\n   - Code must be self-explanatory and readable\n   - Style must match surrounding code patterns\n   - Changes must be minimal - nothing unneeded\n   - Follow KISS principle\n\n3. **Maintainability**\n   - Optimize for future LLM agents working on the codebase\n   - Ensure intent is clear and unambiguous\n   - Verify comments and docs remain in sync with code\n\n4. **User Experience**\n   - Identify areas where extra effort would significantly improve UX\n   - Balance simplicity with meaningful enhancements\n\nReview the changes critically. Focus on issues that matter.\nUse ChunkHound's code research.\nDO NOT EDIT ANYTHING - only review.",
      "caption": "Structured review prompt applying Lesson 4 techniques",
      "speakerNotes": {
        "talkingPoints": "This template integrates techniques from Lesson 4. Notice the persona, Chain-of-Thought instruction, structured categories, and explicit constraint at the end. The $DESCRIBE_CHANGES placeholder grounds the review in your actual intent.",
        "timing": "3-4 minutes",
        "discussion": "Which elements from Lesson 4 do you recognize here? Why is the 'DO NOT EDIT' constraint important?",
        "context": "The constraint prevents the agent from making changes during review—separation of concerns between analysis and modification.",
        "transition": "A single review pass is rarely enough. Let's talk about iterative review."
      }
    },
    {
      "type": "concept",
      "title": "Iterative Review Cycle",
      "content": [
        "Review → Fix → Re-test → Review again (fresh context)",
        "Review itself is probabilistic—agents can be wrong",
        "Tests are the objective arbiter",
        "Continue until green light or diminishing returns"
      ],
      "speakerNotes": {
        "talkingPoints": "Code review is rarely one-pass. First review finds issues, you fix them, re-run tests to catch regressions, then review again in a fresh context. The agent might suggest 'fixes' that break working code—your test suite catches this.",
        "timing": "2-3 minutes",
        "discussion": "What's the difference between a review-induced regression and a legitimate bug fix?",
        "context": "In production, I've seen review suggestions that looked reasonable but broke edge cases the tests caught. Trust your tests.",
        "transition": "Let's be specific about what diminishing returns looks like."
      }
    },
    {
      "type": "comparison",
      "title": "When to Ship vs When to Iterate",
      "left": {
        "label": "Keep Iterating",
        "content": [
          "Critical logic bugs identified",
          "Architecture violations found",
          "Security issues flagged",
          "Tests failing after changes"
        ]
      },
      "right": {
        "label": "Ship It",
        "content": [
          "Tests passing + review green",
          "Only trivial style nitpicks remain",
          "Agent hallucinating non-existent issues",
          "4+ iterations for minor issues"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "This is where operator judgment becomes essential. Left side: legitimate issues worth fixing. Right side: signs of diminishing returns. The agent inventing problems or suggesting fixes that break tests means it's time to trust your tests and ship.",
        "timing": "2-3 minutes",
        "discussion": "Have you experienced an AI review cycle that went on too long? What were the signs?",
        "context": "Excessive review iterations cost tokens, time, and risk degrading quality. Know when to stop.",
        "transition": "Now let's shift to pull requests—they serve two very different audiences."
      }
    },
    {
      "type": "comparison",
      "title": "Two Audiences for PR Descriptions",
      "left": {
        "label": "Human Reviewers",
        "content": [
          "Scan quickly, infer from context",
          "Value concise summaries (1-3 paragraphs)",
          "Want the 'why' and business value",
          "Process holistically at a glance"
        ]
      },
      "right": {
        "label": "AI Review Assistants",
        "content": [
          "Parse chunk-by-chunk sequentially",
          "Need explicit structure and terminology",
          "Require file paths and architectural context",
          "Struggle with vague pronouns"
        ]
      },
      "neutral": true,
      "speakerNotes": {
        "talkingPoints": "PRs serve two fundamentally different audiences that process information differently. Traditional PR descriptions optimize for one or the other—too verbose for humans, too vague for AI. The solution: generate both in a coordinated workflow.",
        "timing": "2-3 minutes",
        "discussion": "Think about a recent PR you reviewed. Was the description optimized for you or for an AI assistant?",
        "context": "This dual-audience challenge is increasingly common as teams adopt AI review assistants like GitHub Copilot.",
        "transition": "Let's look at a prompt pattern that generates both descriptions."
      }
    },
    {
      "type": "code",
      "title": "PR Description Generator Prompt",
      "language": "markdown",
      "code": "You are a contributor to {PROJECT_NAME} creating a GitHub pull request for the current branch. Using the sub task tool to conserve context, explore the changes in the git history relative to mainstream. Summarize and explain like you would to a fellow co-worker:\n- Direct and concise\n- Professional but conversational\n- Assume competence and intelligence\n- Skip obvious explanations\n\nThe intent of the changes is:\n```\n{CHANGES_DESC}\n```\n\nBuilding upon this, draft two markdown files: one for a human review / maintainer for the project and another complementary that's optimized for the reviewer's agent. Explain:\n- What was done and the reasoning behind it\n- Breaking changes, if any exist\n- What value it adds to the project\n\n**CONSTRAINTS:**\n- The human optimized markdown file should be 1-3 paragraphs max\n- Do NOT state what was done, instead praise where due and focus on what needs to be done\n- Agent optimized markdown should focus on explaining the changes efficiently\n\n**DELIVERABLES:**\n- HUMAN_REVIEW.md - Human optimized review\n- AGENT_REVIEW.md - Agent optimized review",
      "caption": "Generates dual-audience PR descriptions in one workflow",
      "speakerNotes": {
        "talkingPoints": "This prompt demonstrates multiple techniques: sub-agents for context conservation, multi-source grounding, structured output requirements. The 'sub task tool' spawns a separate agent for git exploration, preventing context overflow.",
        "timing": "3-4 minutes",
        "discussion": "Why is the sub-task instruction critical for large PRs with many changed files?",
        "context": "Without sub-agents, exploring 20-30 changed files consumes 40K+ tokens, pushing constraints into the attention curve's ignored middle.",
        "transition": "Now let's look at the receiving end—how to review PRs with AI assistance."
      }
    },
    {
      "type": "concept",
      "title": "Consuming Dual PR Descriptions",
      "content": [
        "Read human description first for 'why' and business value",
        "Form initial mental model of the changeset",
        "Feed AI-optimized description to your review assistant",
        "AI gets comprehensive context for accurate analysis"
      ],
      "speakerNotes": {
        "talkingPoints": "When you receive a PR with dual descriptions, use both strategically. The human description orients you quickly. The AI-optimized description provides the grounding your AI assistant needs to avoid hallucinations.",
        "timing": "2 minutes",
        "discussion": "How does having structured context change the quality of AI-assisted reviews?",
        "context": "The AI-optimized description reduces the need for the review agent to explore and infer—it has explicit paths and patterns.",
        "transition": "Let's examine the review prompt pattern for analyzing PRs."
      }
    },
    {
      "type": "code",
      "title": "AI-Assisted PR Review Prompt",
      "language": "markdown",
      "code": "You are ChunkHound's maintainer reviewing {PR_LINK}. Ensure code quality, prevent technical debt, and maintain architectural consistency.\n\n# Review Process\n1. Use `gh` CLI to read the complete PR - all files, commits, comments, related issues.\n2. Think step by step, but only keep a minimum draft for each thinking step, with 5 words at most. End the assessment with a separator ####.\n3. Never speculate about code you haven't read - investigate files before commenting.\n\n# Critical Checks\nBefore approving, verify:\n- Can existing code be extended instead of creating new (DRY)?\n- Does this respect module boundaries and responsibilities?\n- Are there similar patterns elsewhere? Search the codebase.\n- Is this introducing duplication?\n\n# Output Format\n```markdown\n**Summary**: [One sentence verdict]\n**Strengths**: [2-3 items]\n**Issues**: [By severity: Critical/Major/Minor with file:line refs]\n**Reusability**: [Specific refactoring opportunities]\n**Decision**: [APPROVE/REQUEST CHANGES/REJECT]\n```\n\nStart by executing `gh pr view {PR_LINK} --comments`, follow with the Code Research tool for codebase understanding.",
      "caption": "Two-step review: generate analysis, validate, then create output",
      "speakerNotes": {
        "talkingPoints": "Notice step 2: 'Think step by step, but only keep a minimum draft for each thinking step, with 5 words at most.' This is Chain of Draft—an efficiency optimization over Chain of Thought that maintains reasoning quality with fewer tokens.",
        "timing": "3-4 minutes",
        "discussion": "Why does the prompt explicitly say 'Never speculate about code you haven't read'?",
        "context": "Evidence requirements force grounding—the agent must read actual code before commenting, reducing hallucinated issues.",
        "transition": "Let's examine the Chain of Draft technique more closely."
      }
    },
    {
      "type": "comparison",
      "title": "Chain of Thought vs Chain of Draft",
      "left": {
        "label": "Chain of Thought (CoT)",
        "content": [
          "Verbose step-by-step explanations",
          "Full reasoning written out",
          "Higher token consumption",
          "Longer response times"
        ]
      },
      "right": {
        "label": "Chain of Draft (CoD)",
        "content": [
          "Minimal drafts per step (5 words max)",
          "Same reasoning benefits",
          "Reduced token consumption",
          "Faster responses"
        ]
      },
      "neutral": true,
      "speakerNotes": {
        "talkingPoints": "Chain of Draft maintains structured reasoning while being more efficient. The instruction 'think step by step, but only keep a minimum draft' triggers the same logical breakdown as CoT but with compressed output. Final assessment comes after the #### separator.",
        "timing": "2-3 minutes",
        "discussion": "When would you prefer verbose CoT over efficient CoD?",
        "context": "For reviews where you need to understand the agent's reasoning, CoT is better. For production throughput, CoD wins.",
        "transition": "Let's discuss a critical distinction for your codebase."
      }
    },
    {
      "type": "comparison",
      "title": "Agent-Only vs Mixed Codebases",
      "left": {
        "label": "Agent-Only Codebases",
        "content": [
          "Maintained exclusively by AI",
          "Optimize for AI clarity",
          "More explicit type annotations",
          "Review: 'Will an agent understand this later?'"
        ]
      },
      "right": {
        "label": "Mixed Codebases",
        "content": [
          "Human and AI collaboration",
          "Balance human brevity with AI navigability",
          "Most production codebases",
          "Add manual review before committing"
        ]
      },
      "neutral": true,
      "speakerNotes": {
        "talkingPoints": "The same engineering standards apply to both—DRY, YAGNI, architecture. What differs is coding style optimization. Most production codebases are mixed, requiring an additional manual review step to ensure human readability.",
        "timing": "2-3 minutes",
        "discussion": "Which type is your current project? How does that affect your review process?",
        "context": "Without explicit project rules, agents generate code following training patterns that may not match your team's style.",
        "transition": "Let's wrap up with the key principles for code review."
      }
    },
    {
      "type": "takeaway",
      "title": "Key Takeaways",
      "content": [
        "Fresh context prevents confirmation bias",
        "Iterate until green or diminishing",
        "Generate dual-audience PR descriptions",
        "Chain of Draft optimizes reasoning",
        "Tests arbitrate review disagreements"
      ],
      "speakerNotes": {
        "talkingPoints": "Five principles to remember: Review in fresh context for objectivity. Iterate the review cycle but recognize diminishing returns. Generate descriptions for both human and AI reviewers. Use Chain of Draft for efficient reasoning. When review suggestions break tests, trust your tests.",
        "timing": "2-3 minutes",
        "discussion": "Which of these principles will most change how you approach code review tomorrow?",
        "context": "These techniques apply whether you're reviewing your own agent-generated code or PRs from others using AI assistance.",
        "transition": "Next lesson covers Debugging with AI—what happens when things go wrong despite careful review."
      }
    }
  ]
}