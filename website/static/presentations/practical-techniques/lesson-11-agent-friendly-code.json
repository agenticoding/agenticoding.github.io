{
  "metadata": {
    "title": "Agent-Friendly Code",
    "lessonId": "lesson-11-agent-friendly-code",
    "estimatedDuration": "30-40 minutes",
    "learningObjectives": [
      "Understand pattern compounding effects",
      "Co-locate related constraints",
      "Write agent-critical section comments",
      "Break negative feedback loops"
    ]
  },
  "slides": [
    {
      "type": "title",
      "title": "Agent-Friendly Code",
      "subtitle": "Patterns compound—control the direction",
      "content": [],
      "speakerNotes": {
        "talkingPoints": "This lesson covers how agents amplify existing code patterns—both good and bad. We'll learn practical techniques to write code that agents work with effectively, and understand why code review is your critical control point.",
        "timing": "1 minute",
        "discussion": "Ask: How many of you have noticed AI generating code that matches existing (sometimes bad) patterns in your codebase?",
        "context": "Research shows AI-generated code contains 8x more duplicated blocks than human-written code—not because AI creates duplication, but because it amplifies existing patterns.",
        "transition": "Let's start by understanding the compounding mechanism that drives this behavior."
      }
    },
    {
      "type": "concept",
      "title": "The Core Problem",
      "content": [
        "Agents amplify patterns—good or bad",
        "AI code has 8x more duplicated blocks than human code",
        "Agents don't create duplication—they copy what exists",
        "Every accepted PR becomes pattern context for future agents"
      ],
      "speakerNotes": {
        "talkingPoints": "GitClear's analysis of 211 million lines of code revealed the 8x duplication stat. This isn't AI being lazy—it's pattern matching working exactly as designed. When agents grep for examples, they find your existing code and use it as templates.",
        "timing": "2 minutes",
        "discussion": "What's the implication? If your codebase is clean, agents generate clean code. If it's messy, agents generate mess.",
        "context": "This creates exponential quality curves—upward or downward. You control the direction through what patterns you allow to exist.",
        "transition": "But pattern amplification isn't the only source of quality drift..."
      }
    },
    {
      "type": "visual",
      "title": "Quality Compounding Over Iterations",
      "component": "CompoundQualityVisualization",
      "caption": "Accepted patterns compound exponentially—upward or downward.",
      "speakerNotes": {
        "talkingPoints": "This visualization shows how quality diverges over iterations. With good patterns, each iteration builds on quality. With bad patterns, technical debt compounds. The curves diverge exponentially, not linearly.",
        "timing": "2-3 minutes",
        "discussion": "At iteration 1, the difference between good and bad paths seems small. By iteration 5, it's dramatic. Where in your current projects are you on these curves?",
        "context": "The key insight: early decisions have outsized impact. Accepting one bad pattern in iteration 1 creates the template for 5 more in iteration 3.",
        "transition": "Let's break down the two distinct sources of quality drift."
      }
    },
    {
      "type": "comparison",
      "title": "Two Sources of Quality Drift",
      "left": {
        "label": "Copy Machine Effect",
        "content": [
          "Predictable amplification of existing patterns",
          "Agent finds duplication → creates more duplication",
          "Missing tests in similar files → generates without tests",
          "Inconsistent error handling → produces inconsistency"
        ]
      },
      "right": {
        "label": "The Dice Roll",
        "content": [
          "Random probabilistic errors (stochastic)",
          "References functions that don't exist",
          "Imports from non-existent files",
          "Complexity increases error probability"
        ]
      },
      "neutral": true,
      "speakerNotes": {
        "talkingPoints": "The Copy Machine Effect is predictable—show messy patterns, get messy output. The Dice Roll is inherent to LLMs as probabilistic systems. Even with perfect code, random errors occur. You can't prevent dice roll errors with better prompts.",
        "timing": "3 minutes",
        "discussion": "Why does this distinction matter? Because the solution differs. Copy Machine Effect: fix your patterns. Dice Roll: accept that errors will happen and catch them in review.",
        "context": "Both feed the same exponential curve. A random hallucinated API call in iteration 1 becomes a copied pattern by iteration 3 if you accept it.",
        "transition": "Your critical role is breaking the cycle through code review. Now let's look at practical patterns to write agent-friendly code."
      }
    },
    {
      "type": "concept",
      "title": "Co-locate Related Constraints",
      "content": [
        "Agents only see code they explicitly find via search",
        "Scattered constraints = agent misses critical rules",
        "Co-location ensures constraints appear in same search",
        "When separation required: use semantic bridges (comments)"
      ],
      "speakerNotes": {
        "talkingPoints": "From Lesson 5, agents discover code through agentic search—Grep, Read, Glob. When an agent searches for 'createUser', it reads that file. If password validation lives in a separate config file, the agent never sees it.",
        "timing": "2 minutes",
        "discussion": "Think about your current codebase. Where are related constraints scattered across files? What could an agent miss?",
        "context": "This is why 'separation of concerns' sometimes works against AI agents. The concerns are separated for human organization, but agents search for specific functions.",
        "transition": "Let's see a concrete example of scattered vs co-located constraints."
      }
    },
    {
      "type": "codeComparison",
      "title": "Co-location: Scattered vs Co-located",
      "leftCode": {
        "label": "Scattered (Agent Misses)",
        "language": "typescript",
        "code": "// File: services/auth.ts\nfunction createUser(email: string, password: string) {\n  return db.users.insert({ email, password: hashPassword(password) })\n}\n\n// File: config/validation.ts\nconst MIN_PASSWORD_LENGTH = 12  // ← Agent never searches for this file"
      },
      "rightCode": {
        "label": "Co-located (Agent Sees)",
        "language": "typescript",
        "code": "// File: services/auth.ts\nconst MIN_PASSWORD_LENGTH = 12  // ← Agent sees this in same file\n\nfunction createUser(email: string, password: string) {\n  if (password.length < MIN_PASSWORD_LENGTH) {\n    throw new ValidationError(`Password must be ${MIN_PASSWORD_LENGTH}+ chars`)\n  }\n  return db.users.insert({ email, password: hashPassword(password) })\n}"
      },
      "speakerNotes": {
        "talkingPoints": "In the scattered version, agent searches Grep('createUser'), reads auth.ts, and generates code accepting 3-character passwords because it never found the config file. In the co-located version, the constraint appears in the same search result.",
        "timing": "3 minutes",
        "discussion": "This isn't about removing config files entirely. It's about ensuring critical constraints are visible when agents search for related code.",
        "context": "When separation is truly required (DRY principle), use semantic bridges—explicit comments pointing to related files.",
        "transition": "Sometimes we can't co-locate everything. That's when we use comments as agent guidance."
      }
    },
    {
      "type": "concept",
      "title": "Comments as Agent-Critical Sections",
      "content": [
        "For high-risk code: auth, crypto, payments, PII",
        "Write comments as prompts with imperative directives",
        "Use NEVER, MUST, ALWAYS to create deliberate friction",
        "Overuse is counterproductive—signal becomes noise"
      ],
      "speakerNotes": {
        "talkingPoints": "For genuinely high-risk code, write comments that function as agent instructions. Imperative directives create friction that makes agents pause and consider carefully. But use sparingly—if everything is marked CRITICAL, the protection loses meaning.",
        "timing": "2 minutes",
        "discussion": "What areas of your codebase would benefit from agent-critical comments? What areas would be counterproductive?",
        "context": "This is about creating semantic barriers for the most sensitive code paths, not documenting everything.",
        "transition": "Let's see examples of effective agent-critical comments."
      }
    },
    {
      "type": "code",
      "title": "Agent-Critical Comment Example",
      "language": "typescript",
      "code": "// SECURITY-CRITICAL: Payment token handling\n// NEVER log token values, even in debug mode\n// NEVER store tokens in local storage or cookies\n// MUST use secure memory and clear after use\n// See: docs/security/payment-compliance.md\nfunction processPaymentToken(token: SecureToken): Result {\n  // Implementation with security constraints\n}",
      "caption": "Imperative directives create deliberate friction for agents modifying sensitive code",
      "speakerNotes": {
        "talkingPoints": "Notice the structure: category label, imperative directives (NEVER, MUST), and a reference for more context. When an agent encounters this during code research, these directives become part of its context for any modifications.",
        "timing": "2-3 minutes",
        "discussion": "Why imperative voice? Because agents respond to clear instructions. 'Consider security' is vague. 'NEVER log token values' is unambiguous.",
        "context": "This protection mechanism guards sensitive code from accidental modification. But remember: overuse dilutes the signal.",
        "transition": "Now let's discuss a common anti-pattern: caching extracted knowledge."
      }
    },
    {
      "type": "comparison",
      "title": "Knowledge Cache Anti-Pattern",
      "left": {
        "label": "Bad: Cache Extracted Knowledge",
        "content": [
          "Save ARCHITECTURE.md from agent research",
          "Commit extracted patterns to docs/",
          "Create summary files of codebase structure",
          "Cache becomes stale immediately after code changes"
        ]
      },
      "right": {
        "label": "Good: Dynamic Extraction",
        "content": [
          "Use code research tools (Explore, ChunkHound)",
          "Extract knowledge dynamically on-demand",
          "Source code is single source of truth",
          "Always fresh, never stale"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "The moment you commit extracted knowledge, every code change requires documentation updates you'll forget. Future agents find BOTH current code AND outdated cache, causing confusion. Code research tools extract knowledge dynamically every time.",
        "timing": "3 minutes",
        "discussion": "How many of you have ARCHITECTURE.md or similar files that are out of date? How confident are you in their accuracy?",
        "context": "Document decisions and WHY (ADRs, high-level overviews, business concepts), not extracted WHAT that code research can regenerate on demand.",
        "transition": "Let's visualize what happens when you cache knowledge vs extract dynamically."
      }
    },
    {
      "type": "codeExecution",
      "title": "Knowledge Caching Creates Stale Data",
      "steps": [
        {
          "line": "Agent: Research codebase architecture",
          "highlightType": "prediction",
          "annotation": "Agent begins code research phase"
        },
        {
          "line": "Agent executes: Read source code files",
          "highlightType": "execution",
          "annotation": "Extracts current knowledge from code"
        },
        {
          "line": "Knowledge extracted to context",
          "highlightType": "feedback",
          "annotation": "Agent understands architecture"
        },
        {
          "line": "❌ Bad path: Agent saves ARCHITECTURE.md",
          "highlightType": "execution",
          "annotation": "Cache committed to repository"
        },
        {
          "line": "Agent executes: Edit code (makes changes)",
          "highlightType": "execution",
          "annotation": "Code changes, cache now stale"
        },
        {
          "line": "Future agent spawns, researches codebase",
          "highlightType": "prediction",
          "annotation": "New agent needs architecture info"
        },
        {
          "line": "Finds BOTH: current code AND outdated cache",
          "highlightType": "feedback",
          "annotation": "Conflicting information causes confusion"
        },
        {
          "line": "✅ Good path: Extract dynamically each time",
          "highlightType": "summary",
          "annotation": "Source code = single source of truth"
        }
      ],
      "speakerNotes": {
        "talkingPoints": "The bad path shows how knowledge caching creates an impossible cache invalidation problem. The cache is accurate at creation time, but becomes stale with every code change. Future agents find conflicting information.",
        "timing": "3 minutes",
        "discussion": "Why is cache invalidation so hard? Because you'd need to update docs with every code change. Who remembers to do that?",
        "context": "Trust the grounding process (Lesson 5) to re-extract knowledge on-demand. Dynamic extraction is always fresh.",
        "transition": "Let's bring this all together with key takeaways."
      }
    },
    {
      "type": "takeaway",
      "title": "Key Takeaways",
      "content": [
        "Patterns compound exponentially—control direction",
        "Co-locate constraints for agent visibility",
        "Use agent-critical comments sparingly",
        "Code review breaks negative cycles",
        "Extract knowledge dynamically, never cache"
      ],
      "speakerNotes": {
        "talkingPoints": "Remember: agents amplify patterns AND produce stochastic errors. Both feed the same exponential curve. Your code review is the circuit breaker that prevents negative compounding. Co-locate constraints, use agent-critical comments for truly sensitive code, and trust dynamic extraction over cached documentation.",
        "timing": "2-3 minutes",
        "discussion": "What's one change you'll make to your codebase this week based on what we covered?",
        "context": "Every piece of code you accept today becomes pattern context for tomorrow's agents. The quality curve is in your hands.",
        "transition": "In the next lesson, we'll explore more advanced techniques for working effectively with AI coding assistants."
      }
    }
  ]
}