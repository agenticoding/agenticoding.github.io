{
  "metadata": {
    "title": "Lesson 8: Tests as Guardrails",
    "lessonId": "lesson-8-tests-as-guardrails",
    "estimatedDuration": "40-50 minutes",
    "learningObjectives": [
      "Use tests as constraint systems",
      "Write tests in fresh contexts",
      "Build sociable test practices",
      "Diagnose failures systematically"
    ]
  },
  "slides": [
    {
      "type": "title",
      "title": "Tests as Guardrails",
      "subtitle": "Using tests to constrain agent behavior at scale",
      "content": [],
      "speakerNotes": {
        "talkingPoints": "Agents can refactor half your codebase in minutes. This velocity is powerful but dangerous—small logic errors compound fast. Tests are your constraint system that prevents agents from crossing operational boundaries. They also serve as living documentation agents read during the research phase.",
        "timing": "1 minute",
        "discussion": "Ask engineers: How many times have you had an AI assistant make changes that seemed correct but broke something subtle?",
        "context": "This lesson builds on Lesson 7's planning methodology. We'll apply that same research-plan-execute-validate pattern to testing.",
        "transition": "Let's start by understanding what agents actually do when they encounter your tests."
      }
    },
    {
      "type": "concept",
      "title": "Tests as Grounding Constraints",
      "content": [
        "Tests are concrete constraints agents can't cross",
        "They show OAuth users skip email verification",
        "They reveal timezone offset handling for dates",
        "They document that negative quantities are rejected",
        "Bad tests pollute context with noise"
      ],
      "speakerNotes": {
        "talkingPoints": "When tests exist in the context window before refactoring, implementation decisions become grounded in your actual codebase rather than statistical patterns from training data. A test named 'should reject negative quantities' is a concrete constraint. A test named 'works()' with unclear assertions is noise that wastes context. At scale, when 50 tests load into context before an auth refactor, their quality determines whether the agent's implementation follows your constraints or completes patterns from training.",
        "timing": "3-4 minutes",
        "discussion": "Ask: Have you seen tests that made your code behavior clear? What made them effective? Conversely, what made bad tests confusing?",
        "context": "Real scenario: An agent refactors your auth system and finds 12 tests about email verification. Each test documents a real constraint—OAuth users skip email, admin users bypass verification, deleted users are rejected. These constraints ground the implementation.",
        "transition": "Now let's explore how to discover what should be tested before writing code."
      }
    },
    {
      "type": "concept",
      "title": "Research First: Edge Case Discovery",
      "content": [
        "Ask questions before writing tests",
        "Agents search for function implementations",
        "Read existing edge cases and constraints",
        "Build grounded list from actual code",
        "Identify gaps in test coverage"
      ],
      "speakerNotes": {
        "talkingPoints": "Before writing tests, use the planning techniques from Lesson 7—ask questions to discover what needs testing. These questions load implementation details and existing edge cases into context. The agent searches for the function, reads implementation, finds existing tests, and synthesizes findings. This is more effective than asking 'what should we test?' which generates generic test ideas from training data.",
        "timing": "2-3 minutes",
        "discussion": "Ask: What questions would you ask about a payment processing function before writing tests? (e.g., 'What happens with zero amount?', 'How are currency conversions handled?')",
        "context": "Example: Before testing a quantity discount system, ask: 'What quantities trigger bulk pricing? How are fractional quantities handled? What about zero or negative values?' These questions reveal the business rules embedded in existing code.",
        "transition": "Once you understand edge cases, you need to write code and tests separately to avoid a critical problem..."
      }
    },
    {
      "type": "concept",
      "title": "The Closed Loop Problem: Goodhart's Law",
      "content": [
        "When code and tests are generated together",
        "Both inherit the same flawed assumptions",
        "Agent creates code, then writes tests for it",
        "Tests verify the implementation, not correctness",
        "Green tests don't guarantee working software"
      ],
      "speakerNotes": {
        "talkingPoints": "When code and tests are generated in the same context—same conversation, shared reasoning state—they inherit the same assumptions and blind spots. For example, an agent implements an API endpoint accepting zero product quantities, then generates tests verifying that adding zero items succeeds. Both stem from the same flawed reasoning: neither questioned whether quantities must be positive. The test passes, the bug remains. Goodhart's Law states: 'When a measure becomes a target, it ceases to be a good measure.' Here, agents optimize for passing tests rather than correctness.",
        "timing": "3-4 minutes",
        "discussion": "Ask: Have you ever written tests after code and found yourself making the tests match the implementation rather than the requirements? How did that feel?",
        "context": "Real issue: Specification gaming. Agents find shortcuts to achieve green checkmarks. They weaken assertions or find workarounds. At scale across a large codebase, this compounds—you get weaker assertions, buggy implementations, and green CI pipelines validating the wrong behavior.",
        "transition": "The solution is the three-context workflow—separating these concerns into fresh contexts..."
      }
    },
    {
      "type": "visual",
      "title": "The Three-Context Workflow",
      "component": "ThreeContextWorkflow",
      "caption": "Separate contexts prevent agents from defending flawed assumptions.",
      "speakerNotes": {
        "talkingPoints": "Apply the same research-plan-execute-validate methodology from Lesson 7 to each step: writing code, writing tests, and triaging failures. The critical difference is using fresh contexts for each step. Context A writes code grounded in existing patterns. Context B writes tests grounded in requirements—the agent doesn't remember writing the implementation, so tests derive independently. Context C triages failures objectively—the agent doesn't know who wrote code or tests, providing unbiased root cause analysis.",
        "timing": "4-5 minutes",
        "discussion": "Ask: Why would testing in the same context as implementation be a problem? What would the agent be defending?",
        "context": "Enterprise validation: Salesforce reduced debugging time 30% using this workflow with automated root cause analysis. The stateless nature from Lessons 1-2 is the key—each context is a fresh start with no prior commitment to defend.",
        "transition": "Now let's look at the specific practices that make tests effective guardrails at scale..."
      }
    },
    {
      "type": "comparison",
      "title": "Heavily Mocked Tests vs Sociable Tests",
      "left": {
        "label": "Heavily Mocked",
        "content": [
          "Stubs all dependencies (findByEmail, verify, create)",
          "Verifies implementation details, not behavior",
          "Passes even when agent breaks actual code",
          "False confidence—doesn't catch real breakage",
          "Fast but fragile"
        ]
      },
      "right": {
        "label": "Sociable Tests",
        "content": [
          "Uses real database queries and implementations",
          "Exercises actual code paths for internal code",
          "Uses real password hashing and session tokens",
          "Fails immediately if any part breaks",
          "Catches silent logic errors at scale"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "Sociable unit tests and narrow integration tests use real implementations for internal code—only mock external systems (APIs, third-party services that cost money or require credentials). Example: In authentication testing, a heavily mocked test stubs findByEmail(), verify(), and create()—three separate mocks. The test passes even if an agent breaks all three implementations. A sociable test uses real database queries, real password hashing, and real session tokens. If the agent breaks any part of the flow, the test fails immediately.",
        "timing": "3-4 minutes",
        "discussion": "Ask: Why would mocking internal code create false confidence? What happens when you only mock external systems?",
        "context": "Rule of thumb: Mock Stripe (costs money). Use real test database (fast and verifies actual behavior). Testing Without Mocks advocates for 'Nullables'—production code with an off-switch—for in-memory infrastructure testing without complex mock setups.",
        "transition": "But if tests are slow, engineers won't run them. Let's talk about keeping feedback tight..."
      }
    },
    {
      "type": "concept",
      "title": "Smoke Tests: Fast Feedback Loop",
      "content": [
        "Build sub-30-second smoke test suite",
        "Cover critical junctions only: auth, core journey, database",
        "Skip exhaustive coverage—save for full suite",
        "Run after each agent task immediately",
        "Catch failures while context is fresh"
      ],
      "speakerNotes": {
        "talkingPoints": "A 10-minute comprehensive test suite is useless for iterative agent development—you'll skip running it until the end when debugging becomes expensive. Build a sub-30-second smoke test suite covering critical junctions only: core user journey, authentication boundaries, and database connectivity. Run smoke tests after each task to catch failures immediately while context is fresh, rather than making 20 changes before discovering which one broke the system. Jeremy Miller notes: use 'the finest grained mechanism that tells you something important.' Reserve edge cases, detailed validation, and UI rendering details for the full test suite.",
        "timing": "2-3 minutes",
        "discussion": "Ask: If you had to deploy code based on test results in 10 minutes, which tests would you run? Those are your smoke tests.",
        "context": "Codify this in your project's AGENTS.md or CLAUDE.md (Lesson 6) so agents automatically run smoke tests after completing each task without explicit reminders. Some teams use GitHub status checks that block PRs if smoke tests fail, but full suite failures only warn.",
        "transition": "Tests also help agents discover problems humans don't expect. That's where autonomous testing comes in..."
      }
    },
    {
      "type": "concept",
      "title": "Autonomous Testing: Agent Simulation",
      "content": [
        "Deterministic tests verify known requirements",
        "Agent simulation discovers unknown edge cases",
        "Agents explore state spaces non-deterministically",
        "Run same script twice → different paths each iteration",
        "Unreliable for CI/CD, excellent for discovery"
      ],
      "speakerNotes": {
        "talkingPoints": "AI agents can simulate actual user behavior by giving them a task and tools—browser automation (MCP servers), CLI access, API clients. Like a human tester exploring your application, the agent navigates, clicks, fills forms, and observes results. The critical difference: agents make probabilistic decisions. Run the same test script twice, and the agent explores different paths each time. One iteration tests the happy path, another accidentally discovers a race condition by clicking rapidly, a third stumbles onto an edge case with unicode characters you never considered. This randomness is unreliable for regression testing—you can't guarantee the agent will exercise the same code paths in CI/CD. But it's excellent for discovery.",
        "timing": "3-4 minutes",
        "discussion": "Ask: What edge cases would you discover if an AI agent randomly explored your product for an hour? What would it find that deterministic tests miss?",
        "context": "Real scenario: An agent testing an e-commerce site non-deterministically discovers that adding 999,999 items to a cart causes an integer overflow in the price calculation. A human tester wouldn't think to try that. Now you solidify this finding into a deterministic regression test.",
        "transition": "Once you've caught failures, the question is: is this a test problem or a code problem? That's where systematic diagnosis comes in..."
      }
    },
    {
      "type": "code",
      "title": "Diagnostic Prompt: Structured Failure Analysis",
      "language": "markdown",
      "code": "Run the failing test:\n\n```\nnpm test -- Auth.test.ts\n```\n\nUse the code research to analyze the\ntest failure above.\n\nDIAGNOSE:\n\n1. Examine the test code and its assertions\n2. Understand and clearly explain the\n   intention and reasoning of the test\n3. Compare against the implementation code\n   being tested\n4. Identify the root cause of failure\n\nDETERMINE:\nIs this a test that needs updating or\na real bug in the implementation?\n\nProvide your conclusion with evidence.",
      "caption": "Chain-of-thought diagnosis prevents jumping to conclusions.",
      "speakerNotes": {
        "talkingPoints": "This diagnostic prompt applies techniques from Lesson 4: Chain-of-Thought sequential steps, constraints requiring evidence, and structured format. The fenced code block preserves error formatting. 'Use the code research' forces codebase search instead of hallucination. The numbered DIAGNOSE steps implement CoT—the agent can't jump to 'root cause' without examining test intent first. Step 2 ('Understand the intention') ensures the agent articulates WHY the test exists, not just WHAT it does. The DETERMINE binary decision constrains output to 'bug vs outdated test' instead of open-ended conclusions. Finally, 'Provide evidence' requires file paths and line numbers, not vague assertions.",
        "timing": "3-4 minutes",
        "discussion": "Ask: Why does requiring evidence (file paths, line numbers) matter? What happens if you skip that step?",
        "context": "You can adapt this pattern for performance issues, security vulnerabilities, or deployment failures by changing the diagnostic steps while preserving the structure: sequential CoT → constrained decision → evidence requirement. Same pattern, different domain.",
        "transition": "Let's recap the core principles that make testing effective with agents..."
      }
    },
    {
      "type": "takeaway",
      "title": "Key Takeaways",
      "content": [
        "Tests ground agents in your actual codebase constraints, not training data patterns",
        "Use separate contexts for code, tests, and debugging to prevent specification gaming",
        "Write sociable tests that exercise real code paths; only mock external systems",
        "Build sub-30-second smoke tests for immediate feedback during agent iteration",
        "Systematic diagnosis—research, examine intent, compare, determine root cause—solves most failures"
      ],
      "speakerNotes": {
        "talkingPoints": "These five principles form the foundation of effective testing with AI agents. Tests become guardrails—defining operational boundaries agents cannot cross—when they're well-written, grounded in reality, and kept close to the development loop. The three-context workflow ensures agents don't defend flawed assumptions. Sociable tests catch real breakage. Smoke tests keep feedback tight. And systematic diagnosis prevents you from accepting the first explanation when failures occur.",
        "timing": "2-3 minutes",
        "discussion": "Ask: Which of these principles feels most critical for your team right now? Where do you want to start?",
        "context": "Next lesson (Lesson 9) covers code review practices—how to evaluate agent-generated code before it reaches production. These testing practices set the foundation for effective review workflows.",
        "transition": "That concludes Lesson 8. Let's move into Lesson 9: Reviewing Code."
      }
    }
  ]
}