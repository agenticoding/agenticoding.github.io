{
  "metadata": {
    "title": "Tests as Guardrails",
    "lessonId": "lesson-8-tests-as-guardrails",
    "estimatedDuration": "35-45 minutes",
    "learningObjectives": [
      "Tests ground agent code quality",
      "Prevent cycle of self-deception",
      "Smoke tests enable fast iteration",
      "Agent simulation discovers edge cases"
    ]
  },
  "slides": [
    {
      "type": "title",
      "title": "Tests as Guardrails",
      "subtitle": "Constraining Agent Velocity with Living Documentation",
      "content": [],
      "speakerNotes": {
        "talkingPoints": "This lesson covers how tests serve as both constraints and documentation for AI agents. We'll explore how to prevent agents from breaking production code at scale, discover edge cases through autonomous testing, and debug failures systematically.",
        "timing": "1 minute",
        "discussion": "Ask students: Have you experienced an AI agent refactoring code successfully but breaking subtle edge cases?",
        "context": "Agents can modify dozens of files in minutes. Without tests, small logic errors compound fast.",
        "transition": "Let's start by understanding why tests matter at agent velocity..."
      }
    },
    {
      "type": "concept",
      "title": "The Velocity Problem",
      "content": [
        "Agents refactor 30+ files in minutes",
        "2,000-line diffs cause pattern blindness",
        "Manual review misses subtle logic errors",
        "Small bugs compound at scale"
      ],
      "speakerNotes": {
        "talkingPoints": "Agents operate at velocity humans can't match. When an agent refactors 30 files in one session, unintended changes hide in massive diffs. You review 2,000 lines, develop pattern blindness, and might miss 2 files with subtle logic errors among 28 correct ones. Tests cement which behaviors are intentional and must not change.",
        "timing": "2-3 minutes",
        "discussion": "Have students share experiences reviewing large AI-generated diffs. What got missed?",
        "context": "In production, an agent might 'simplify' away critical rounding logic buried in a 50-line file change. Without tests, this ships to production.",
        "transition": "Tests aren't just verification—they're documentation agents actually read during their research phase..."
      }
    },
    {
      "type": "concept",
      "title": "Tests as Documentation: What Agents Read",
      "content": [
        "Tests load into context during research phase",
        "Good tests show concrete constraints and edge cases",
        "Bad tests pollute context with noise",
        "Tests ground implementation in your codebase"
      ],
      "speakerNotes": {
        "talkingPoints": "Before agents write code, they research—searching for relevant files and reading implementations. Both source code and tests load into context. Tests aren't documentation agents 'learn' from; they're concrete constraints that ground implementation in your actual codebase rather than statistical patterns from training data. When 50 tests load into context before an auth refactor, their quality determines grounding.",
        "timing": "3 minutes",
        "discussion": "Ask: What makes a test 'good documentation'? What information do test names and assertions convey?",
        "context": "Tests named 'test(works)' with unclear assertions fill context with noise. Tests showing 'OAuth users skip email verification' provide concrete constraints.",
        "transition": "Let's look at how to discover what needs testing using agent questions..."
      }
    },
    {
      "type": "code",
      "title": "Research First: Discover Edge Cases Through Questions",
      "language": "text",
      "code": "How does validateUser() work? What edge cases exist in the current implementation?\nWhat special handling exists for different auth providers?\nSearch for related tests and analyze what they cover.",
      "caption": "Prompt pattern for edge case discovery",
      "speakerNotes": {
        "talkingPoints": "Before writing tests, use planning techniques from Lesson 7 to discover what needs testing. This prompt shows the pattern: ask questions that load implementation details and existing edge cases into context. The agent searches for the function, reads implementation, finds existing tests, and synthesizes findings—loading concrete constraints like 'OAuth users skip email verification' or 'admin users bypass rate limits' into context.",
        "timing": "3 minutes",
        "discussion": "What other questions would help discover edge cases for a payment processing function?",
        "context": "This grounded approach derives edge cases from actual code, not generic testing advice.",
        "transition": "Once you've discovered edge cases, follow up to identify gaps in test coverage..."
      }
    },
    {
      "type": "code",
      "title": "Follow Up: Identify Coverage Gaps",
      "language": "text",
      "code": "Based on the implementation you found, what edge cases are NOT covered by tests?\nWhat happens with:\n- Null or undefined inputs\n- Users mid-registration (incomplete profile)\n- Concurrent validation requests",
      "caption": "Prompt pattern for gap analysis",
      "speakerNotes": {
        "talkingPoints": "This follow-up prompt guides the agent to analyze the implementation against specific questions and identify untested paths. Notice the structured format with specific scenarios. The agent produces a grounded list of edge cases derived from actual code, not hypothetical scenarios.",
        "timing": "2 minutes",
        "discussion": "What edge case categories apply universally? (null/undefined, concurrency, partial state, error conditions)",
        "context": "This two-step research approach (discover implementation, then identify gaps) ensures comprehensive test coverage.",
        "transition": "Now we'll address a critical failure mode when agents generate both code and tests in the same context..."
      }
    },
    {
      "type": "concept",
      "title": "The Cycle of Self-Deception",
      "content": [
        "Code and tests in same context inherit identical assumptions",
        "Agent implements bug, then writes passing test for buggy behavior",
        "Example: Accept zero quantities, test verifies zero succeeds",
        "Specification gaming: optimize for green tests, not correctness"
      ],
      "speakerNotes": {
        "talkingPoints": "When code and tests are generated within the same context—the same conversation, shared reasoning state—they inherit the same assumptions and blind spots. An agent might implement an API endpoint that accepts zero quantities, then generate tests verifying that adding zero items succeeds. Both artifacts stem from the same flawed reasoning. The test passes, the bug remains. This is Goodhart's Law: when tests become the optimization target, agents optimize for passing tests rather than correctness.",
        "timing": "3-4 minutes",
        "discussion": "Ask students: Have you seen tests that validate incorrect behavior? How did it happen?",
        "context": "Research shows this occurs in ~1% of test-code generation cycles, but compounds across large codebases. Enterprise systems at Salesforce validate this pattern.",
        "transition": "The solution requires a circuit breaker that prevents convergence on mutually-compatible-but-incorrect artifacts..."
      }
    },
    {
      "type": "visual",
      "title": "The Three-Context Workflow",
      "component": "ThreeContextWorkflow",
      "caption": "Fresh contexts prevent assumption inheritance and enable objective analysis",
      "speakerNotes": {
        "talkingPoints": "The solution uses fresh contexts for each step: Write code in Context A, write tests in fresh Context B, triage failures in fresh Context C. This leverages statelessness from Lessons 1-2—the agent doesn't carry assumptions or defend prior decisions between contexts. In Context B, the agent doesn't remember writing implementation, so tests derive independently from requirements. In Context C, the agent doesn't know who wrote code or tests, providing objective analysis.",
        "timing": "4 minutes",
        "discussion": "Why does fresh context matter? What assumptions get broken when the agent 'forgets' writing the code?",
        "context": "Salesforce reduced debugging time 30% using automated root cause analysis for millions of daily test runs with this approach.",
        "transition": "Each context follows the same four-phase workflow from Lesson 7: Research, Plan, Execute, Validate. Now let's look at test design patterns..."
      }
    },
    {
      "type": "comparison",
      "title": "Sociable Tests vs Heavy Mocking",
      "left": {
        "label": "Heavy Mocking",
        "content": [
          "Stubs findByEmail(), verify(), create()",
          "Verifies function calls, not behavior",
          "Passes even when implementations break",
          "False confidence at scale"
        ]
      },
      "right": {
        "label": "Sociable Tests",
        "content": [
          "Uses real database queries and password hashing",
          "Exercises actual code paths",
          "Fails immediately when agent breaks logic",
          "Mocks only external systems (Stripe, APIs)"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "Tests with heavy mocking give false confidence. They verify implementation details (function calls) rather than behavior. Sociable unit tests and narrow integration tests use real implementations for internal code. For authentication testing, a heavily mocked test passes even when an agent breaks all three implementations. A sociable test uses real database queries, real password hashing, and real session tokens—if the agent breaks any part of the flow, the test fails. Mock only what costs money or requires external credentials.",
        "timing": "3-4 minutes",
        "discussion": "What's the trade-off? When would you use mocks? (External APIs, third-party services, infrastructure you don't control)",
        "context": "Testing Without Mocks advocates for 'Nullables'—production code with an off switch—for in-memory infrastructure testing.",
        "transition": "Sociable tests catch breakage, but only if you actually run them. Let's talk about fast feedback..."
      }
    },
    {
      "type": "concept",
      "title": "Fast Feedback with Smoke Tests",
      "content": [
        "Build sub-30-second smoke test suite",
        "Cover critical junctions only (auth, core journey, DB)",
        "Run after each task during iteration",
        "Codify in AGENTS.md or CLAUDE.md"
      ],
      "speakerNotes": {
        "talkingPoints": "A 10-minute comprehensive suite is useless for iterative agent development—you'll skip running it until the end when debugging becomes expensive. Build a sub-30-second smoke test suite covering critical junctions only. Run smoke tests after each task to catch failures immediately while context is fresh, rather than making 20 changes before discovering which one broke the system. Reserve edge cases and detailed validation for the full test suite. Codify this practice in your project's AGENTS.md so agents automatically run smoke tests without explicit reminders.",
        "timing": "2-3 minutes",
        "discussion": "What belongs in smoke tests vs comprehensive suites? How do you decide the boundary?",
        "context": "As Jeremy Miller notes, use 'the finest grained mechanism that tells you something important.' Smoke tests prevent compounding errors during rapid iteration.",
        "transition": "Deterministic tests verify known requirements. Now let's explore autonomous testing for discovering unknown edge cases..."
      }
    },
    {
      "type": "concept",
      "title": "Autonomous Agent Testing: Complementary Strategies",
      "content": [
        "Deterministic tests: verify known requirements, reliable in CI/CD",
        "Agent simulation: discover unknown edge cases, non-deterministic exploration",
        "Workflow: agents discover, deterministic tests solidify",
        "Connect agents via MCP servers (browser, mobile, desktop)"
      ],
      "speakerNotes": {
        "talkingPoints": "AI agents can simulate actual user behavior by giving them a task and tools to interact with your product—browser automation, CLI access, API clients. The critical difference: agents explore state spaces non-deterministically. Run the same test script twice, and the agent explores different paths each time. This randomness is unreliable for regression testing but excellent for discovery—finding edge cases, state machine bugs, and input validation gaps you didn't think to write. Use agents for discovery, then solidify findings into deterministic tests.",
        "timing": "3-4 minutes",
        "discussion": "What kinds of bugs are hard to find with deterministic tests but easy with randomized exploration? (Race conditions, timing bugs, creative inputs)",
        "context": "MCP servers give agents 'eyes and hands': Chrome DevTools MCP (Google), Playwright MCP (Microsoft), mobile-mcp for iOS/Android, and emerging desktop automation.",
        "transition": "When tests fail, you need systematic diagnosis. Let's look at the agent-driven debug cycle..."
      }
    },
    {
      "type": "code",
      "title": "The Diagnostic Prompt Pattern",
      "language": "markdown",
      "code": "```\n$FAILURE_DESCRIPTION\n```\n\nUse the code research to analyze the test failure above.\n\nDIAGNOSE:\n1. Examine the test code and its assertions.\n2. Understand and clearly explain the intention and reasoning of the test - what is it testing?\n3. Compare against the implementation code being tested\n4. Identify the root cause of failure\n\nDETERMINE:\nIs this a test that needs updating or a real bug in the implementation?\n\nProvide your conclusion with evidence.",
      "caption": "Chain-of-Thought diagnostic pattern with evidence constraints",
      "speakerNotes": {
        "talkingPoints": "This diagnostic prompt applies techniques from Lesson 4: Chain-of-Thought sequential steps, constraints requiring evidence, and structured format. The fenced code block preserves error formatting. 'Use the code research' forces codebase search instead of hallucination. DIAGNOSE numbered steps implement CoT—can't jump to 'root cause' without examining test intent first. 'Understand the intention' ensures the agent articulates WHY the test exists. DETERMINE constrains output to binary decision. 'Provide evidence' requires file paths and line numbers, not vague assertions.",
        "timing": "4 minutes",
        "discussion": "Why does step 2 ('understand the intention') matter? What happens if you skip it?",
        "context": "You can adapt this for performance issues, security vulnerabilities, or deployment failures by changing diagnostic steps while preserving the structure: sequential CoT, constrained decision, evidence requirement.",
        "transition": "Let's wrap up with the key takeaways from this lesson..."
      }
    },
    {
      "type": "takeaway",
      "title": "Key Takeaways",
      "content": [
        "Tests ground agent implementation decisions",
        "Use three separate contexts always",
        "Sociable tests catch real breakage",
        "Agent simulation discovers unknown edges"
      ],
      "speakerNotes": {
        "talkingPoints": "First, tests are documentation agents actually read—they load into context during research and provide concrete constraints. Second, use separate contexts for code, tests, and debugging to prevent the cycle of self-deception. Third, write sociable tests that use real implementations for internal code, mocking only external systems. Fourth, autonomous testing discovers edge cases through non-deterministic exploration—use it for discovery, then solidify into deterministic tests. Remember: green tests don't equal working software—run the actual product yourself.",
        "timing": "3 minutes",
        "discussion": "Which takeaway will change your workflow immediately? What will you implement first?",
        "context": "Next lesson covers code review patterns for agent-generated code. These testing patterns provide the foundation for effective review.",
        "transition": "Questions before we move to Lesson 9?"
      }
    }
  ]
}
