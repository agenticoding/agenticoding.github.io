{
  "metadata": {
    "title": "Tests as Guardrails",
    "lessonId": "lesson-8-tests-as-guardrails",
    "estimatedDuration": "35-45 minutes",
    "learningObjectives": [
      "Write tests agents read",
      "Apply three-context workflow",
      "Build fast smoke suites",
      "Diagnose failures systematically"
    ]
  },
  "slides": [
    {
      "type": "title",
      "title": "Tests as Guardrails",
      "subtitle": "Constraining agent behavior through living documentation",
      "content": [],
      "speakerNotes": {
        "talkingPoints": "AI agents can refactor half your codebase in minutes. They'll rename functions, restructure modules, and update dozens of files—all while you grab coffee. This velocity is powerful, but dangerous. Small logic errors compound fast when changes happen at scale. Tests are your constraint system.",
        "timing": "1 minute",
        "discussion": "Ask: How many of you have had an agent make a change that broke something unexpected?",
        "context": "This lesson connects the planning methodology from Lesson 7 to testing practices. Tests aren't just verification—they're documentation agents actually read.",
        "transition": "Let's start with what agents actually see when they research your codebase..."
      }
    },
    {
      "type": "concept",
      "title": "Tests as Documentation Agents Read",
      "content": [
        "Agents search and read tests during research phase",
        "Tests load into context window before implementation",
        "Good tests ground decisions in your codebase",
        "Bad tests pollute context with noise"
      ],
      "speakerNotes": {
        "talkingPoints": "Before agents write code, they research—searching for relevant files and reading implementations. Both source code and tests load into the context window. Tests show OAuth users skip email verification, dates handle timezone offsets, negative quantities are rejected. Tests named 'test works' with unclear assertions fill context with noise.",
        "timing": "2-3 minutes",
        "discussion": "What makes a test 'good documentation' versus noise? Think about test names, assertions, and comments.",
        "context": "When 50 tests load into context before an auth refactor, their quality determines whether the agent's implementation is grounded in your constraints or completes patterns from training.",
        "transition": "Before writing tests, we need to discover what actually needs testing..."
      }
    },
    {
      "type": "codeComparison",
      "title": "Edge Case Discovery: Two-Step Pattern",
      "leftCode": {
        "label": "Step 1: Discover",
        "language": "text",
        "code": "How does validateUser() work?\nWhat edge cases exist in the\ncurrent implementation?\nWhat special handling exists for\ndifferent auth providers?\nSearch for related tests and\nanalyze what they cover."
      },
      "rightCode": {
        "label": "Step 2: Identify Gaps",
        "language": "text",
        "code": "Based on the implementation you\nfound, what edge cases are NOT\ncovered by tests?\nWhat happens with:\n- Null or undefined inputs\n- Users mid-registration\n- Concurrent validation requests"
      },
      "speakerNotes": {
        "talkingPoints": "This two-step pattern grounds test generation in actual code rather than generic advice. Step 1 loads concrete constraints into context: OAuth users skip email verification, admin users bypass rate limits. Step 2 analyzes implementation against your questions to identify untested paths.",
        "timing": "3-4 minutes",
        "discussion": "What domain-specific edge cases would you add for your systems? Payment processing? Data transformation? API endpoints?",
        "context": "The agent searches for the function, reads implementation, finds existing tests, and synthesizes findings. You get a grounded list of edge cases derived from actual code.",
        "transition": "Now let's address the biggest trap in agent-generated testing..."
      }
    },
    {
      "type": "concept",
      "title": "The Cycle of Self-Deception",
      "content": [
        "Same context = same blind spots",
        "Agent implements API accepting zero quantities",
        "Same agent generates test verifying zero succeeds",
        "Test passes, bug remains undetected",
        "~1% of cycles exhibit specification gaming"
      ],
      "speakerNotes": {
        "talkingPoints": "When code and tests are generated within the same context, they inherit the same assumptions and blind spots. The agent might implement an endpoint accepting zero product quantities, then generate tests verifying that adding zero items succeeds. Both artifacts stem from flawed reasoning.",
        "timing": "2-3 minutes",
        "discussion": "Has anyone experienced this? Agent writes code and tests that both pass but miss the actual requirement?",
        "context": "This is Goodhart's Law in action: when tests become the optimization target, agents optimize for passing tests rather than correctness. The solution requires a circuit breaker.",
        "transition": "The solution leverages the stateless nature of LLMs we covered in earlier lessons..."
      }
    },
    {
      "type": "visual",
      "title": "The Three-Context Workflow",
      "component": "ThreeContextWorkflow",
      "caption": "Fresh contexts prevent shared blind spots between code, tests, and debugging.",
      "speakerNotes": {
        "talkingPoints": "Use fresh contexts for each step. Context A: Write code—research patterns, plan, execute. Context B: Write tests—agent doesn't remember writing implementation, tests derive independently from requirements. Context C: Triage failures—objective analysis without knowing who wrote what.",
        "timing": "3-4 minutes",
        "discussion": "Why does statelessness help here? What assumptions carry over when using the same context?",
        "context": "Salesforce reduced debugging time 30% using automated root cause analysis for millions of daily test runs. This approach scales.",
        "transition": "Let's talk about what makes tests effective at catching agent mistakes..."
      }
    },
    {
      "type": "comparison",
      "title": "Sociable Tests vs Heavy Mocking",
      "left": {
        "label": "Heavy Mocking",
        "content": [
          "Stubs findByEmail(), verify(), create()",
          "Verifies function calls, not behavior",
          "Passes when agent breaks implementations",
          "False confidence in green tests"
        ]
      },
      "right": {
        "label": "Sociable Tests",
        "content": [
          "Real database queries and password hashing",
          "Exercises actual code paths",
          "Fails when agent breaks any part",
          "Mock only external systems (Stripe, APIs)"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "Tests with heavy mocking give false confidence. They verify implementation details rather than behavior. Sociable tests use real implementations for internal code. Mock Stripe because it costs money. Use real test database because it's fast and verifies actual behavior.",
        "timing": "3-4 minutes",
        "discussion": "Where do you draw the line? What's 'internal code' vs 'external system' in your architecture?",
        "context": "Testing Without Mocks advocates for 'Nullables'—production code with an off switch—for in-memory infrastructure testing without complex mock setups.",
        "transition": "Fast feedback is critical when agents iterate rapidly..."
      }
    },
    {
      "type": "concept",
      "title": "Fast Feedback with Smoke Tests",
      "content": [
        "Build sub-30-second smoke suite",
        "Cover critical junctions only",
        "Run after each agent task",
        "Catch failures while context is fresh",
        "Codify in AGENTS.md or CLAUDE.md"
      ],
      "speakerNotes": {
        "talkingPoints": "A 10-minute comprehensive suite is useless for iterative agent development. Run smoke tests after each task to catch failures immediately. As Jeremy Miller notes, use 'the finest grained mechanism that tells you something important.' Reserve edge cases for full suite.",
        "timing": "2-3 minutes",
        "discussion": "What's in your smoke suite? Authentication, database connectivity, core user journey—what else?",
        "context": "Codify this practice in your project's AGENTS.md so agents automatically run smoke tests after completing each task without requiring explicit reminders.",
        "transition": "Now let's look at how agents can help discover edge cases you missed..."
      }
    },
    {
      "type": "comparison",
      "title": "Complementary Testing Strategies",
      "neutral": true,
      "left": {
        "label": "Deterministic Tests",
        "content": [
          "Verify known requirements",
          "Reliable in CI/CD pipelines",
          "Catch regressions on changes",
          "Document expected behavior"
        ]
      },
      "right": {
        "label": "Agent Simulation",
        "content": [
          "Discover unknown edge cases",
          "Explore unexpected user journeys",
          "Find race conditions and timing bugs",
          "Simulate creative/adversarial users"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "Deterministic tests verify known requirements. Agent simulation discovers unknown edge cases. Both are essential. LLM-based agents make probabilistic decisions, exploring different paths each run. One iteration tests happy path, another discovers a race condition by clicking rapidly.",
        "timing": "3 minutes",
        "discussion": "How would you integrate non-deterministic testing into your pipeline? Discovery phase only?",
        "context": "The workflow: Use agents for discovery, then solidify findings into deterministic tests. Agents explore the unknown; deterministic tests prevent backsliding on the known.",
        "transition": "When tests fail, we need a systematic approach to diagnosis..."
      }
    },
    {
      "type": "code",
      "title": "Test Failure Diagnosis Prompt",
      "language": "text",
      "code": "```\n$FAILURE_DESCRIPTION\n```\n\nUse the code research to analyze the\ntest failure above.\n\nDIAGNOSE:\n\n1. Examine the test code and its assertions.\n2. Understand and clearly explain the\n   intention and reasoning of the test -\n   what is it testing?\n3. Compare against the implementation\n   code being tested\n4. Identify the root cause of failure\n\nDETERMINE:\nIs this a test that needs updating or\na real bug in the implementation?\n\nProvide your conclusion with evidence.",
      "caption": "Chain-of-thought diagnosis with evidence requirements",
      "speakerNotes": {
        "talkingPoints": "This prompt applies techniques from Lesson 4: Chain-of-Thought sequential steps, constraints requiring evidence, and structured format. Fenced code block preserves error formatting. 'Use the code research' is an explicit grounding directive. DIAGNOSE steps force sequential analysis.",
        "timing": "3-4 minutes",
        "discussion": "Why is step 2 (understand intention) critical? What happens if you skip straight to 'identify root cause'?",
        "context": "The binary DETERMINE decision constrains output to 'bug vs outdated test' instead of open-ended conclusions. 'Provide evidence' requires file paths and line numbers—concrete proof, not vague assertions.",
        "transition": "Let's look at why green tests don't always mean working software..."
      }
    },
    {
      "type": "concept",
      "title": "Green Tests ≠ Working Software",
      "content": [
        "Tests verify logic, not UX",
        "Real-world usability gaps remain",
        "Tests catch ~80% of issues",
        "Human verification catches remaining 20%",
        "Run the actual product yourself"
      ],
      "speakerNotes": {
        "talkingPoints": "Tests are essential but not sufficient. They verify business logic and constraints. They don't verify that the UX makes sense, that error messages are helpful, or that the flow feels right. You must run the actual product.",
        "timing": "2 minutes",
        "discussion": "What have you caught in manual testing that tests missed? UI issues? Confusing flows?",
        "context": "In production, the 20% that tests miss often includes the most impactful user-facing issues. Balance automation with human judgment.",
        "transition": "Let's wrap up with the key principles to remember..."
      }
    },
    {
      "type": "takeaway",
      "title": "Key Takeaways",
      "content": [
        "Tests ground agent implementations",
        "Separate contexts prevent deception",
        "Sociable tests catch breakage",
        "Smoke suites enable iteration",
        "Agent simulation discovers unknowns"
      ],
      "speakerNotes": {
        "talkingPoints": "Tests are documentation agents actually read—write tests that explain 'why'. Use three-context workflow to prevent blind spots. Build sociable tests with real implementations. Create sub-30-second smoke suites for fast feedback. Use agent simulation for edge case discovery.",
        "timing": "2 minutes",
        "discussion": "Which of these practices would have the biggest impact on your current projects?",
        "context": "These practices scale. Salesforce reduced debugging time 30% with automated root cause analysis across millions of daily test runs.",
        "transition": "Next lesson covers code review practices for agent-generated code."
      }
    }
  ]
}