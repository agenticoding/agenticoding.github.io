{
  "metadata": {
    "title": "Lesson 8: Tests as Guardrails",
    "lessonId": "lesson-8-tests-as-guardrails",
    "estimatedDuration": "45-60 minutes",
    "learningObjectives": [
      "Ground agent implementation with tests",
      "Separate code, test, debug contexts",
      "Write sociable tests, not mocks",
      "Diagnose failures systematically"
    ]
  },
  "slides": [
    {
      "type": "title",
      "title": "Tests as Guardrails",
      "subtitle": "Grounding agent behavior at scale",
      "content": [],
      "speakerNotes": {
        "talkingPoints": "This lesson covers how tests function as constraints that guide agent behavior at scale. We'll explore tests as documentation, discuss the critical importance of context separation when writing code and tests, and learn systematic approaches to debugging failures.",
        "timing": "1 minute",
        "discussion": "Ask students: What problems have you seen when agents generate code without test guidance?",
        "context": "This is a practical lesson focused on production workflows. Tests aren't just verification—they're the primary way agents learn about your codebase's constraints, edge cases, and domain-specific rules.",
        "transition": "Let's start by understanding what agents actually read when they encounter tests."
      }
    },
    {
      "type": "concept",
      "title": "Tests as Living Documentation",
      "content": [
        "Good tests ground agent decisions in real constraints",
        "Bad tests pollute context with noise",
        "Agents read test names, assertions, and comments",
        "Quality of tests determines quality of implementation"
      ],
      "speakerNotes": {
        "talkingPoints": "Tests aren't passive documentation—agents actively search for and read them during the research phase. A test that clearly shows 'negative quantities are rejected' teaches the agent a concrete constraint. A test named 'test(\"works\")' teaches nothing. The tests that load into context window during research directly influence implementation decisions.",
        "timing": "3-4 minutes",
        "discussion": "Show an example of a poorly named test. Ask: What would an agent infer from this? Then show a well-named test with clear assertions. The difference is dramatic.",
        "context": "In production, test quality determines whether agents implement features correctly or just make assumptions based on training data patterns.",
        "transition": "Before writing tests, we need to discover what should be tested. Let's look at the discovery process."
      }
    },
    {
      "type": "codeComparison",
      "title": "Edge Case Discovery: Weak vs Strong",
      "leftCode": {
        "label": "Vague Discovery",
        "language": "text",
        "code": "What edge cases exist in validateUser()?"
      },
      "rightCode": {
        "label": "Grounded Discovery",
        "language": "text",
        "code": "How does validateUser() work? What edge cases exist\n  in the current implementation?\nWhat special handling exists for different auth providers?\nSearch for related tests and analyze what they cover."
      },
      "speakerNotes": {
        "talkingPoints": "The left side asks a generic question—the agent might hallucinate generic auth edge cases. The right side is specific: it asks the agent to read the actual implementation, find real tests, and synthesize findings. This loads concrete constraints into context derived from YOUR codebase, not from training patterns.",
        "timing": "2-3 minutes",
        "discussion": "Ask: Why is 'Search for related tests and analyze' different from just asking 'what are edge cases'? The answer: it grounds discovery in actual code.",
        "context": "This discovery phase is the foundation for all subsequent testing. When you skip this step, agents generate tests that pass but don't capture real constraints.",
        "transition": "Once you've discovered edge cases, you follow up with deeper analysis to identify gaps."
      }
    },
    {
      "type": "codeComparison",
      "title": "Gap Analysis: Finding Untested Paths",
      "leftCode": {
        "label": "Generic Follow-up",
        "language": "text",
        "code": "What edge cases should we test?"
      },
      "rightCode": {
        "label": "Implementation-Grounded Follow-up",
        "language": "text",
        "code": "Based on the implementation you found,\n  what edge cases are NOT covered by tests?\nWhat happens with:\n- Null or undefined inputs\n- Users mid-registration (incomplete profile)\n- Concurrent validation requests"
      },
      "speakerNotes": {
        "talkingPoints": "This follow-up is critical. The agent has now read the actual implementation and existing tests. This question forces the agent to compare what's actually implemented against what's tested. The specific examples (null inputs, mid-registration, concurrency) guide the agent toward high-value edge cases rather than generic testing advice.",
        "timing": "2-3 minutes",
        "discussion": "Why is asking for specific cases better than 'find gaps'? Because it forces the agent to think about your actual system, not generic testing patterns.",
        "context": "At scale, this difference is enormous. An agent that discovers 3-5 real untested edge cases per session prevents bugs that would otherwise reach production.",
        "transition": "Now we understand discovery. The challenge comes when code and tests are written together—this creates a 'cycle of self-deception.'"
      }
    },
    {
      "type": "concept",
      "title": "The Cycle of Self-Deception",
      "content": [
        "Agent implements feature in Context A",
        "Same agent writes tests in Context A immediately after",
        "Tests inherit implementation's blind spots",
        "Both artifacts are 'correct' together but wrong independently",
        "Specification gaming: tests weaken to match implementation"
      ],
      "speakerNotes": {
        "talkingPoints": "When an agent writes code and tests in the same conversation, they share the same assumptions. If the agent missed a constraint (negative quantities should be rejected), the test will verify the buggy implementation. Both pass validation. The test doesn't catch the bug because the agent who wrote the implementation also wrote the test. This is Goodhart's Law: when the test becomes the optimization target, agents optimize for passing tests rather than correctness.",
        "timing": "4-5 minutes",
        "discussion": "Real example: agent implements payment processing, allows $0 transactions in code. In same session, agent writes test verifying $0 transactions work. Test passes. Bug reaches production. Ask: How would separate contexts have prevented this?",
        "context": "Research shows this 'specification gaming' occurs in ~1% of test-code cycles but compounds across large codebases. Enterprise systems report this is a leading cause of preventable bugs in AI-assisted development.",
        "transition": "The solution is context separation. Let's look at the three-context workflow."
      }
    },
    {
      "type": "visual",
      "title": "The Three-Context Workflow",
      "component": "ThreeContextWorkflow",
      "caption": "Fresh contexts prevent blind spots from compounding",
      "speakerNotes": {
        "talkingPoints": "The diagram shows three separate conversations: Context A for code, Context B for tests, Context C for debugging. Each context is stateless—the agent doesn't carry assumptions from the previous step. This forces independent thinking at each phase.",
        "timing": "3-4 minutes",
        "discussion": "Walk through each context: In A, agent writes code grounded in existing patterns. In B (fresh context), agent reads requirements and edge cases without knowledge of A's implementation—tests derive independently. In C (fresh context), agent analyzes failures objectively without defending either code or tests.",
        "context": "Salesforce reported 30% reduction in debugging time using this approach at scale (millions of test runs). The stateless nature forces objectivity at each phase.",
        "transition": "Now let's discuss how to write tests that actually catch bugs: sociable tests versus heavily mocked tests."
      }
    },
    {
      "type": "comparison",
      "title": "Test Quality: Sociable vs Heavily Mocked",
      "left": {
        "label": "Heavily Mocked",
        "content": [
          "Stubs internal implementations",
          "Passes even when code breaks",
          "Verifies function calls, not behavior",
          "False confidence in CI pipeline"
        ]
      },
      "right": {
        "label": "Sociable Tests",
        "content": [
          "Uses real implementations for internal code",
          "Fails immediately when behavior changes",
          "Exercises actual code paths",
          "Catches silent breakage from agent changes"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "A heavily mocked test might stub findByEmail(), verify(), and create(). If an agent breaks all three implementations, the test still passes—it only checked that methods were called. A sociable test uses real database queries, real password hashing, real session tokens. If any part of the flow breaks, the test fails. Mock only external systems (Stripe, third-party APIs) that cost money or require credentials.",
        "timing": "3-4 minutes",
        "discussion": "Show both versions of an authentication test. Ask students: Which one would catch a bug if an agent refactored the password hashing logic?",
        "context": "This is a critical distinction for AI-assisted development. Mocking made sense when tests were written by humans who knew what to test. Agents need real implementations to discover what actually breaks.",
        "transition": "Writing good tests is important, but speed matters too. Let's talk about smoke test strategy."
      }
    },
    {
      "type": "concept",
      "title": "Smoke Tests: Fast Feedback Loop",
      "content": [
        "Sub-30-second smoke suite is essential",
        "Cover only critical junctions (auth, core flows, DB)",
        "Run after every agent task, not just end of session",
        "10-minute comprehensive suite = no one runs it",
        "Codify in AGENTS.md: agents auto-run smoke tests"
      ],
      "speakerNotes": {
        "talkingPoints": "When agents refactor rapidly, a 10-minute test suite is ignored until the end when bugs have compounded. A 30-second smoke suite that runs after each task catches failures immediately while context is fresh. This prevents the snowball effect where 20 tasks hide which one broke the system.",
        "timing": "2-3 minutes",
        "discussion": "Ask: How long would YOU wait to run tests if they took 10 minutes? Most teams skip it. Now ask: How fast do you need feedback from an agent? Immediate feedback prevents compounding errors.",
        "context": "Jeremy Miller's research on test automation strategy supports this: use 'the finest grained mechanism that tells you something important.' Smoke tests are that mechanism.",
        "transition": "We've covered how to write tests. Now let's discuss using agents themselves to find edge cases."
      }
    },
    {
      "type": "concept",
      "title": "Autonomous Testing: Agent Simulation",
      "content": [
        "Non-deterministic exploration finds unknown edge cases",
        "Agents explore different paths on each run",
        "Unreliable for regression testing, excellent for discovery",
        "Give agents product access: browser, CLI, API",
        "MCP servers (Playwright, Chrome DevTools, mobile-mcp)"
      ],
      "speakerNotes": {
        "talkingPoints": "Deterministic tests verify known requirements. Agent simulation discovers unknown edge cases. These are complementary. When you give an agent access to your product (via MCP servers), it behaves like a human tester exploring—but it explores non-deterministically. Run it twice and it takes different paths. This randomness is useless for CI/CD but excellent for finding race conditions, unicode handling bugs, state machine errors you didn't think to test for.",
        "timing": "3-4 minutes",
        "discussion": "Ask: What edge cases have you missed that a random tester would have found? That's what autonomous testing does.",
        "context": "Use agents for discovery, then codify findings into deterministic tests. Agents explore the unknown; deterministic tests prevent backsliding on the known.",
        "transition": "Eventually tests will fail. When they do, we need a systematic approach to diagnosis."
      }
    },
    {
      "type": "code",
      "title": "Diagnostic Prompt Pattern",
      "language": "markdown",
      "code": "```\n$FAILURE_DESCRIPTION\n```\n\nUse the code research to analyze the test failure above.\n\nDIAGNOSE:\n\n1. Examine the test code and its assertions.\n2.\n  Understand and clearly explain the intention and\n  reasoning of the test - what is it testing?\n3. Compare against the implementation code being tested\n4. Identify the root cause of failure\n\nDETERMINE:\nIs this a test that needs updating or a real bug\n  in the implementation?\n\nProvide your conclusion with evidence.",
      "caption": "Chain-of-Thought forces sequential analysis before jumping to conclusions",
      "speakerNotes": {
        "talkingPoints": "This prompt applies Chain-of-Thought, constraints, and structured format from Lesson 4. The numbered DIAGNOSE steps force sequential analysis—agent can't jump to 'root cause' without examining test intent first. The DETERMINE question constrains output to a binary decision. The evidence requirement forces grounding. This pattern can be adapted for performance issues, security vulnerabilities, or deployment failures.",
        "timing": "3-4 minutes",
        "discussion": "Walk through why each element matters: Fenced code block preserves error formatting. 'Use the code research' prevents hallucination. Numbered steps implement CoT. 'Evidence requirement' prevents vague assertions. Ask: How would this change if you removed any of these elements?",
        "context": "This diagnostic approach is more reliable than asking agents for general debugging help. The constraints and structure guide agents toward systematic analysis.",
        "transition": "Let's talk about why each part of this prompt works."
      }
    },
    {
      "type": "concept",
      "title": "Why Diagnostic Prompts Work",
      "content": [
        "Fenced code blocks prevent interpretation as instructions",
        "'Use code research' forces grounding over hallucination",
        "Numbered steps implement Chain-of-Thought reasoning",
        "Binary DETERMINE question constrains scope",
        "Evidence requirement prevents vague assertions"
      ],
      "speakerNotes": {
        "talkingPoints": "Each element of the diagnostic prompt serves a specific function. Fenced code blocks are a structured format that prevents the LLM from misinterpreting error messages as commands. The explicit grounding directive 'use code research' forces the agent to search the codebase instead of reasoning from training data. Numbered steps force sequential analysis. The binary decision (bug vs test update) prevents open-ended speculation. The evidence requirement (file paths, line numbers) requires concrete proof.",
        "timing": "2-3 minutes",
        "discussion": "Show what happens if you remove the evidence requirement: agents give vague answers like 'probably a timing issue.' With the requirement: agents cite specific line numbers and explain why.",
        "context": "This is an example of how constraints from Lesson 4 apply to real debugging scenarios. Structure enables precision.",
        "transition": "Now let's summarize the key takeaways from this lesson."
      }
    },
    {
      "type": "takeaway",
      "title": "Key Takeaways",
      "content": [
        "Tests ground agent implementation decisions",
        "Separate contexts prevent specification gaming",
        "Sociable tests catch silent breakage",
        "Smoke tests enable fast feedback"
      ],
      "speakerNotes": {
        "talkingPoints": "The core insight: tests are the primary way agents learn about your system. Good tests provide grounding; bad tests pollute context. Context separation prevents the cycle of self-deception where agents optimize tests to match buggy implementations. Sociable tests catch actual breakage; heavily mocked tests hide problems. Fast smoke tests enable iteration without compounding errors.",
        "timing": "2-3 minutes",
        "discussion": "Ask students to think of a test they've written that didn't catch a real bug. What would have made it better? What context was missing?",
        "context": "These principles apply whether you're working with Claude, other AI models, or human developers. Tests are your primary guardrail at scale.",
        "transition": "We've covered testing strategy. Next lesson is code review—how to verify agent work before it reaches production."
      }
    }
  ]
}
