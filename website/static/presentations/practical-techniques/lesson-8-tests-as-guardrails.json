{
  "metadata": {
    "title": "Tests as Guardrails: Preventing Regressions at Agent Scale",
    "lessonId": "lesson-8-tests-as-guardrails",
    "estimatedDuration": "40-50 minutes",
    "learningObjectives": [
      "Use tests as grounding constraints",
      "Prevent specification gaming with contexts",
      "Build sociable tests over mocks",
      "Diagnose failures systematically"
    ]
  },
  "slides": [
    {
      "type": "title",
      "title": "Tests as Guardrails",
      "subtitle": "Preventing Regressions at Agent Scale",
      "content": [],
      "speakerNotes": {
        "talkingPoints": "Agents operate at velocity humans can't match. They refactor 30 files in minutes, and their changes compound fast. Tests are your constraint system—they define boundaries agents cannot cross. More importantly, they're living documentation that agents actually read to understand intent, edge cases, and tribal knowledge. In this lesson, we'll learn how to use tests as grounding mechanisms and prevent the hidden pitfalls of agent-driven code generation.",
        "timing": "1 minute",
        "discussion": "Ask: 'Who's had an agent introduce a subtle bug that slipped past code review?' This sets the stage for why test infrastructure matters at scale.",
        "context": "Senior engineers often treat tests as a checkbox. With agents, tests become a critical safety boundary—they're not optional quality improvements, they're operational constraints.",
        "transition": "Let's start by understanding what agents actually read when they research your codebase."
      }
    },
    {
      "type": "concept",
      "title": "Tests as Documentation",
      "content": [
        "Agents search and read tests during research phase",
        "Tests show OAuth users skip email verification",
        "Tests reveal timezone offset handling, negative quantity rejection",
        "Good tests ground implementation in concrete examples",
        "Bad tests (vague names, heavy mocks) pollute context"
      ],
      "speakerNotes": {
        "talkingPoints": "When agents research your codebase, they read both source code and tests. These tests aren't documentation agents learn about—they're concrete constraints that ground subsequent implementation decisions. If your tests are clear and specific, the agent's implementation stays within your actual constraints. If your tests are vague (test('works')), or heavy on mocking, the agent fills its context window with noise and falls back to training patterns instead of your actual requirements.",
        "timing": "3 minutes",
        "discussion": "Ask: 'What makes a test 'good documentation' for an AI agent? How is that different from good documentation for humans?' Students should recognize that clarity, specificity, and behavioral focus matter even more for agents.",
        "context": "In production, Stripe's payment processing tests are specific and clear: they document that amounts below $0.50 are rejected, that customer metadata is preserved, etc. An agent reading these tests implements accordingly. Compare to generic test names like 'test_payment' that provide no insight.",
        "transition": "So how do we systematically discover what tests we need in the first place?"
      }
    },
    {
      "type": "concept",
      "title": "Research First: Edge Case Discovery",
      "content": [
        "Use planning techniques to discover testable scenarios",
        "Ask questions that load implementation details into context",
        "Agent searches function, reads implementation, finds tests",
        "Synthesize findings into concrete list of edge cases",
        "Analyze implementation against questions to identify gaps"
      ],
      "speakerNotes": {
        "talkingPoints": "Before writing tests, use the planning methodology from Lesson 7 to discover what actually needs testing. Don't start with generic test checklist. Instead, ask targeted questions about your specific code: 'What happens when a user has no email? When they have multiple verified emails? When they delete their account while signing in?' The agent searches for these scenarios in your existing code, finds edge cases you've already handled, and identifies gaps you haven't.",
        "timing": "3-4 minutes",
        "discussion": "Ask: 'What's one edge case in your actual production code that you only discovered after a customer reported it?' Students should see how agent-driven research could have uncovered these proactively.",
        "context": "GitHub's authentication flow has to handle SAML accounts with no email, GitHub-native accounts with multiple emails, and enterprise accounts with SSO. Rather than guessing these edge cases, you ask about them during planning, and the agent grounds discoveries in actual code.",
        "transition": "Once you've discovered what needs testing, you face a subtle but critical pitfall: the closed-loop problem."
      }
    },
    {
      "type": "concept",
      "title": "The Cycle of Self-Deception",
      "content": [
        "Same context generates both code and tests → shared assumptions",
        "Agent implements zero-quantity acceptance, tests verify it succeeds",
        "Both artifacts stem from same flawed reasoning",
        "Bug passes tests; CI pipeline validates wrong behavior",
        "At scale: specification gaming, weakened assertions, shortcuts"
      ],
      "speakerNotes": {
        "talkingPoints": "Here's the dangerous pattern: when code and tests are generated in the same conversation, they inherit the same assumptions and blind spots. An agent might write an API endpoint that accepts zero or negative quantities, then write tests that verify zero-quantity acceptance succeeds. Both are 'correct' relative to each other, but both are wrong relative to your actual requirements. The bug passes tests. The CI pipeline is green. The problem compounds at scale when agents engage in 'specification gaming'—weakening assertions to achieve green checkmarks.",
        "timing": "3-4 minutes",
        "discussion": "Ask: 'Has anyone encountered a 'test that proves the bug'? Where the implementation was wrong but the test was written to match it?' This connects the abstract concept to real experience.",
        "context": "Goodhart's Law: 'When a measure becomes a target, it ceases to be a good measure.' If your agent optimizes for passing tests, and those tests were generated in the same session as the code, you've created a feedback loop that validates the wrong behavior.",
        "transition": "The solution is to break that feedback loop using a three-context workflow."
      }
    },
    {
      "type": "visual",
      "component": "WorkflowCircle",
      "title": "Three-Context Workflow",
      "caption": "Fresh contexts prevent shared assumptions",
      "speakerNotes": {
        "talkingPoints": "The antidote to the cycle of self-deception is simple but rigorous: use fresh contexts for each step. Write code in Context A—research existing patterns, plan implementation, execute, verify correctness. Then write tests in fresh Context B—the agent doesn't remember writing the implementation, so tests derive independently from requirements. Finally, triage failures in fresh Context C—the agent doesn't know who wrote code or tests, providing objective analysis. This leverages the stateless nature from Lessons 1 and 2.",
        "timing": "2-3 minutes",
        "discussion": "Ask: 'Why is stateless context important here? What would happen if the agent 'remembered' writing the implementation when writing tests?' Students should see how memory creates bias.",
        "context": "Enterprise systems validate this approach. Salesforce reduced debugging time 30% using automated root cause analysis with fresh contexts for millions of daily test runs. The detailed diagnostic workflow enables that objectivity.",
        "transition": "Now let's zoom into what makes tests effective in the first place. It all comes down to how you write them."
      }
    },
    {
      "type": "comparison",
      "title": "Sociable Tests vs Heavy Mocking",
      "left": {
        "label": "Heavy Mocking",
        "content": [
          "Stubs all dependencies (findByEmail, verify, create)",
          "Verifies function calls, not behavior",
          "False confidence—passes when implementations break",
          "Tests the mock contract, not the actual code",
          "Difficult to maintain when implementations change"
        ]
      },
      "right": {
        "label": "Sociable Tests",
        "content": [
          "Real database, real password hashing, real tokens",
          "Exercises actual code paths end-to-end",
          "Fails when agent breaks any part of flow",
          "Tests real behavior users depend on",
          "Mocks only external systems (Stripe, SMS, APIs)"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "Heavily mocked tests give false confidence. They verify that functions call each other correctly, not that they actually work. A sociable test uses real implementations for internal code and mocks only external systems—the ones that cost money or require API keys. When an agent refactors authentication, a sociable test exercises the real password hashing, real database queries, and real session tokens. If the agent breaks any part, the test fails immediately.",
        "timing": "4 minutes",
        "discussion": "Ask: 'In your codebase, which tests are heavily mocked? Would those tests catch a real bug, or just verify the mock setup?' Have students identify a specific example.",
        "context": "Payment processing is a perfect example: mock Stripe (external), but test your actual billing logic against a real test database. That test will catch if your agent accidentally breaks discount calculations or tax handling.",
        "transition": "But there's another critical aspect: feedback speed. Even sociable tests can slow you down if there are too many."
      }
    },
    {
      "type": "concept",
      "title": "Smoke Tests for Fast Feedback",
      "content": [
        "Sub-30-second suite covering critical junctions only",
        "Core user journey, auth boundaries, DB connectivity",
        "Run after each task—catch failures while context fresh",
        "Skip smoke tests → discover bugs after 20 changes",
        "Codify in AGENTS.md so agents run automatically"
      ],
      "speakerNotes": {
        "talkingPoints": "A 10-minute comprehensive test suite is useless for iterative agent development. You won't run it until the end, and then debugging becomes expensive. Build a sub-30-second smoke test suite covering only the critical junctions: core user journey, authentication boundaries, database connectivity. Run it after each task. When it fails, you know exactly which change broke it because the context is fresh. As Jeremy Miller notes, use 'the finest grained mechanism that tells you something important.'",
        "timing": "3 minutes",
        "discussion": "Ask: 'What's the critical path in your application? The minimum test coverage needed to know if the app is broken?' That's your smoke test.",
        "context": "Codify this in your AGENTS.md or CLAUDE.md (from Lesson 6) so agents automatically run smoke tests after completing each task. That way, failures surface immediately during iteration, not during final review.",
        "transition": "So far we've talked about deterministic tests. But there's a complementary testing strategy that agents are uniquely suited for."
      }
    },
    {
      "type": "concept",
      "title": "Autonomous Agent Testing",
      "content": [
        "Give agents a task and tools to interact with product",
        "Browser automation, CLI access, API clients",
        "Non-deterministic: same test explores different paths each run",
        "Excellent for discovery; unreliable for regression testing",
        "Agents find race conditions, input gaps you never considered"
      ],
      "speakerNotes": {
        "talkingPoints": "AI agents can simulate actual user behavior. Give them a task ('Find bugs in the checkout flow'), give them tools (browser automation), and watch them explore your application like a human tester would. The critical difference: agents explore non-deterministically. Run the same test twice, and the agent explores different paths each time. This randomness makes them useless for regression testing in CI/CD, but perfect for discovery. One run finds the happy path, another discovers a race condition by clicking rapidly, a third stumbles onto an edge case with Unicode characters you never considered.",
        "timing": "3-4 minutes",
        "discussion": "Ask: 'What's one production bug you discovered through human testing, not code review? How might an agent have found that?'",
        "context": "Shopify uses agent-driven testing for their checkout flow. Agents discover that rapid successive clicks on 'Place Order' create duplicate orders, that pasting Unicode into address fields breaks validation, that adding items while the cart loads causes race conditions.",
        "transition": "Once agents discover edge cases, solidify them as deterministic tests. But first, let's talk about connecting agents to your actual product."
      }
    },
    {
      "type": "concept",
      "title": "MCP Servers for Product Interaction",
      "content": [
        "Chrome DevTools MCP: Full browser automation and profiling",
        "Playwright MCP: Cross-browser testing, accessibility tree",
        "mobile-mcp: iOS/Android simulator and real device automation",
        "Computer Use MCP: Desktop application control",
        "Configure in assistant settings to connect agents directly"
      ],
      "speakerNotes": {
        "talkingPoints": "To enable agents to interact with your product, use Model Context Protocol servers that give agents 'eyes and hands' across platforms. Chrome DevTools MCP gives full CDP access for browser automation. Playwright MCP works across browsers with accessibility-tree-based interactions (agents can reference 'the submit button' naturally). mobile-mcp handles iOS and Android. Computer Use MCP controls desktop apps. Configure these in your AI assistant's MCP settings, and agents can now test your actual product, not just read code.",
        "timing": "2-3 minutes",
        "discussion": "Ask: 'Which platform are your users on most? What's the first MCP server you'd set up?'",
        "context": "Early-stage but growing: Microsoft, Cloudflare, IBM are integrating MCP across platforms. This is the infrastructure enabling agent-driven product testing at scale.",
        "transition": "Now, when tests fail, how do you figure out what went wrong? This is where a systematic diagnostic workflow becomes critical."
      }
    },
    {
      "type": "code",
      "title": "Diagnostic Prompt Pattern",
      "language": "text",
      "code": "Use the code research to analyze\nthe test failure above.\n\nDIAGNOSE:\n1. Examine the test code\nand its assertions\n2. Understand and clearly explain\nthe intention and reasoning\n3. Compare against implementation\n4. Identify root cause\n\nDETERMINE:\nIs this a test that needs\nupdating or a real bug?\n\nProvide conclusion with evidence.",
      "caption": "Structured diagnostics prevent hallucination",
      "speakerNotes": {
        "talkingPoints": "This diagnostic prompt applies techniques from Lesson 4: chain-of-thought sequential steps, constraints requiring evidence, and structured format. The fenced code block preserves error formatting. 'Use the code research' is an explicit grounding directive forcing codebase search instead of hallucination. Numbered DIAGNOSE steps implement chain-of-thought. 'Understand the intention' (step 2) ensures the agent articulates WHY the test exists, not just WHAT it does. DETERMINE constrains output to 'bug vs outdated test.' 'Provide evidence' requires file paths and line numbers—concrete proof, not vague assertions.",
        "timing": "4-5 minutes",
        "discussion": "Walk through a real test failure example if available. Show how the structured steps force analytical thinking instead of guessing.",
        "context": "You can adapt this pattern for performance issues, security vulnerabilities, or deployment failures by changing diagnostic steps while preserving the structure: sequential CoT → constrained decision → evidence requirement.",
        "transition": "Let's recap the core principles and look at how they fit together."
      }
    },
    {
      "type": "concept",
      "title": "Deterministic vs Agent Testing",
      "content": [
        "Deterministic: verify known requirements, fast CI/CD feedback",
        "Deterministic: document expected behavior, prevent regressions",
        "Agent testing: discover unknown edge cases, state machine bugs",
        "Agent testing: find input validation gaps, race conditions",
        "Workflow: agents explore unknown, deterministic tests prevent backsliding"
      ],
      "speakerNotes": {
        "talkingPoints": "You need both strategies. Deterministic tests (unit, integration, E2E) verify known requirements and run reliably in CI/CD. Agent simulation discovers unknown edge cases—race conditions, timing bugs, input validation gaps. The workflow is: use agents for discovery, then solidify findings into deterministic tests. Agents explore the unknown; deterministic tests prevent backsliding on the known.",
        "timing": "2-3 minutes",
        "discussion": "Ask: 'In the last week, what bug would deterministic tests have caught vs agent testing?'",
        "context": "This mirrors the research-planning-execution cycle from Lesson 3. Tests are part of the same systematic approach.",
        "transition": "Let's bring this all together with a final set of principles."
      }
    },
    {
      "type": "takeaway",
      "title": "Key Takeaways",
      "content": [
        "Tests are documentation agents read—write them as concrete constraints, not generic advice",
        "Separate contexts for code, tests, and debugging prevent shared blind spots and specification gaming",
        "Use sociable tests with real code paths; mock only external systems (APIs, Stripe)",
        "Build sub-30-second smoke tests for fast feedback during iteration; deterministic tests catch regressions",
        "Agents discover edge cases through non-deterministic exploration; solidify findings as regression tests"
      ],
      "speakerNotes": {
        "talkingPoints": "These five principles form a cohesive testing strategy for agent-driven development. Tests aren't a checkbox—they're a constraint system that keeps agents honest. When you use fresh contexts, build sociable tests, run fast feedback loops, and combine deterministic verification with agent-driven discovery, you get robust code at the velocity agents can provide.",
        "timing": "3 minutes",
        "discussion": "Ask: 'Which of these five principles would have the biggest impact on your team's testing practice? Where would you start?'",
        "context": "Senior engineers often have strong testing opinions. This lesson doesn't replace that—it layers agent-specific considerations on top of solid testing fundamentals.",
        "transition": "In Lesson 9, we'll look at code review from the agent perspective. You'll learn how to structure reviews that give agents actionable feedback."
      }
    },
    {
      "type": "concept",
      "title": "Practical Implementation",
      "content": [
        "Start: identify 5-10 critical user journeys for smoke tests",
        "Codify smoke test execution in AGENTS.md or CLAUDE.md",
        "Refactor heavily mocked tests: identify external deps vs internal",
        "Add edge case discovery prompts to your planning questions",
        "Set up one MCP server (browser automation) for agent testing"
      ],
      "speakerNotes": {
        "talkingPoints": "Here's how to apply this in the next week: First, identify your 5-10 most critical user journeys and build tests that cover them in under 30 seconds. Second, document in AGENTS.md that agents should run these after each task. Third, pick one heavily mocked test and refactor it to use real implementations. Fourth, add edge case discovery questions to your planning process. Fifth, set up Chrome DevTools MCP or Playwright MCP so agents can interact with your product.",
        "timing": "2-3 minutes",
        "discussion": "Ask students to commit to one of these actions by next week.",
        "context": "These aren't theoretical—they're concrete steps that compound into a testing infrastructure aligned with agent development.",
        "transition": "Next lesson: code review. Let's dive in."
      }
    }
  ]
}
