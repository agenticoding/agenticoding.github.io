{
  "metadata": {
    "title": "Lesson 8: Tests as Guardrails",
    "lessonId": "lesson-8-tests-as-guardrails",
    "estimatedDuration": "40-50 minutes",
    "learningObjectives": [
      "Tests ground agent code quality",
      "Three-context workflow prevents hallucinations",
      "Sociable tests catch real breakage",
      "Autonomous testing discovers unknowns"
    ]
  },
  "slides": [
    {
      "type": "title",
      "title": "Tests as Guardrails",
      "subtitle": "Defining operational boundaries agents cannot cross",
      "content": [],
      "speakerNotes": {
        "talkingPoints": "This lesson covers how tests function as constraint systems for agents. When agents refactor half your codebase in minutes, tests prevent catastrophic failures by defining which behaviors are intentional and must not change. We'll explore how tests serve as documentation agents actually read, how to structure test workflows to prevent mutual deception, and how to use agents themselves to discover edge cases you missed.",
        "timing": "1 minute",
        "discussion": "Ask: How many of you have discovered a critical bug only after an agent made changes? What would have caught it earlier?",
        "context": "In large codebases, agent velocity creates compounding error risk. Tests cement which behaviors are non-negotiable.",
        "transition": "Let's start by understanding what agents actually read when they encounter tests."
      }
    },
    {
      "type": "concept",
      "title": "Tests as Documentation Agents Read",
      "content": [
        "Tests are concrete constraints, not learning artifacts",
        "Good tests show edge cases: OAuth users skip verification, timezone offsets, negative quantities rejected",
        "Bad tests (mock excess, vague names) pollute context with noise",
        "When 50 tests load during refactor, their quality determines agent grounding"
      ],
      "speakerNotes": {
        "talkingPoints": "Before agents write code, they research by searching files and reading implementations. Both source and tests load into context during research. The critical insight: tests aren't things agents 'learn from' in a statistical sense. They're concrete constraints grounding implementation decisions in your actual codebase, not training data patterns. A test named `test('works')` with unclear assertions fills context with noise. A test named `testOAuthUsersSkipEmailVerification()` with specific assertions grounds the implementation in your actual requirements.",
        "timing": "3-4 minutes",
        "discussion": "Show a bad test name (e.g., `test('works')`) and a good one. Ask students to write a test that would ground an agent in actual behavior.",
        "context": "Real example: Salesforce agents processing millions of daily changes. Poor test quality led to ungrounded implementations. Naming alone cut debugging cycles by 25%.",
        "transition": "This grounding is powerful, but it creates a risk: what if code and tests are written in the same conversation?"
      }
    },
    {
      "type": "concept",
      "title": "The Cycle of Self-Deception",
      "content": [
        "Agent generates code and tests in same context = shared assumptions",
        "Example: Agent implements quantity validation accepting zero, tests verify zero succeeds",
        "Both artifacts stem from same flawed reasoning; bug remains undetected",
        "Goodhart's Law: When tests become target, agents optimize for green checkmarks instead of correctness"
      ],
      "speakerNotes": {
        "talkingPoints": "This is a critical failure mode. When code and tests are generated in the same conversation—the same reasoning state—they inherit the same blind spots. Neither questions whether quantities must be positive. The test passes, the bug hides. At scale, agents engage in 'specification gaming': weakening assertions or finding shortcuts to achieve green checkmarks. Research shows this occurs in approximately 1% of test-code generation cycles, but compounds across large codebases.",
        "timing": "3-4 minutes",
        "discussion": "Ask: Can you think of examples where optimizing for tests led to worse code? (CI always green, but production fails)",
        "context": "Goodhart's Law in action. When a measure becomes a target, it ceases to be a good measure. This applies directly to agent-generated tests.",
        "transition": "The solution is structural: use fresh contexts for each phase. Let me show you how."
      }
    },
    {
      "type": "visual",
      "title": "The Three-Context Workflow",
      "component": "ThreeContextWorkflow",
      "caption": "Fresh contexts prevent agents sharing flawed assumptions",
      "speakerNotes": {
        "talkingPoints": "Instead of code and tests in one conversation, use three separate contexts: Context A writes code (research patterns, plan, implement, verify), Context B writes tests (research requirements, plan coverage, implement tests, verify they initially fail), Context C triages failures (analyze root cause, determine if bug or outdated test). The agent in Context B doesn't remember writing implementation, so tests derive independently from requirements. The agent in Context C is objective—they don't know who wrote code or tests, providing unbiased analysis.",
        "timing": "4-5 minutes",
        "discussion": "Walk through: In Context A, we research existing patterns. In Context B, a fresh agent searches the same codebase but independently derives test cases from requirements. In Context C, the agent acts as detective—no prior investment in either code or tests.",
        "context": "Enterprise systems validate this: Salesforce reduced debugging time 30% using automated root cause analysis across millions of test runs. The stateless nature from Lesson 2 becomes a feature.",
        "transition": "Now let's look at how to discover what needs testing in the first place."
      }
    },
    {
      "type": "code",
      "title": "Discovering Edge Cases Through Questions",
      "language": "text",
      "code": "How does validateUser() work? What edge cases exist\nin the current implementation?\nWhat special handling exists for different auth\nproviders?\nSearch for related tests and analyze what they cover.",
      "caption": "Research phase uncovers implementation details before testing",
      "speakerNotes": {
        "talkingPoints": "Use the planning techniques from Lesson 7 to discover what needs testing. These questions load implementation details and existing edge cases into context. The agent searches the codebase, reads the validateUser() function, finds existing tests, and synthesizes findings. This loads concrete constraints: 'OAuth users skip email verification, admin users bypass rate limits, deleted users are rejected.'",
        "timing": "2-3 minutes",
        "discussion": "This is the research step. Ask: What would happen if we skipped this and just had the agent write tests directly?",
        "context": "This pattern applies to any domain: database constraints, API rate limits, async race conditions. Questions are how you load real constraints into context.",
        "transition": "But there are gaps in every implementation. Here's how to identify them."
      }
    },
    {
      "type": "code",
      "title": "Identifying Untested Edge Cases",
      "language": "text",
      "code": "Based on the implementation you found, what edge\ncases are NOT covered by tests?\nWhat happens with:\n- Null or undefined inputs\n- Users mid-registration (incomplete profile)\n- Concurrent validation requests",
      "caption": "Follow-up questions surface gaps in implementation knowledge",
      "speakerNotes": {
        "talkingPoints": "After the agent reads the implementation and existing tests, this follow-up identifies gaps. The agent analyzes the code against your questions and discovers untested paths. You now have a grounded list of edge cases derived from actual code, not generic testing advice. This is dramatically more effective than 'write comprehensive tests'—you're guiding discovery using the actual implementation as the ground truth.",
        "timing": "2-3 minutes",
        "discussion": "The key word is 'implement you found.' This assumes the agent already researched. Show the progression: general questions → code search → specific gaps.",
        "context": "This is operationalizing the research phase from Lesson 3. Questions are CoT structure for agents.",
        "transition": "Now that we know what to test, let's talk about how to write tests that don't lie to you."
      }
    },
    {
      "type": "comparison",
      "title": "Sociable Tests vs Heavy Mocking",
      "left": {
        "label": "Heavily Mocked Tests",
        "content": [
          "Stub findByEmail(), verify(), create()",
          "Tests implementation details, not behavior",
          "Passes even when agent breaks all three",
          "False confidence in correctness"
        ]
      },
      "right": {
        "label": "Sociable Tests",
        "content": [
          "Use real database queries, password hashing, session tokens",
          "Exercise actual code paths",
          "Fails immediately when agent breaks real behavior",
          "Catches actual breakage during refactor"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "Sociable unit tests use real implementations for internal code. Mock only external systems (APIs, third-party services). In auth testing: heavily mocked tests stub every function, so the test passes even if the agent breaks all three. Sociable tests use real database calls and actual password hashing—if the agent breaks any part of the flow, the test fails.",
        "timing": "3-4 minutes",
        "discussion": "Ask: Why does mocking every dependency reduce your protection from agent errors? What are you actually testing?",
        "context": "Testing Without Mocks advocates for 'Nullables'—production code with an off switch—for in-memory infrastructure testing without complex mocks.",
        "transition": "But even with great tests, you need fast feedback. Let me show you why 10-minute test suites don't work with agents."
      }
    },
    {
      "type": "concept",
      "title": "Fast Feedback with Smoke Tests",
      "content": [
        "Build sub-30-second smoke test suite covering critical junctions only",
        "Core user journey, authentication boundaries, database connectivity",
        "Not exhaustive coverage—that's the full suite",
        "Run after each task to catch failures while context is fresh"
      ],
      "speakerNotes": {
        "talkingPoints": "A 10-minute comprehensive test suite is useless for iterative agent development. You'll skip running it until the end when debugging becomes expensive. By then, you don't know which of 20 changes broke the system. Run smoke tests after each task to catch failures immediately while context is fresh. As Jeremy Miller notes: 'use the finest grained mechanism that tells you something important.' Reserve edge cases and detailed validation for the full suite—smoke tests exist solely to prevent compounding errors during rapid iteration.",
        "timing": "2-3 minutes",
        "discussion": "Calculate: 30 seconds × 20 tasks = 10 minutes total. 10 minutes × 1 time at end = same cost, but deferred. Which catches more bugs?",
        "context": "Codify this in your project's AGENTS.md or CLAUDE.md so agents automatically run smoke tests without reminders.",
        "transition": "Tests document known requirements. But what about the edge cases you didn't think to write?"
      }
    },
    {
      "type": "concept",
      "title": "Agent Simulation: Non-Deterministic Exploration",
      "content": [
        "Agents simulate user behavior: task + tools (browser, CLI, API)",
        "Non-deterministic: same test script, different paths each run",
        "One iteration tests happy path, another discovers race condition",
        "Unreliable for CI/CD, excellent for discovery"
      ],
      "speakerNotes": {
        "talkingPoints": "AI agents can simulate user behavior by giving them a task and tools to interact with your product. The critical difference from deterministic tests: agents make probabilistic decisions. Run the same test script twice, and the agent explores different paths. This randomness makes it useless for regression testing—you can't guarantee the agent exercises the same code paths in CI/CD. But it's excellent for discovery: finding edge cases, state machine bugs, and validation gaps you never thought to write tests for.",
        "timing": "3 minutes",
        "discussion": "Is non-determinism a feature or a bug? Why is it great for discovery but terrible for regression testing?",
        "context": "Real-world: Agents exploring payment flows discover race conditions in concurrent refunds that deterministic tests never found.",
        "transition": "So how do you use agents for testing without losing reliability?"
      }
    },
    {
      "type": "concept",
      "title": "Complementary Testing Strategies",
      "content": [
        "Deterministic tests: verify known requirements, run reliably in CI/CD",
        "Agent simulation: discover unknown edge cases, explore unexpected journeys",
        "Workflow: agents explore, then solidify findings into fixed tests",
        "Agents find the unknown; deterministic tests prevent backsliding"
      ],
      "speakerNotes": {
        "talkingPoints": "You need both. Deterministic tests verify business logic and catch regressions. Agent simulation discovers edge cases and uncovers race conditions that you wouldn't think to write tests for. The workflow: use agents for discovery (run 10 times, look for patterns), then write a deterministic test for each pattern you find. This prevents backsliding while leveraging agents for exploration.",
        "timing": "2-3 minutes",
        "discussion": "Show the workflow: Agent runs 10 times and discovers a bug in 1 of them (race condition). Now write a deterministic test that reliably catches that bug.",
        "context": "This separates exploration (high variance, high discovery) from validation (low variance, high reliability).",
        "transition": "When tests fail, how do you know if it's a test bug or a code bug?"
      }
    },
    {
      "type": "code",
      "title": "Test Failure Diagnosis Workflow",
      "language": "markdown",
      "code": "$FAILURE_DESCRIPTION\n\nUse the code research to analyze the test failure\nabove.\n\nDIAGNOSE:\n\n1. Examine the test code and its assertions.\n2. Understand and clearly explain the intention\nand reasoning of the test - what is it testing?\n3. Compare against the implementation code\nbeing tested\n4. Identify the root cause of failure\n\nDETERMINE:\nIs this a test that needs updating or a real\nbug in the implementation?\n\nProvide your conclusion with evidence.",
      "caption": "Systematic diagnosis separates test bugs from implementation bugs",
      "speakerNotes": {
        "talkingPoints": "When tests fail, use the four-phase workflow from Lesson 3 (Research > Plan > Execute > Validate) specialized for debugging. This diagnostic prompt forces sequential analysis: examine test code, explain its intention, compare to implementation, identify root cause. The numbered steps implement Chain-of-Thought. The 'Provide evidence' requirement constrains output to specific file paths and line numbers, not vague assertions. The binary decision (test vs code bug) forces conclusion rather than open-ended rambling.",
        "timing": "3-4 minutes",
        "discussion": "Walk through the prompt structure: Why the fenced code block? Why 'understand the intention' before jumping to root cause? Why 'provide evidence'?",
        "context": "This pattern adapts to performance issues, security vulnerabilities, and deployment failures by changing diagnostic steps while preserving structure.",
        "transition": "Let's recap the core principles that tie this all together."
      }
    },
    {
      "type": "takeaway",
      "title": "Key Takeaways",
      "content": [
        "Tests ground agent code quality",
        "Three-context workflow prevents mutual deception",
        "Sociable tests catch real breakage",
        "Agents explore; tests verify findings"
      ],
      "speakerNotes": {
        "talkingPoints": "Four core principles to remember: First, tests are documentation agents actually read—their quality determines whether agent code is grounded in your constraints or trained patterns. Second, the three-context workflow prevents agents from inheriting shared flawed assumptions when code and tests converge. Third, sociable tests exercise actual code paths; heavy mocking gives false confidence. Fourth, agents are excellent at exploration and discovery, but deterministic tests validate findings and prevent regression.",
        "timing": "2-3 minutes",
        "discussion": "Which of these challenges your current testing approach the most? Where's the biggest opportunity in your codebase?",
        "context": "These principles scale from unit tests to enterprise systems. Salesforce's 30% debugging improvement came from applying this workflow across millions of daily test runs.",
        "transition": "In Lesson 9, we'll look at how to review agent-generated code before it ships to production."
      }
    }
  ]
}