{
  "metadata": {
    "title": "Debugging with AI Agents",
    "lessonId": "lesson-10-debugging",
    "estimatedDuration": "45-60 minutes",
    "learningObjectives": [
      "Require evidence before accepting fixes",
      "Master closed-loop debugging workflow",
      "Analyze logs with AI patterns",
      "Design reproducible debugging environments"
    ]
  },
  "slides": [
    {
      "type": "title",
      "title": "Debugging with AI Agents",
      "subtitle": "Evidence-driven investigation, closed-loop verification",
      "content": [],
      "speakerNotes": {
        "talkingPoints": "Debugging with AI is fundamentally different from traditional debugging. Instead of describing symptoms and hoping for solutions, you demand evidence at every step. The agent becomes your tireless investigator—but only when given access to diagnostic tools and failing environments.",
        "timing": "1 minute",
        "discussion": "Ask students: 'How often do you accept a fix without verifying it actually works?' This sets up the core tension we'll address.",
        "context": "Senior engineers know that unverified fixes create cascading problems. This lesson teaches a systematic approach where AI handles the tedious diagnostic work while you maintain verification rigor.",
        "transition": "Let's start with the fundamental principle that changes how we approach debugging with AI..."
      }
    },
    {
      "type": "concept",
      "title": "The Core Shift: Evidence Over Speculation",
      "content": [
        "Move from 'what do you think is wrong?' to 'prove the bug exists'",
        "AI excels at pattern recognition with concrete data",
        "AI fails spectacularly when forced to speculate",
        "Never accept a fix without reproducible proof it works"
      ],
      "speakerNotes": {
        "talkingPoints": "The anti-pattern is describing a bug and asking the agent to fix it blindly. The production pattern is providing reproduction steps, giving the agent diagnostic tools, and requiring before/after evidence. This shift moves debugging from guesswork to systematic investigation.",
        "timing": "3-4 minutes",
        "discussion": "Ask: 'What's the cost of accepting a fix that sounds right but doesn't actually work?' In production systems, unverified fixes cause cascading failures. Have students think about recent bugs they debugged without full proof.",
        "context": "In production, the worst bugs are ones that 'seem fixed' but have subtle regressions. Demanding evidence prevents this. AI agents are great at this—they can run multiple test scenarios and provide concrete before/after comparisons.",
        "transition": "Now that we understand the principle, let's look at the specific techniques. We'll start with code inspection—understanding architecture before diving into symptoms..."
      }
    },
    {
      "type": "concept",
      "title": "Code Inspection: Understand Before Fixing",
      "content": [
        "Have agent explain architecture and execution flow first",
        "Trace request paths and identify data flow patterns",
        "Use semantic code search for relevant components",
        "Focus on critical paths, not entire codebase",
        "Spot edge cases and assumptions the team missed"
      ],
      "speakerNotes": {
        "talkingPoints": "Before diving into logs or reproduction, have the agent build a mental model of your system. Ask specific questions: 'Trace the authentication flow from API request to database. Where could a race condition occur?' The agent's explanation often reveals edge cases.",
        "timing": "3 minutes",
        "discussion": "Show an example: 'We have a race condition in our payment processing.' Instead of jumping to fix, ask the agent to explain the entire payment flow first. Often the explanation itself reveals the bug.",
        "context": "This is where tools like ChunkHound's code research shine. The agent can get architectural context across the entire codebase rather than reading individual files. In smaller codebases, Grep and Read work fine.",
        "transition": "Once the agent understands the architecture, the next step is analyzing the evidence—and logs are where AI has its biggest advantage..."
      }
    },
    {
      "type": "concept",
      "title": "Log Analysis: AI's Superpower",
      "content": [
        "AI processes thousands of log lines humans can't parse manually",
        "Spots patterns across inconsistent log formats",
        "Identifies cascading errors in distributed systems",
        "Detects timing patterns indicating race conditions",
        "Works with whatever you have—raw output, grep, JSON"
      ],
      "speakerNotes": {
        "talkingPoints": "This is where AI debugging has the biggest advantage over human capability. Multi-line stack traces scattered across thousands of entries? Different services with different logging formats? Raw debug output without structured fields? AI excels at processing this chaos. What takes a senior engineer days takes AI minutes. The agent correlates patterns, identifies user cohorts experiencing failures, and traces cascading errors across system boundaries.",
        "timing": "4-5 minutes",
        "discussion": "Ask: 'Ever spent hours correlating logs from different services with different formats?' Have students share frustrating examples. Then emphasize: AI doesn't need perfect JSON—it parses whatever you give it. The messier the logs, the bigger AI's advantage.",
        "context": "Real-world example: A production incident with logs from 7 different microservices in 5 different formats. An agent can ingest all of it, spot the cascading failure pattern, and identify the root cause in the first service. This is fundamentally faster than human log analysis.",
        "transition": "But logs often tell you what's happening, not why. When you need to reproduce and understand the exact conditions, that's where reproduction scripts come in..."
      }
    },
    {
      "type": "concept",
      "title": "Reproduction Scripts: Building Proof",
      "content": [
        "Code generation is cheap—leverage it massively",
        "AI creates complex environments humans would skip",
        "Docker configs, database snapshots, mock services",
        "Capture full context: state, external responses, config",
        "Environments that take hours to set up take minutes"
      ],
      "speakerNotes": {
        "talkingPoints": "When code inspection and log analysis aren't sufficient—when you need bulletproof evidence or must reproduce complex state/timing conditions—reproduction scripts are invaluable. This is where AI agents' massive code generation capabilities shine. AI can generate Docker configs, database state initialization, mock external services, and test scenarios in minutes. What would be prohibitively tedious setup for humans becomes trivial.",
        "timing": "3 minutes",
        "discussion": "Ask: 'How long does it take to reproduce a customer bug locally?' Then show: 'AI can generate the entire reproduction environment—Docker setup, database state, configuration—in one prompt.' The economic shift is profound: reproducibility becomes cheap, so you demand it.",
        "context": "Example: A customer reports a race condition in payment processing under specific load. AI generates a complete reproduction: Docker Compose with load generator, database snapshots mimicking production state, mock external services, and a script that reproduces the failure reliably. No human would write all this—it's tedious. AI does it in minutes.",
        "transition": "With reproducible environments, we can now place agents inside failing systems to test and verify their fixes. This is the closed-loop debugging workflow..."
      }
    },
    {
      "type": "comparison",
      "title": "Open-Loop vs Closed-Loop Debugging",
      "left": {
        "label": "Open-Loop (Agent Without Environment Access)",
        "content": [
          "Agent researches code and online issues",
          "Reports hypothesis: 'Try adding validation at jwt.ts:67'",
          "Engineer must apply fix and verify manually",
          "No feedback loop for the agent"
        ]
      },
      "right": {
        "label": "Closed-Loop (Agent With Environment Access)",
        "content": [
          "Agent researches AND applies fixes within environment",
          "Runs reproduction to verify fix works",
          "Reports: 'Fixed at jwt.ts:67, reproduction now passes'",
          "Complete feedback loop enables iteration"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "The difference is profound. An open-loop agent can research and hypothesize, but cannot validate its own reasoning. A closed-loop agent applies fixes, re-runs reproduction, and proves they work—or iterates when they don't. Without environment access, you get educated guesses. With it, you get verified solutions.",
        "timing": "2-3 minutes",
        "discussion": "Ask students: 'Which would you rather have—a suggestion you need to verify, or a fix you can trust because the agent verified it?' Emphasize that closed-loop doesn't mean the agent is always right—it means the agent can't hide behind speculation.",
        "context": "In production debugging, this feedback loop is everything. The agent learns from failed verification attempts and refines its hypotheses. Open-loop agents are helpful for research but can't serve as the final verification step.",
        "transition": "Now let's look at the specific closed-loop workflow that ties everything together..."
      }
    },
    {
      "type": "codeExecution",
      "title": "Closed-Loop Debugging: BUILD → REPRODUCE → PLACE → INVESTIGATE → VERIFY",
      "steps": [
        {
          "line": "BUILD:\n  Create Docker environment with snapshots, configs, mocks",
          "highlightType": "human",
          "annotation": "Engineer defines reproducible conditions"
        },
        {
          "line": "REPRODUCE: Verify bug manifests consistently with evidence",
          "highlightType": "execution",
          "annotation": "Agent runs reproduction script, captures logs/errors"
        },
        {
          "line": "PLACE: Grant agent tool access in failing environment",
          "highlightType": "human",
          "annotation": "Engineer enables CLI/tool execution inside environment"
        },
        {
          "line": "INVESTIGATE:\n  Agent correlates runtime behavior + code + known issues",
          "highlightType": "prediction",
          "annotation": "Agent uses ChunkHound for code research, ArguSeek for ecosystem patterns"
        },
        {
          "line": "VERIFY:\n  Agent applies fix, re-runs reproduction, confirms resolution",
          "highlightType": "execution",
          "annotation": "Concrete before/after proof—bug is resolved or iteration continues"
        }
      ],
      "speakerNotes": {
        "talkingPoints": "This is the complete closed-loop workflow. Unlike traditional debugging where you gather info and hand off to an engineer, the agent operates inside the failing environment with real feedback. Each step closes the loop tighter: BUILD provides reproducibility, REPRODUCE provides evidence, PLACE provides agency, INVESTIGATE provides hypothesis, VERIFY provides proof.",
        "timing": "4-5 minutes",
        "discussion": "Walk through a concrete example: 'Memory leak in a Node.js service. Build: Docker with load simulator. Reproduce: Show memory growth. Place: Agent has Bash access. Investigate: Agent reads heap snapshots, correlates with code. Verify: Agent applies fix, re-runs load test, confirms memory stabilizes.' Ask: 'At which step could this have gone wrong if we skipped the next one?'",
        "context": "This workflow works at any scale—local reproduction, staging, or production. The key is that the agent can't speculate about the final step. VERIFY is non-negotiable.",
        "transition": "One critical detail: CLI agents enable this workflow anywhere. IDE agents can't operate in Docker, remote servers, or CI/CD pipelines. Let's clarify the distinction..."
      }
    },
    {
      "type": "comparison",
      "title": "CLI Agents vs IDE Agents for Debugging",
      "left": {
        "label": "IDE Agents (Chat, Copilot Chat)",
        "content": [
          "Tied to local development machine",
          "Can't execute in Docker containers",
          "Can't access remote servers or CI/CD",
          "Limited to code reading and local tools"
        ]
      },
      "right": {
        "label": "CLI Agents (Claude Code, Codex CLI)",
        "content": [
          "Run anywhere you have shell access",
          "Execute inside Docker, remote servers, production",
          "Integrate with CI/CD pipelines seamlessly",
          "Enable true closed-loop debugging at scale"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "This is a critical distinction that many engineers miss. IDE agents are great for code generation and local exploration, but they can't implement closed-loop debugging in production environments. CLI agents can run anywhere you have shell access—that's the entire point. For remote diagnosis, customer deployments, and edge infrastructure, CLI agents are indispensable.",
        "timing": "2-3 minutes",
        "discussion": "Ask: 'How would an IDE agent help debug a production incident on a customer's infrastructure?' The answer is: it can't. But a CLI agent running in their environment (or yours via SSH) can. This is why CLI agents are the choice for production debugging.",
        "context": "This is practical guidance for tool selection. Use IDE agents for local development and exploration. Use CLI agents when you need environment access, especially in production or locked-down systems.",
        "transition": "Sometimes you don't have direct access to failing environments—customer deployments or locked-down production. Let's look at how to debug remotely..."
      }
    },
    {
      "type": "concept",
      "title": "Remote Diagnosis: Scripts Over Direct Access",
      "content": [
        "Generate targeted diagnostic scripts for each hypothesis",
        "Scripts check configuration, versions, timing, cross-references",
        "Send script to customer, load output into agent's context",
        "Agent correlates evidence with hypotheses to identify root cause",
        "Trade developer time for compute time and thoroughness"
      ],
      "speakerNotes": {
        "talkingPoints": "When you can't reproduce bugs locally or access the failing environment—customer deployments, edge infrastructure, locked-down production—you face limited information and no iteration cycle. This is where AI agents' probabilistic reasoning becomes a feature. The workflow: ground yourself in the codebase using code research, research known issues, then have the agent generate ranked hypotheses. Then produce targeted diagnostic scripts that collect evidence for each hypothesis. What would be days of manual work becomes 30 minutes for the agent to generate comprehensive diagnostics.",
        "timing": "3-4 minutes",
        "discussion": "Example: 'Customer reports intermittent 502 errors. You can't reproduce locally. Ask the agent: What are the top 10 hypotheses? What diagnostic data proves/refutes each?' The agent generates a script checking all of them. Customer runs it once, you load the output, agent identifies the root cause. This transforms 'send logs and wait' into active investigation.",
        "context": "This is real-world debugging. Not every bug can be reproduced in your environment. The agent's code generation capability lets you collect evidence systematically without manual back-and-forth with customers.",
        "transition": "Now let's consolidate the key principles and patterns you should remember from this lesson..."
      }
    },
    {
      "type": "takeaway",
      "title": "Key Takeaways",
      "content": [
        "Evidence trumps speculation always",
        "Closed-loop feedback enables verification",
        "AI excels at log analysis",
        "Place agents in failing environments",
        "Generate diagnostic scripts liberally"
      ],
      "speakerNotes": {
        "talkingPoints": "These five principles transform debugging from a research exercise into a systematic, verifiable process. Evidence means you never accept unverified fixes. Closed-loop means the agent can't hide behind speculation. AI excels at log analysis—leverage that. Placement means the agent can test its hypotheses. Liberal script generation means you demand thorough diagnostics instead of settling for manual investigation.",
        "timing": "3 minutes",
        "discussion": "Ask students to commit to one change: 'Next time you find a bug with AI assistance, require proof before accepting the fix. Demand before/after evidence. Build a reproduction first. How does that change your debugging process?'",
        "context": "These principles apply whether you're debugging locally, in staging, or in production. The workflow scales—only the environment and tooling changes.",
        "transition": "You're now equipped with a complete debugging framework. In the next session, we'll look at how to handle edge cases and complex scenarios where debugging becomes particularly challenging..."
      }
    }
  ]
}
