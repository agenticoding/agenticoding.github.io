{
  "metadata": {
    "title": "Debugging with AI Agents",
    "lessonId": "lesson-10-debugging",
    "estimatedDuration": "35-45 minutes",
    "learningObjectives": [
      "Require evidence for every debugging claim",
      "Use AI to analyze logs and correlate patterns",
      "Create reproducible environments systematically",
      "Build closed-loop debugging workflows"
    ]
  },
  "slides": [
    {
      "type": "title",
      "title": "Debugging with AI Agents",
      "subtitle": "From speculation to evidence-driven investigation",
      "content": [],
      "speakerNotes": {
        "talkingPoints": "This lesson shifts debugging from 'what do you think is wrong?' to 'prove the bug exists, then prove your fix works.' We'll explore how AI agents' pattern recognition and code generation capabilities transform debugging from manual investigation to systematic evidence collection and closed-loop verification.",
        "timing": "1 minute",
        "discussion": "As we start, think about the last production bug you debugged. How much time was spent speculating versus collecting concrete evidence?",
        "context": "Debugging with AI requires a fundamentally different approach than traditional debugging. You're not asking the AI to guess—you're building an evidence-rich environment where the AI can verify hypotheses systematically.",
        "transition": "Let's start with the core principle that changes everything about AI-assisted debugging."
      }
    },
    {
      "type": "concept",
      "title": "Evidence Over Speculation",
      "content": [
        "Never accept fixes without reproducible proof",
        "Anti-pattern: Describe bug → hope agent fixes it",
        "Production pattern: Provide data → require verification",
        "Shift from 'what do you think?' to 'prove it works'"
      ],
      "speakerNotes": {
        "talkingPoints": "The fundamental mindset change: AI agents excel at pattern recognition when given concrete data, but fail spectacularly when forced to speculate. Instead of describing a problem and hoping for a solution, you provide reproduction steps, diagnostic data, and require the agent to prove both that the bug exists and that the fix resolves it. This transforms debugging from opinion-based to evidence-based.",
        "timing": "2-3 minutes",
        "discussion": "Ask: When you ask an AI agent 'Can you fix this bug?', what could go wrong? (Answer: It generates code that's never been tested. It's a prediction, not proof.) How do you change your prompt to require evidence?",
        "context": "In production systems, a 'we think we fixed it' is dangerous. You need reproducible test cases that pass before and fail after the fix. This is exactly where AI-assisted debugging differs from traditional pair debugging.",
        "transition": "Before we even get to fixing, let's talk about understanding the code first."
      }
    },
    {
      "type": "concept",
      "title": "Code Inspection: Architecture First",
      "content": [
        "Understand execution flow before looking at logs",
        "Use code search to find relevant components",
        "Trace request paths and data flow patterns",
        "Identify failure points from code structure",
        "Conversation reveals edge cases you missed"
      ],
      "speakerNotes": {
        "talkingPoints": "Don't jump straight to logs. Start by having the agent explain the architecture and how requests flow through the system. Ask: 'Trace the authentication flow from API request to database. Where could a race condition occur?' This conversation-based approach often reveals assumptions and edge cases you missed. The agent doesn't need to read every line—use semantic code search to find critical paths and focus there.",
        "timing": "2-3 minutes",
        "discussion": "Real scenario: You get a production alert. Do you immediately grep logs or start by understanding the code path? What's the advantage of understanding code structure first?",
        "context": "In complex microservices, understanding data flow and message ordering is often more valuable than reading individual log lines. The agent's explanation of how components interact often surfaces the actual problem.",
        "transition": "Once you understand the structure, logs become much more meaningful. Let's talk about AI's superpower: analyzing chaos."
      }
    },
    {
      "type": "concept",
      "title": "Log Analysis: AI's Superpower",
      "content": [
        "Process thousands of lines humans can't parse",
        "Spot patterns across inconsistent log formats",
        "Correlate cascading errors in microservices",
        "Identify timing patterns indicating race conditions",
        "Works with any format: grep output, raw logs, JSON"
      ],
      "speakerNotes": {
        "talkingPoints": "This is where AI agents have a massive advantage over human engineers. Multi-line stack traces, inconsistent formats from different services, verbose debug output—that's where AI excels. What takes senior engineers days of manual correlation happens in minutes. AI spots patterns that are invisible to human analysis: timing correlations between services, specific user cohorts experiencing failures, cascading error chains across fragmented logs.",
        "timing": "2-3 minutes",
        "discussion": "Think about logs you've analyzed. How many times did you manually grep and correlate? What patterns did you miss? How much time would you save if an agent did the correlation?",
        "context": "Structured logs (JSON, consistent timestamps, request IDs) are good engineering practice—they help both humans and AI. But AI can work with whatever you have. The messier your logs, the greater AI's advantage.",
        "transition": "Logs tell you what happened. But sometimes you need to create controlled scenarios to reproduce the bug reliably."
      }
    },
    {
      "type": "concept",
      "title": "Reproduction Scripts: Code is Cheap",
      "content": [
        "Environments that take humans hours to set up",
        "AI generates Docker configs and database snapshots",
        "Capture full context: state, APIs, configuration",
        "Create verifiable test cases trivially",
        "Eliminate ambiguity with concrete reproduction"
      ],
      "speakerNotes": {
        "talkingPoints": "This is where AI's code generation capabilities become a debugging superpower. Complex environments—Kubernetes setups, Docker networks, database state initialization, mock services—take humans hours to set up. AI generates comprehensive reproduction environments in minutes. Instead of 'I can reproduce it sometimes,' you can say 'I have a 100% reliable reproduction script.' This transforms debugging from 'it seems broken' to 'here's proof.'",
        "timing": "2-3 minutes",
        "discussion": "Scenario: A bug only happens with specific database state and timing. How would you normally handle this? How does AI change your approach? What's the cost of generating a comprehensive reproduction environment with AI?",
        "context": "The shift in economics here is significant: AI makes it trivial to add diagnostic logs at dozens of points—far more than humans would—because the agent generates and places them in minutes. Once verified, the agent systematically removes temporary diagnostics, maintaining code hygiene. This shifts debugging from 'minimal instrumentation' to 'evidence-rich exploration.'",
        "transition": "But reproduction scripts only work if the agent can test them. Let's talk about placing agents inside failing environments."
      }
    },
    {
      "type": "codeExecution",
      "title": "Closed-Loop Debugging Workflow",
      "steps": [
        {
          "line": "BUILD: Create reproducible environment\n(Docker, scripts, database snapshots)",
          "highlightType": "human",
          "annotation": "You provide the environment specification"
        },
        {
          "line": "REPRODUCE: Verify bug manifests consistently\n(logs, status codes, error output)",
          "highlightType": "human",
          "annotation": "Establish baseline failure condition"
        },
        {
          "line": "PLACE: Give agent tool access within\nenvironment (shell, files, runtime)",
          "highlightType": "prediction",
          "annotation": "Agent can now execute and observe"
        },
        {
          "line": "INVESTIGATE: Agent correlates three sources\n- Runtime behavior (commands, logs)\n- Codebase (code research tools)\n- Known issues (research, CVEs)",
          "highlightType": "execution",
          "annotation": "Systematic investigation from evidence"
        },
        {
          "line": "Agent forms hypothesis based on evidence",
          "highlightType": "prediction",
          "annotation": "Prediction informed by facts, not guessing"
        },
        {
          "line": "VERIFY: Apply fix, re-run reproduction,\nconfirm resolution or iterate",
          "highlightType": "execution",
          "annotation": "Proof, not just code"
        },
        {
          "line": "Bug is verified fixed or new hypothesis\nforms for next iteration",
          "highlightType": "summary",
          "annotation": "Closed-loop feedback validates reasoning"
        }
      ],
      "speakerNotes": {
        "talkingPoints": "This is the complete debugging workflow with AI. The critical difference from traditional debugging: the agent has a closed-loop environment where it can test its own hypotheses. It doesn't research, propose, and hand off—it researches, fixes, tests, and proves. This transforms debugging from 'the agent is an oracle' to 'the agent is a systematic investigator.'",
        "timing": "3-4 minutes",
        "discussion": "Notice how this differs from open-loop debugging: Agent researches code, says 'I think RS256 validation is missing,' and you manually apply it. With closed-loop: Agent applies the fix, re-runs the reproduction, and reports 'Fixed and verified.' Which would you rather trust in production?",
        "context": "This is why CLI agents (Claude Code, terminal-based assistants) matter more than IDE agents for debugging. CLI agents can run anywhere you have shell access: Docker containers, remote servers, CI/CD pipelines, production instances. IDE agents are stuck on your local machine.",
        "transition": "Speaking of environment access, let's talk about the difference between local and remote debugging."
      }
    },
    {
      "type": "comparison",
      "title": "Open-Loop vs Closed-Loop Debugging",
      "left": {
        "label": "Open-Loop",
        "content": [
          "Agent researches code and logs",
          "Proposes fix without testing",
          "You apply and manually verify",
          "Iteration requires human handoff",
          "Evidence is secondary to reasoning"
        ]
      },
      "right": {
        "label": "Closed-Loop",
        "content": [
          "Agent investigates environment",
          "Applies fix and runs reproduction",
          "Observes result immediately",
          "Iterates automatically on failure",
          "Proof validates hypothesis"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "The difference is profound. Open-loop: Agent predicts → you execute → you verify. Closed-loop: Agent predicts → agent executes → agent observes → agent iterates. With closed-loop, you're not betting on the agent's pattern recognition—you're leveraging it within a system that automatically refutes wrong hypotheses.",
        "timing": "2 minutes",
        "discussion": "Which approach scales better? Which is safer for production? Why does CLI agent access matter for closed-loop?",
        "context": "This is why debugging with AI is fundamentally different from coding with AI. For coding, open-loop works (code compiles, passes tests). For debugging, you need environment access to prove fixes work in context.",
        "transition": "But what if you can't access the failing environment? That's a common scenario in production systems."
      }
    },
    {
      "type": "concept",
      "title": "Remote Diagnosis: Scripts Over Access",
      "content": [
        "Limited access: customer deployments, locked production",
        "Trade developer time for agent compute time",
        "Generate ranked hypotheses from evidence",
        "Create targeted diagnostic scripts trivially",
        "Comprehensive diagnostics in 30 minutes vs days"
      ],
      "speakerNotes": {
        "talkingPoints": "You can't reproduce locally. You can't SSH into the customer's infrastructure. You get logs and error reports. This is where AI's probabilistic reasoning becomes a feature. The agent grounds itself in the codebase (understand architecture around the failing component), researches known issues (similar bugs in the ecosystem), and generates ranked hypotheses based on evidence. Then it produces comprehensive diagnostic scripts that collect evidence for each hypothesis: configuration states, version mismatches, timing data. You send the script to the customer, load the output, and the agent correlates evidence with hypotheses to identify root cause.",
        "timing": "2-3 minutes",
        "discussion": "Scenario: Customer reports random 500 errors. You have logs but can't reproduce. How would you normally debug? How does AI change your approach?",
        "context": "The key insight: Write a comprehensive diagnostic script checking dozens of potential issues in 30 minutes with AI. Humans would spend days. The script runs on customer infrastructure, collects evidence, and the agent analyzes it all at once.",
        "transition": "Let's look at concrete patterns for how to structure your prompts when debugging with agents."
      }
    },
    {
      "type": "codeComparison",
      "title": "Debugging Prompts: Ineffective vs Effective",
      "leftCode": {
        "label": "Ineffective",
        "language": "text",
        "code": "The API is returning 401 errors\nfor some users. Can you debug this?\n\nHere are the logs:\n[pasted 500 lines]"
      },
      "rightCode": {
        "label": "Effective",
        "language": "text",
        "code": "API endpoint POST /api/auth/token\nreturns 401 for specific users.\n\nReproduction:\n1. Create user with enterprise domain\n2. POST with valid credentials\n3. Returns 401 instead of token\n\nLogs show JWT validation failing.\nTrace the authentication flow.\nWhere could enterprise domain\ncause validation failure?"
      },
      "speakerNotes": {
        "talkingPoints": "Ineffective prompt: 'Here's a problem and some logs, fix it.' The agent has no context, no reproduction, no clear scope. Effective prompt: 'Here's exactly how to reproduce it, here's what we observe, now explain the code path that's failing.' The second gives the agent concrete evidence to work with. It also shows you've done investigative work first—you're not asking the agent to figure out what the problem is, you're asking it to explain why the evidence points to a specific cause.",
        "timing": "2-3 minutes",
        "discussion": "What information is missing from the ineffective prompt? Why does the effective prompt start with reproduction steps? How does that change the agent's investigation?",
        "context": "Good debugging prompts follow this pattern: 'Here's the reproduction, here's what we observe, here's what we've already checked, now help me understand X.' You're positioning the agent as a research partner, not an oracle.",
        "transition": "Now let's talk about what not to do—common pitfalls that waste time."
      }
    },
    {
      "type": "concept",
      "title": "Debugging Anti-Patterns to Avoid",
      "content": [
        "Don't describe symptoms without reproduction",
        "Don't ask agent to analyze without environment access",
        "Don't accept 'probably' answers—require evidence",
        "Don't skip code inspection for log diving",
        "Don't leave temporary diagnostics in production"
      ],
      "speakerNotes": {
        "talkingPoints": "Anti-pattern 1: 'Users report X is slow.' Agent: 'Maybe it's a database query.' You: 'Let me check.' You've wasted context and iteration. Instead: Create a reproduction script, profile it with the agent's help, get concrete evidence. Anti-pattern 2: 'Fix this error.' Agent applies change. You deploy. Now you're taking risks. Pattern: Agent fixes, reproduces failure locally, confirms fix, you review before deploying. Anti-pattern 3: Leaving diagnostic logs in code. The agent adds 20 log statements to debug. You must have the agent remove them—this is trivial for AI but easy to forget.",
        "timing": "2-3 minutes",
        "discussion": "From your experience: What's the most common debugging mistake with AI agents? (Likely: trusting code without testing.) How do the patterns we've discussed prevent that?",
        "context": "These anti-patterns will waste your time if you're not deliberate. They're tempting because they seem faster—ask, get answer, move on. But in reality, they introduce risk and iteration cycles.",
        "transition": "Let's bring this all together with a real example of the complete workflow."
      }
    },
    {
      "type": "codeExecution",
      "title": "Real Debugging Scenario: Complete Workflow",
      "steps": [
        {
          "line": "Engineer reports: 'Webhook timeouts\nfor specific merchants, not reproducible locally'",
          "highlightType": "human",
          "annotation": "Problem statement with symptoms"
        },
        {
          "line": "Agent request: 'Create reproduction script\nwith production-like data volume and timing'",
          "highlightType": "prediction",
          "annotation": "Evidence-first approach"
        },
        {
          "line": "Engineer provides database snapshot,\nwebhook payload samples, merchant configs",
          "highlightType": "human",
          "annotation": "Concrete grounding data"
        },
        {
          "line": "Agent generates: Docker Compose setup,\nscript that simulates high-volume webhooks",
          "highlightType": "execution",
          "annotation": "Reproduces the scenario"
        },
        {
          "line": "Engineer runs: Reproduces timeout\nconsistently in local environment",
          "highlightType": "feedback",
          "annotation": "Confirms bug exists in controlled setting"
        },
        {
          "line": "Agent investigates: Reads webhook code,\nanalyzes timeout patterns in reproduction logs",
          "highlightType": "execution",
          "annotation": "Systematic investigation with evidence"
        },
        {
          "line": "Agent predicts: 'Missing connection pool\nrecycling under sustained load. Connections\nget stale after ~2K webhooks.'",
          "highlightType": "prediction",
          "annotation": "Hypothesis grounded in evidence"
        },
        {
          "line": "Agent fixes: Adds connection pool config,\nre-runs reproduction, confirms timeouts stop",
          "highlightType": "execution",
          "annotation": "Proof through execution"
        },
        {
          "line": "Engineer reviews fix, verifies against\nproduction merchant counts, deploys",
          "highlightType": "summary",
          "annotation": "Confident deployment from evidence"
        }
      ],
      "speakerNotes": {
        "talkingPoints": "Walk through this complete scenario: Problem comes in with vague symptoms. Instead of asking 'why?', the agent asks 'how do I reproduce it?' With production data and context, the agent builds a reproducible environment. Now the bug is visible and testable. The agent investigates the code and logs together, forms a hypothesis, applies a fix, and proves it works—all within the reproduction environment. Only then does the engineer review and deploy. This is the closed-loop pattern working perfectly.",
        "timing": "3-4 minutes",
        "discussion": "Where would this have gone wrong with traditional debugging? (No reproduction, hard to verify fix.) How does the reproduction script change the timeline? What's the risk reduction?",
        "context": "This workflow is very different from 'Hey agent, fix this bug.' It's: 'Here's the problem with reproduction data, help me understand it, verify your fix works, then we deploy together.' You're a team.",
        "transition": "Let's summarize the key principles that make AI-assisted debugging work."
      }
    },
    {
      "type": "takeaway",
      "title": "Debugging Principles That Work",
      "content": [
        "Evidence always > speculation—require reproducible proof",
        "Understand code architecture before diving into logs",
        "Logs are AI's strength—process chaos humans can't parse",
        "Reproduction scripts eliminate ambiguity and enable iteration",
        "Closed-loop debugging: investigate, fix, test, prove"
      ],
      "speakerNotes": {
        "talkingPoints": "These five principles form the foundation of effective AI-assisted debugging. Each one shifts you away from opinion-based debugging toward evidence-based debugging. They work together: you build understanding (code inspection), create reproducible scenarios (scripts), analyze evidence systematically (logs + code), and close the loop (test your fixes before deploying). This approach scales from local development to remote production diagnosis.",
        "timing": "1-2 minutes",
        "discussion": "Which principle resonates most with your debugging experience? Which would have biggest impact on your team?",
        "context": "These aren't just 'nice to have' practices. In production systems, they're essential. Debugging without evidence is how you ship broken fixes or waste days on speculation.",
        "transition": "You now have the frameworks and patterns. The real skill is knowing when to apply each one. Practice with your team."
      }
    },
    {
      "type": "concept",
      "title": "Debugging Workflow Decision Tree",
      "content": [
        "Can you reproduce locally? → BUILD reproduction script",
        "Bug visible in logs? → Have agent ANALYZE patterns",
        "Logs unclear? → Add DIAGNOSTIC instrumentation",
        "Need to iterate quickly? → Use CLOSED-LOOP environment",
        "Can't access environment? → Generate DIAGNOSTIC scripts"
      ],
      "speakerNotes": {
        "talkingPoints": "This is your decision tree for choosing which approach to use. Different scenarios call for different strategies. Local reproducible bugs? Build a test that fails and iterates quickly. Remote production issues? Generate comprehensive diagnostic scripts. The common thread: always have concrete evidence before accepting a fix. The tools and patterns change, but the principle stays constant.",
        "timing": "2 minutes",
        "discussion": "Think about a recent bug on your team. Where would it fit in this tree? What's your team's current approach to debugging with AI? How would these patterns change it?",
        "context": "Your team will develop its own variations on these patterns as you gain experience. This is your starting point—the fundamental patterns that work across different systems and teams.",
        "transition": "You're ready to apply these patterns to your own debugging workflows. Start with one: choose reproduction scripts or log analysis first, master that, then expand."
      }
    }
  ]
}
