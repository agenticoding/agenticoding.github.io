{
  "metadata": {
    "title": "Debugging with AI Agents",
    "lessonId": "lesson-10-debugging",
    "estimatedDuration": "35-45 minutes",
    "learningObjectives": [
      "Require evidence for fixes",
      "Master closed-loop debugging",
      "Leverage AI log analysis",
      "Generate diagnostic scripts"
    ]
  },
  "slides": [
    {
      "type": "title",
      "title": "Debugging with AI Agents",
      "subtitle": "Evidence-based debugging workflows",
      "content": [],
      "speakerNotes": {
        "talkingPoints": "This lesson transforms how you debug with AI. The core shift: from describing symptoms to demanding proof. We'll cover systematic investigation, log analysis, reproduction scripts, and closed-loop debugging where agents verify their own fixes.",
        "timing": "1 minute",
        "discussion": "How many of you have asked an AI to fix a bug and gotten a confident but wrong answer?",
        "context": "Senior engineers often struggle with AI debugging because they treat it like pair programming—describing problems verbally. AI excels when given concrete data and required to prove fixes work.",
        "transition": "Let's start with the fundamental principle that changes everything..."
      }
    },
    {
      "type": "concept",
      "title": "The Core Principle: Evidence Over Speculation",
      "content": [
        "Never accept fixes without reproducible proof",
        "Move from 'what's wrong?' to 'prove it'",
        "AI excels with concrete data, fails with speculation",
        "Require before/after evidence for every fix"
      ],
      "speakerNotes": {
        "talkingPoints": "The fundamental shift in AI debugging is moving from speculation to evidence. AI agents are powerful pattern recognizers when given concrete data, but they hallucinate confidently when forced to guess. Your job is to create conditions where evidence is abundant.",
        "timing": "2 minutes",
        "discussion": "What's the difference between a bug description and bug evidence? Have students share examples of each.",
        "context": "In production, accepting unverified fixes leads to chained bugs—the 'fix' breaks something else, which gets another unverified fix. Evidence-based debugging breaks this cycle.",
        "transition": "Let's look at a systematic prompt structure that enforces evidence-based investigation..."
      }
    },
    {
      "type": "code",
      "title": "Evidence-Based Debugging Prompt",
      "language": "markdown",
      "code": "```\n$ERROR_DESCRIPTION\n```\n\nUse the code research to analyze the error above.\n\nINVESTIGATE:\n\n1. Read relevant source files and trace the code path\n2. Examine error messages, stack traces, and logs\n3. Identify the specific location of the failure\n4. Understand the surrounding architecture and data flow\n\nANALYZE:\n\n5. Compare expected vs actual behavior\n6. Identify the root cause of the failure\n7. Determine if related issues exist elsewhere\n\nEXPLAIN:\n\nProvide your root cause analysis with evidence:\n- File paths and line numbers (`src/auth/jwt.ts:45-67`)\n- Actual values from code (`port: 8080`)\n- Specific identifiers (`validateJWT()`)\n- Exact error messages\n\nThen propose a fix.",
      "caption": "Structured prompt forcing evidence collection before fixes",
      "speakerNotes": {
        "talkingPoints": "This prompt structure prevents hallucination by requiring the agent to gather evidence before proposing solutions. Notice the three phases: INVESTIGATE forces code reading, ANALYZE requires comparison, and EXPLAIN demands specific citations. The agent can't skip to guessing.",
        "timing": "3-4 minutes",
        "discussion": "Why does requiring file paths and line numbers in the output change agent behavior? It forces actual code retrieval.",
        "context": "In production debugging, vague answers like 'there might be a race condition' are useless. This prompt structure produces actionable output: 'Race condition at OrderService.ts:89 where inventory check and decrement aren't atomic.'",
        "transition": "Before diving into logs or reproduction, start with root cause analysis..."
      }
    },
    {
      "type": "concept",
      "title": "Root Cause Analysis: The 5 Whys",
      "content": [
        "Identify fundamental causes, not symptoms",
        "Have agent explain architecture and execution flow",
        "Use semantic search to find relevant components",
        "Apply 5 Whys: don't stop at first answer",
        "Agent explanations reveal missed edge cases"
      ],
      "speakerNotes": {
        "talkingPoints": "Root cause analysis means understanding before fixing. Have the agent trace request paths, explain data flow, and identify failure points. The 5 Whys technique is powerful here: 'Where could a race condition occur? Why would it happen? Why isn't it handled?' Each 'why' exposes deeper causes.",
        "timing": "2-3 minutes",
        "discussion": "When was the last time you fixed a symptom instead of a root cause? What happened later?",
        "context": "Production bugs often have multiple contributing factors. The 5 Whys technique with AI reveals the chain: 'Why timeout? Connection pool exhausted. Why? Unclosed connections. Why? Missing finally block. Why? Copied pattern from legacy code without cleanup.'",
        "transition": "Once you understand the system, logs become your richest evidence source..."
      }
    },
    {
      "type": "concept",
      "title": "Log Analysis: AI's Superpower",
      "content": [
        "AI excels at messy, unstructured logs",
        "Multi-line traces across thousands of entries",
        "Spots patterns humans can't parse manually",
        "Correlates timing, cascading errors, user cohorts",
        "Works with whatever you have—no perfect format needed"
      ],
      "speakerNotes": {
        "talkingPoints": "Log analysis is where AI has the biggest advantage over humans. Multi-line stack traces scattered across thousands of entries? Inconsistent formats from different services? AI processes chaos humans can't parse. What takes senior engineers days of correlation happens in minutes.",
        "timing": "2-3 minutes",
        "discussion": "What's your current log analysis workflow? How long does it take to correlate errors across services?",
        "context": "AI spots patterns across log formats: cascading errors in microservices with different logging styles, timing patterns indicating race conditions, specific user cohorts experiencing failures. The messier the logs, the more AI outpaces human capability.",
        "transition": "AI also transforms how we instrument code for debugging..."
      }
    },
    {
      "type": "concept",
      "title": "Diagnostic Logging: Evidence-Rich Exploration",
      "content": [
        "AI makes heavy instrumentation trivial",
        "Add logs at dozens of strategic points in minutes",
        "Agent guides what to log based on hypothesis",
        "Analyze new output immediately",
        "Remove all temporary logs after fix verified"
      ],
      "speakerNotes": {
        "talkingPoints": "This insight transforms debugging economics. AI makes it trivial to add diagnostic logs at dozens of strategic points—far more than humans would manually instrument. Once the bug is verified fixed, the same agent removes all temporary statements, restoring code hygiene.",
        "timing": "2 minutes",
        "discussion": "How often do you avoid adding diagnostic logs because removing them later is tedious?",
        "context": "What would be prohibitively tedious for humans (add logs, analyze, remove logs) becomes routine with AI. Fifteen minutes writing specific log output beats hours of speculation.",
        "transition": "When logs aren't enough, reproduction scripts provide bulletproof evidence..."
      }
    },
    {
      "type": "concept",
      "title": "Reproduction Scripts: Code is Cheap",
      "content": [
        "Environments taking hours manually take AI minutes",
        "Capture full context: DB state, configs, mocks",
        "Eliminate ambiguity with verifiable test cases",
        "Docker for isolated reproduction environments",
        "Agent iterates fixes against reliable reproduction"
      ],
      "speakerNotes": {
        "talkingPoints": "When code inspection and log analysis aren't sufficient, reproduction scripts become invaluable. This is where AI's massive code generation capabilities shine: K8s configs, Docker setups, database snapshots, mock services—what takes humans hours takes AI minutes.",
        "timing": "2-3 minutes",
        "discussion": "What's the most complex reproduction environment you've had to set up? How long did it take?",
        "context": "Reproduction scripts eliminate ambiguity. They capture full context: database state, external API responses, configuration, user inputs. Once you have reliable reproduction, the agent can iterate on fixes and verify each attempt.",
        "transition": "Now we reach the most powerful pattern: closing the loop..."
      }
    },
    {
      "type": "codeExecution",
      "title": "Closed-Loop Debugging Workflow",
      "steps": [
        {
          "line": "1. BUILD - Create reproducible environment",
          "highlightType": "human",
          "annotation": "Docker, scripts, database snapshots that reliably trigger bug"
        },
        {
          "line": "2. REPRODUCE - Verify bug manifests consistently",
          "highlightType": "execution",
          "annotation": "Concrete evidence: logs, status codes, error output"
        },
        {
          "line": "3. PLACE - Give agent tool access WITHIN environment",
          "highlightType": "human",
          "annotation": "Not just code—runtime execution capabilities"
        },
        {
          "line": "4. INVESTIGATE - Agent correlates runtime + codebase + known issues",
          "highlightType": "prediction",
          "annotation": "Forms hypotheses using grounding techniques"
        },
        {
          "line": "5. VERIFY - Agent applies fix, re-runs reproduction",
          "highlightType": "execution",
          "annotation": "Confirms bug resolved or iterates with new hypothesis"
        }
      ],
      "speakerNotes": {
        "talkingPoints": "This workflow transforms debugging from 'research and guess' to 'research, fix, test, and prove.' The key is step 3: placing the agent INSIDE the failing environment with tool access. Without environment access, agents propose solutions they can't validate. With closed-loop access, they apply fixes and prove they work.",
        "timing": "3-4 minutes",
        "discussion": "What's the difference between an agent that proposes 'try adding algorithm validation' versus one that says 'Fixed and verified—reproduction now passes'?",
        "context": "An open-loop agent researches and reports hypotheses. A closed-loop agent applies fixes, re-runs reproduction, and proves they work—or iterates when they don't. This is the difference between debugging assistance and debugging automation.",
        "transition": "CLI agents have a key advantage for closed-loop debugging..."
      }
    },
    {
      "type": "comparison",
      "title": "CLI Agents vs IDE Assistants for Debugging",
      "left": {
        "label": "IDE Assistants",
        "content": [
          "Tied to local development machine",
          "Can't access Docker containers directly",
          "No remote server access",
          "Limited to local file system"
        ]
      },
      "right": {
        "label": "CLI Agents",
        "content": [
          "Run anywhere with shell access",
          "Inside Docker containers",
          "On remote servers and CI/CD",
          "On problematic production instances"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "CLI agents like Claude Code, Codex, and Copilot CLI shine for closed-loop debugging because they work anywhere you have shell access. IDE agents are tied to your local machine. When the bug only reproduces in a specific environment, CLI agents can investigate directly.",
        "timing": "2-3 minutes",
        "discussion": "How often do bugs only reproduce in specific environments? Production-only, customer deployments, specific configurations?",
        "context": "This is why CLI mastery matters for senior engineers. The bug might only appear in the customer's Kubernetes cluster, in your CI/CD pipeline, or on a specific production node. CLI agents go where the bug lives.",
        "transition": "But what about environments you can't access at all?"
      }
    },
    {
      "type": "concept",
      "title": "Remote Diagnosis: Scripts Over Access",
      "content": [
        "Customer deployments and locked-down production",
        "AI's probabilistic reasoning becomes a feature",
        "Generate ranked hypotheses based on evidence",
        "Produce targeted diagnostic scripts per hypothesis",
        "Trade developer time for compute time"
      ],
      "speakerNotes": {
        "talkingPoints": "When you can't access the failing environment—customer deployments, edge infrastructure, locked-down production—AI's probabilistic reasoning becomes a feature. Ground yourself in the codebase, search for known issues, then have the agent generate comprehensive diagnostic scripts that collect evidence for each hypothesis.",
        "timing": "2-3 minutes",
        "discussion": "How do you currently handle bugs in customer environments you can't access?",
        "context": "Writing comprehensive diagnostic scripts takes humans days but takes agents 30 minutes. Send the script to the customer, load output into context, and the agent correlates evidence with hypotheses. What would be tedious manual work becomes a simple prompt.",
        "transition": "Let's summarize the key principles for debugging with AI..."
      }
    },
    {
      "type": "takeaway",
      "title": "Key Takeaways",
      "content": [
        "Demand proof for every fix",
        "Log analysis is AI's superpower",
        "Code is cheap—write liberally",
        "Close the loop with BUILD→VERIFY",
        "CLI agents access any environment"
      ],
      "speakerNotes": {
        "talkingPoints": "Debugging with AI is about building diagnostic environments where evidence is abundant and verification is systematic. The agent is your tireless investigator—give it the tools and demand proof. Never accept a fix without reproducible evidence it works.",
        "timing": "2 minutes",
        "discussion": "Which of these principles will most change your debugging workflow? What's the first bug you'll try this approach on?",
        "context": "The shift from 'help me debug this' to 'prove this fix works' transforms AI from an unreliable assistant into a powerful investigation tool. Evidence-based debugging eliminates the guesswork that makes AI debugging frustrating.",
        "transition": "Practice these patterns on your next production bug. The difference is immediate and dramatic."
      }
    }
  ]
}
