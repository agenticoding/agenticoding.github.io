{
  "metadata": {
    "title": "Debugging with AI Agents",
    "lessonId": "lesson-10-debugging",
    "estimatedDuration": "35-45 minutes",
    "learningObjectives": [
      "Require evidence before fixes",
      "Master closed-loop debugging workflow",
      "Leverage AI log analysis",
      "Generate diagnostic scripts remotely"
    ]
  },
  "slides": [
    {
      "type": "title",
      "title": "Debugging with AI Agents",
      "subtitle": "Evidence over speculation",
      "content": [],
      "speakerNotes": {
        "talkingPoints": "This lesson transforms how you debug with AI. The core shift: never accept a fix without reproducible proof it works. AI agents excel at systematic investigation with concrete data but fail when forced to speculate.",
        "timing": "1 minute",
        "discussion": "How many times have you asked an AI to fix a bug and it proposed something that didn't work?",
        "context": "In production, blind fixes waste hours. Evidence-based debugging cuts iteration cycles dramatically.",
        "transition": "Let's start with the fundamental principle that changes everything..."
      }
    },
    {
      "type": "comparison",
      "title": "The Debugging Mindset Shift",
      "left": {
        "label": "Speculation-Based",
        "content": [
          "Describe symptoms, hope for solutions",
          "Accept first proposed fix",
          "Trust pattern matching alone",
          "Skip reproduction steps"
        ]
      },
      "right": {
        "label": "Evidence-Based",
        "content": [
          "Provide concrete data and access",
          "Require before/after proof",
          "Combine patterns with verification",
          "Build reproducible environments"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "The fundamental shift is from 'what do you think is wrong?' to 'prove the bug exists, then prove your fix works.' AI excels at pattern recognition when given concrete data, but fails spectacularly when forced to speculate.",
        "timing": "2-3 minutes",
        "discussion": "What's the worst debugging experience you've had with AI? Was it due to speculation?",
        "context": "This applies to any AI coding assistant—Claude Code, Copilot, Cursor, all of them.",
        "transition": "Let's look at a systematic workflow that enforces evidence collection..."
      }
    },
    {
      "type": "code",
      "title": "Evidence-Based Debugging Prompt",
      "language": "markdown",
      "code": "```\n$ERROR_DESCRIPTION\n```\n\nUse the code research to analyze the error above.\n\nINVESTIGATE:\n\n1. Read relevant source files and trace the code path\n2. Examine error messages, stack traces, and logs\n3. Identify the specific location of the failure\n4. Understand the surrounding architecture and data flow\n\nANALYZE:\n\n5. Compare expected vs actual behavior\n6. Identify the root cause of the failure\n7. Determine if related issues exist elsewhere\n\nEXPLAIN:\n\nProvide your root cause analysis with evidence:\n- File paths and line numbers (`src/auth/jwt.ts:45-67`)\n- Actual values from code (`port: 8080`)\n- Specific identifiers (`validateJWT()`)\n- Exact error messages\n\nThen propose a fix.",
      "caption": "Systematic workflow combining Chain-of-Thought, grounding, and evidence requirements",
      "speakerNotes": {
        "talkingPoints": "This prompt structure forces evidence at every step. INVESTIGATE requires the agent to read actual code. ANALYZE demands comparison of expected vs actual. EXPLAIN requires concrete citations—file paths, line numbers, actual values. No speculation allowed.",
        "timing": "3-4 minutes",
        "discussion": "Notice the structure: investigate, analyze, explain. How does this compare to how you currently prompt for debugging help?",
        "context": "The evidence requirements in EXPLAIN are critical—they prevent hallucination by demanding specific citations the agent must verify.",
        "transition": "Before diving into code, we should understand the system architecture first..."
      }
    },
    {
      "type": "concept",
      "title": "Root Cause Analysis: Understanding Before Fixing",
      "content": [
        "Have agent explain architecture and execution flow first",
        "Use semantic code search to find relevant components",
        "Apply the 5 Whys technique—don't stop at first answer",
        "Identify mismatches between mental model and actual behavior",
        "Trace request paths to find potential failure points"
      ],
      "speakerNotes": {
        "talkingPoints": "Before logs or reproduction, start with root cause analysis. Have the agent trace request paths and explain data flow. The 5 Whys technique is powerful: Why did it fail? Why wasn't that handled? Why was that assumption made? Each 'why' exposes deeper causes.",
        "timing": "2-3 minutes",
        "discussion": "When was the last time you traced a full request path before fixing a bug? What did you discover?",
        "context": "This isn't about reading every line—use semantic search to find critical paths, then focus conversation on those areas.",
        "transition": "Once we understand the architecture, logs become incredibly powerful..."
      }
    },
    {
      "type": "concept",
      "title": "Log Analysis: AI's Superpower",
      "content": [
        "AI excels with messy logs humans struggle to parse",
        "Spots patterns across inconsistent formats and services",
        "Correlates timing patterns indicating race conditions",
        "Identifies cascading errors in microservices",
        "Processes thousands of lines in minutes vs days"
      ],
      "speakerNotes": {
        "talkingPoints": "Multi-line stack traces scattered across thousands of entries? Inconsistent formats from different services? Raw debug output? This is where AI has the biggest advantage—processing chaos humans can't parse manually. What takes senior engineers days happens in minutes.",
        "timing": "2-3 minutes",
        "discussion": "Think of a recent debugging session that involved correlating logs from multiple services. How long did it take?",
        "context": "Give agents access however works: paste grep output, pipe scripts, upload raw files, or direct CLI access. AI doesn't need perfect JSON with correlation IDs—it parses whatever you have.",
        "transition": "AI can even help you add better logging on the fly..."
      }
    },
    {
      "type": "concept",
      "title": "Dynamic Diagnostic Instrumentation",
      "content": [
        "Add targeted diagnostic logs during investigation",
        "Agent guides what to log based on hypothesis",
        "Instrument dozens of points in minutes—trivial for AI",
        "Analyze new output immediately",
        "Remove all temporary logs after fix verified"
      ],
      "speakerNotes": {
        "talkingPoints": "Here's a game-changer: AI makes it trivial to add diagnostic logs at dozens of strategic points. What would be prohibitively tedious for humans becomes routine. Add logs, analyze, fix, then have the agent remove all temporary statements. Evidence-rich exploration without code pollution.",
        "timing": "2 minutes",
        "discussion": "How often do you avoid adding logs because cleanup is tedious?",
        "context": "Fifteen minutes writing specific log output beats hours of speculation. The agent generates and places them in minutes.",
        "transition": "Sometimes inspection and logs aren't enough—we need reproducible environments..."
      }
    },
    {
      "type": "concept",
      "title": "Reproduction Scripts: Code is Cheap",
      "content": [
        "AI generates complex environments in minutes",
        "Capture full context: DB state, APIs, config, inputs",
        "Use Docker for isolated reproduction",
        "Iterate fixes against reliable reproduction",
        "Verify each attempt with concrete evidence"
      ],
      "speakerNotes": {
        "talkingPoints": "When you need bulletproof evidence or must reproduce complex timing conditions, AI's code generation shines. Environments that take humans hours—K8s configs, database snapshots, mock services—take AI minutes. Ask for the scaffolding, get comprehensive reproduction.",
        "timing": "2-3 minutes",
        "discussion": "What's the most complex reproduction environment you've built manually? How long did it take?",
        "context": "Reproduction scripts eliminate ambiguity. Once you have reliable reproduction, the agent iterates on fixes and verifies each attempt.",
        "transition": "But the real power comes from placing agents inside failing environments..."
      }
    },
    {
      "type": "comparison",
      "title": "Open-Loop vs Closed-Loop Debugging",
      "left": {
        "label": "Open-Loop",
        "content": [
          "Agent researches code and online issues",
          "Proposes solutions it can't validate",
          "Reports: 'Try adding algorithm validation'",
          "Human must test and iterate"
        ]
      },
      "right": {
        "label": "Closed-Loop",
        "content": [
          "Agent has runtime execution capabilities",
          "Applies fix and re-runs reproduction",
          "Reports: 'Fixed and verified—now passes'",
          "Iterates until evidence confirms fix"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "Open-loop: agent reports 'The bug is likely missing RS256 validation at jwt.ts:67—try adding algorithm validation.' Closed-loop: agent applies that fix, re-runs the failing request, observes it returns 401 correctly, reports 'Fixed and verified.' The difference is proof.",
        "timing": "3 minutes",
        "discussion": "How often do you give your AI assistant access to actually run the code it suggests?",
        "context": "Without environment access, agents propose solutions they can't validate. With it, they iterate until the fix is proven.",
        "transition": "Let's look at the complete closed-loop workflow..."
      }
    },
    {
      "type": "codeExecution",
      "title": "The Closed-Loop Debugging Workflow",
      "steps": [
        {
          "line": "1. BUILD - Create reproducible environment",
          "highlightType": "human",
          "annotation": "Docker, scripts, database snapshots that trigger the bug"
        },
        {
          "line": "2. REPRODUCE - Verify bug manifests consistently",
          "highlightType": "execution",
          "annotation": "Concrete evidence: logs, status codes, error output"
        },
        {
          "line": "3. PLACE - Give agent tool access WITHIN environment",
          "highlightType": "human",
          "annotation": "Not just code access—runtime execution capabilities"
        },
        {
          "line": "4. INVESTIGATE - Agent correlates runtime + codebase + known issues",
          "highlightType": "prediction",
          "annotation": "Executes diagnostics, uses code research, searches for similar bugs"
        },
        {
          "line": "5. VERIFY - Agent applies fix, re-runs reproduction",
          "highlightType": "execution",
          "annotation": "Confirms bug resolved—or forms new hypothesis and iterates"
        },
        {
          "line": "Loop continues until evidence confirms fix",
          "highlightType": "summary",
          "annotation": "Research, fix, test, prove—closed feedback loop"
        }
      ],
      "speakerNotes": {
        "talkingPoints": "BUILD creates the reproducible environment. REPRODUCE verifies it fails consistently. PLACE gives the agent runtime access inside that environment. INVESTIGATE combines all grounding techniques. VERIFY closes the loop—apply fix, re-run, prove it works or iterate.",
        "timing": "3-4 minutes",
        "discussion": "Which step do most debugging sessions skip? What's the impact?",
        "context": "This transforms debugging from 'research and guess' to 'research, fix, test, and prove.'",
        "transition": "CLI agents are uniquely suited for this workflow..."
      }
    },
    {
      "type": "comparison",
      "title": "CLI Agents vs IDE Agents for Debugging",
      "left": {
        "label": "IDE Agents",
        "content": [
          "Tied to local development machine",
          "Limited to IDE's runtime context",
          "Can't access remote environments",
          "Good for local development"
        ]
      },
      "right": {
        "label": "CLI Agents",
        "content": [
          "Run anywhere with shell access",
          "Inside Docker containers",
          "On remote servers and CI/CD",
          "On problematic production instances"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "CLI agents like Claude Code, Codex CLI, and Copilot CLI shine here because they run anywhere you have shell access. Docker containers, remote servers, CI/CD pipelines, production instances. IDE agents are tied to your local machine.",
        "timing": "2 minutes",
        "discussion": "Have you tried running a CLI agent inside a Docker container or on a remote server?",
        "context": "This is a key architectural advantage of CLI agents over IDE assistants for debugging workflows.",
        "transition": "But what about when you can't access the failing environment at all?"
      }
    },
    {
      "type": "concept",
      "title": "Remote Diagnosis: Scripts Over Access",
      "content": [
        "Ground in codebase and known issues first",
        "Generate ranked hypotheses based on evidence",
        "Produce targeted diagnostic scripts for each hypothesis",
        "Trade developer time for compute time",
        "Correlate script output to identify root cause"
      ],
      "speakerNotes": {
        "talkingPoints": "When you can't access the failing environment—customer deployments, locked-down production—AI's probabilistic reasoning becomes a feature. Ground yourself in the code, search for similar issues, then generate comprehensive diagnostic scripts that collect evidence for each hypothesis.",
        "timing": "2-3 minutes",
        "discussion": "How do you currently handle debugging customer environments you can't access?",
        "context": "Writing thorough diagnostics takes humans days but agents 30 minutes. Scripts that check dozens of issues, cross-reference config, output structured data.",
        "transition": "Let's wrap up with the key principles to remember..."
      }
    },
    {
      "type": "takeaway",
      "title": "Key Takeaways",
      "content": [
        "Require reproducible proof for fixes",
        "Log analysis is AI's superpower",
        "Closed-loop verifies agent reasoning",
        "CLI agents access any environment",
        "Generate diagnostic scripts remotely"
      ],
      "speakerNotes": {
        "talkingPoints": "Remember: evidence over speculation—never accept fixes without proof. AI processes chaotic logs humans can't. Closed-loop means placing agents inside failing environments. CLI agents work anywhere with shell access. When you can't access environments, generate diagnostic scripts.",
        "timing": "2 minutes",
        "discussion": "Which technique will you try first in your next debugging session?",
        "context": "Debugging with AI is about building diagnostic environments where evidence is abundant and verification is systematic.",
        "transition": "The agent is your tireless investigator—give it the tools and demand proof."
      }
    }
  ]
}