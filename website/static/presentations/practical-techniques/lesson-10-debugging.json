{
  "metadata": {
    "title": "Debugging with AI Agents",
    "lessonId": "lesson-10-debugging",
    "estimatedDuration": "40-50 minutes",
    "learningObjectives": [
      "Require evidence at every step",
      "Leverage AI for log analysis",
      "Build closed-loop debugging workflows",
      "Generate diagnostic scripts effortlessly"
    ]
  },
  "slides": [
    {
      "type": "title",
      "title": "Debugging with AI Agents",
      "subtitle": "Evidence-Based Investigation and Closed-Loop Verification",
      "content": [],
      "speakerNotes": {
        "talkingPoints": "Debugging with AI agents requires a fundamental shift: from describing symptoms and hoping for solutions to requiring reproducible proof at every step. The core principle is never accepting a fix without evidence it works.",
        "timing": "1 minute",
        "discussion": "Ask students: How do you currently debug production issues? Do you verify fixes before deploying?",
        "context": "This lesson focuses on production debugging workflows where speculation is expensive and evidence is critical.",
        "transition": "Let's start with the fundamental principle that transforms AI debugging from guesswork to systematic investigation."
      }
    },
    {
      "type": "concept",
      "title": "The Fundamental Shift: Evidence Over Speculation",
      "content": [
        "Never accept fixes without reproducible proof",
        "Provide reproduction steps and diagnostic tool access",
        "Require before/after evidence for every change",
        "Move from 'what do you think?' to 'prove it works'"
      ],
      "speakerNotes": {
        "talkingPoints": "The anti-pattern is describing a bug and asking the agent to fix it blindly. The production pattern is providing reproduction steps, giving the agent access to diagnostic tools, and requiring before/after evidence. AI agents excel at pattern recognition and systematic investigation when given concrete data, but fail spectacularly when forced to speculate.",
        "timing": "3-4 minutes",
        "discussion": "Have students share examples of debugging failures caused by unverified fixes. What was the cost of deploying an untested solution?",
        "context": "In production environments, deploying speculative fixes causes cascading failures. Evidence-based debugging prevents these catastrophic outcomes.",
        "transition": "Before diving into logs or reproduction, let's understand the system architecture through code inspection."
      }
    },
    {
      "type": "concept",
      "title": "Code Inspection: Understanding Before Fixing",
      "content": [
        "Have agent explain architecture and execution flow",
        "Use semantic search to find relevant components",
        "Ask agent to trace request paths and data flow",
        "Identify mismatches between mental model and reality"
      ],
      "speakerNotes": {
        "talkingPoints": "Use conversational analysis to identify mismatches between your mental model and actual system behavior. This isn't about having the agent read every line of code—use semantic code search and research tools to find relevant components, then focus the conversation on critical paths. For example: 'Trace the authentication flow from API request to database query. Where could a race condition occur?' The agent's explanation often reveals edge cases or assumptions you missed.",
        "timing": "4-5 minutes",
        "discussion": "Ask: When was the last time your mental model of a system was wrong? How did you discover the mismatch?",
        "context": "Code inspection catches architectural misunderstandings that lead to incorrect hypotheses. It's faster to understand the system first than to chase wrong theories.",
        "transition": "Now that we understand the architecture, let's look at where AI agents truly excel: log analysis."
      }
    },
    {
      "type": "concept",
      "title": "Log Analysis: AI's Superpower",
      "content": [
        "Processes messy logs humans can't parse manually",
        "Spots patterns across inconsistent formats",
        "Correlates cascading errors in minutes, not days",
        "Works with whatever logs you have—no perfect structure needed"
      ],
      "speakerNotes": {
        "talkingPoints": "AI agents excel with the messy logs humans struggle with. Multi-line stack traces scattered across thousands of entries? Inconsistent formats from different services? Raw debug output without structured fields? That's where AI has the biggest advantage—processing chaos humans can't parse manually. What takes senior engineers days of manual correlation happens in minutes. The messier the logs, the more AI's pattern recognition outpaces human capability.",
        "timing": "5-6 minutes",
        "discussion": "Have students share their most painful log analysis experiences. What patterns were they trying to find? How long did it take?",
        "context": "Production systems generate millions of log lines. AI makes it trivial to find the signal in the noise: cascading errors in microservices, timing patterns indicating race conditions, specific user cohorts experiencing failures.",
        "transition": "AI doesn't need JSON with correlation IDs to be effective—it parses whatever you have. Let's talk about making logs even more useful."
      }
    },
    {
      "type": "concept",
      "title": "Logging Economics with AI",
      "content": [
        "Add diagnostic logs at dozens of strategic points",
        "Agent generates and places logs in minutes",
        "Agent systematically removes temporary logs after fix",
        "Shifts debugging from 'minimal' to 'evidence-rich' instrumentation"
      ],
      "speakerNotes": {
        "talkingPoints": "This insight transforms the debugging economics: AI makes it trivial to add diagnostic logs at dozens of strategic points—far more volume than humans would ever instrument manually—because the agent can generate and place them in minutes. Once the bug is verified fixed, the same agent systematically removes all temporary diagnostic statements, restoring code hygiene and baseline logging practices. What would be prohibitively tedious instrumentation for humans becomes a routine part of AI-assisted investigation.",
        "timing": "4-5 minutes",
        "discussion": "Ask: How often do you skip adding diagnostic logs because it's too tedious? What if that work was free?",
        "context": "The agent can guide what to log based on its hypothesis—then analyze the new output immediately. Fifteen minutes writing specific log output beats hours of speculation.",
        "transition": "When logs aren't sufficient, we need bulletproof evidence through reproduction scripts."
      }
    },
    {
      "type": "concept",
      "title": "Reproduction Scripts: Code is Cheap",
      "content": [
        "Eliminate ambiguity with verifiable test cases",
        "AI generates complex environments in minutes",
        "Capture full context: database state, configs, API responses",
        "Create isolated Docker environments for reliability"
      ],
      "speakerNotes": {
        "talkingPoints": "Reproduction scripts are where AI agents' massive code generation capabilities shine: environments that take humans hours to set up—K8s, Docker configs, database snapshots, mock services, state initialization—take AI minutes to generate. Reproduction scripts eliminate ambiguity and create verifiable test cases. They capture full context: database state, external API responses, configuration, and user inputs. Once you have reliable reproduction, the agent can iterate on fixes and verify each attempt.",
        "timing": "4-5 minutes",
        "discussion": "Have students share bugs they couldn't reproduce. What context was missing? How long did investigation take?",
        "context": "For complex systems, use Docker to create isolated reproduction environments. Snapshot production database state, configure services with production-like settings, and write a script that triggers the bug reliably.",
        "transition": "Reproduction gives us reliable evidence. Now let's close the loop by placing agents inside failing environments."
      }
    },
    {
      "type": "comparison",
      "title": "Open-Loop vs Closed-Loop Debugging",
      "left": {
        "label": "Open-Loop",
        "content": [
          "Agent researches code and online issues",
          "Reports: 'Bug is likely missing RS256 validation'",
          "You manually apply fix and test",
          "No verification by agent—speculation only"
        ]
      },
      "right": {
        "label": "Closed-Loop",
        "content": [
          "Agent researches, applies fix, re-runs reproduction",
          "Reports: 'Fixed and verified—RS256 validation added'",
          "Agent proves fix works or iterates on new hypothesis",
          "Evidence-based validation in every iteration"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "Closing the loop means the agent can test its fixes and verify its reasoning actually works. Without environment access, the agent proposes solutions it can't validate. With closed-loop access, it applies fixes, re-runs reproduction, and proves they work—or iterates on new hypotheses when they don't. The difference is moving from 'research and guess' to 'research, fix, test, and prove.'",
        "timing": "4-5 minutes",
        "discussion": "Ask: How many times have you applied a suggested fix only to find it doesn't work? What if the agent verified it first?",
        "context": "This is where CLI agents like Claude Code shine over IDE assistants. CLI agents can run anywhere you have shell access: inside Docker containers, on remote servers, in CI/CD pipelines, on problematic production instances. IDE agents are tied to your local development machine.",
        "transition": "Let's walk through the complete closed-loop debugging workflow."
      }
    },
    {
      "type": "codeExecution",
      "title": "The Closed-Loop Debugging Workflow",
      "steps": [
        {
          "line": "BUILD: Create reproducible environment (Docker, scripts, database snapshots)",
          "highlightType": "human",
          "annotation": "Engineer sets up isolated environment that reliably triggers the bug"
        },
        {
          "line": "REPRODUCE: Verify bug manifests consistently with concrete evidence",
          "highlightType": "execution",
          "annotation": "Run reproduction script to capture logs, status codes, error output"
        },
        {
          "line": "PLACE: Give agent tool access WITHIN the environment",
          "highlightType": "human",
          "annotation": "Not just code access—runtime execution capabilities"
        },
        {
          "line": "INVESTIGATE: Agent correlates runtime behavior, codebase, and known issues",
          "highlightType": "prediction",
          "annotation": "Agent forms hypotheses using grounding techniques from Lesson 5"
        },
        {
          "line": "Agent executes: Bash diagnostic commands, Read relevant code, Grep for patterns",
          "highlightType": "execution",
          "annotation": "Agent gathers evidence from runtime and codebase"
        },
        {
          "line": "Agent predicts: 'Missing RS256 signature verification at jwt.ts:67'",
          "highlightType": "prediction",
          "annotation": "Hypothesis formed from correlating evidence"
        },
        {
          "line": "VERIFY: Agent applies fix, re-runs reproduction script",
          "highlightType": "execution",
          "annotation": "Edit jwt.ts:67 to add algorithm validation, then Bash to re-run test"
        },
        {
          "line": "Reproduction now passes—401 returned correctly",
          "highlightType": "feedback",
          "annotation": "Environment validates the fix works"
        },
        {
          "line": "If reproduction still fails: iterate with new hypothesis",
          "highlightType": "summary",
          "annotation": "Closed feedback loop continues until verified"
        }
      ],
      "speakerNotes": {
        "talkingPoints": "This workflow transforms debugging from 'research and guess' to 'research, fix, test, and prove.' The agent leverages grounding techniques to form hypotheses by correlating runtime behavior (execute diagnostic commands, inspect responses, analyze logs), codebase (use ChunkHound's code research for comprehensive investigation with architectural context), and known issues (research error patterns, CVEs, and similar bugs using tools like ArguSeek). The environment validates or refutes the agent's reasoning in every iteration.",
        "timing": "6-7 minutes",
        "discussion": "Walk through each step and ask: Where would manual debugging fail here? How does closed-loop access change the investigation?",
        "context": "CLI agents can run anywhere you have shell access: inside Docker containers, on remote servers, in CI/CD pipelines, on problematic production instances. This is a massive advantage over IDE-bound assistants.",
        "transition": "But what about situations where you can't access the failing environment? Let's talk about remote diagnosis."
      }
    },
    {
      "type": "concept",
      "title": "Remote Diagnosis: Scripts Over Access",
      "content": [
        "Can't reproduce locally? Generate comprehensive diagnostic scripts",
        "Agent produces ranked hypotheses based on codebase + known issues",
        "Scripts check dozens of potential issues automatically",
        "Trade developer time for compute time—30 minutes to generate thorough diagnostics"
      ],
      "speakerNotes": {
        "talkingPoints": "When you can't reproduce bugs locally or access the failing environment—customer deployments, edge infrastructure, locked-down production—you face limited information and no iteration cycle. This is where AI agents' probabilistic reasoning becomes a feature, not a limitation. Combined with their code generation capabilities, agents turn remote diagnosis from 'send me logs and wait' into an active investigation workflow. Writing a comprehensive diagnostic script takes humans days but takes agents 30 minutes. Send the script to the customer, load the output into the agent's context, and it correlates evidence with hypotheses to identify the root cause.",
        "timing": "5-6 minutes",
        "discussion": "Ask students: Have you debugged customer deployments you couldn't access? How long did the back-and-forth take?",
        "context": "The workflow follows the research-first pattern from Lesson 5: ground yourself in the codebase (understand the architecture around the failing component using code research) and in known issues (search for similar problems in the ecosystem). Then generate targeted diagnostic scripts that collect evidence for each hypothesis: configuration states, version mismatches, timing data, whatever's needed to validate or refute each theory.",
        "transition": "Let's summarize the key principles for debugging with AI agents."
      }
    },
    {
      "type": "takeaway",
      "title": "Key Takeaways",
      "content": [
        "Require reproducible proof always",
        "Log analysis is AI's superpower",
        "Generate comprehensive diagnostic scripts effortlessly",
        "Closed-loop workflows enable verification",
        "CLI agents access any environment"
      ],
      "speakerNotes": {
        "talkingPoints": "Debugging with AI agents is about building diagnostic environments where evidence is abundant and verification is systematic. The agent is your tireless investigator—give it the tools and demand proof. Never accept fixes without reproducible evidence. Use AI's strength in processing messy logs and generating code liberally. Place agents inside failing environments with the BUILD → REPRODUCE → PLACE → INVESTIGATE → VERIFY workflow. When direct access isn't possible, generate comprehensive diagnostic scripts instead.",
        "timing": "3-4 minutes",
        "discussion": "Final question: What's one debugging workflow you'll change after this lesson?",
        "context": "These principles apply across all debugging scenarios: local development, staging, production, customer deployments. The core principle remains: evidence over speculation.",
        "transition": "End of lesson. Questions?"
      }
    }
  ]
}
