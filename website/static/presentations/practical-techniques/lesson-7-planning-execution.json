{
  "metadata": {
    "title": "Lesson 7: Planning & Execution",
    "lessonId": "lesson-7-planning-execution",
    "estimatedDuration": "45-50 minutes",
    "learningObjectives": [
      "Master active context engineering techniques",
      "Review agent plans before execution",
      "Enable parallel workflows with worktrees",
      "Validate logic and catch invention patterns"
    ]
  },
  "slides": [
    {
      "type": "title",
      "title": "Planning & Execution",
      "subtitle": "Grounding, review, and reliable autonomous workflows",
      "content": [],
      "speakerNotes": {
        "talkingPoints": "This lesson bridges Lesson 6 (gathering context) and Lesson 9 (validation). We shift from one-time upfront context gathering to continuous grounding during execution. The key insight: context management is ongoing, not a setup phase. You load context, review plans, execute, validate, and iterate. These tactical techniques turn agents from code generators into reliable systems that fit your architecture.",
        "timing": "1 minute",
        "discussion": "Ask students: 'How many of you have had an agent generate code that compiles but doesn't match your architecture?' This sets up why plan review matters.",
        "context": "This is a hands-on lesson. Students should have experience with agent workflows already. The techniques here apply to any agentic coding system.",
        "transition": "Let's start with the core insight that changes everything about how you work with agents: questions aren't tests—they're context engineering tools."
      }
    },
    {
      "type": "concept",
      "title": "The Shift: Continuous Grounding",
      "content": [
        "Context gathering ends, active grounding begins",
        "Load context → Review plan → Execute → Validate",
        "Grounding is continuous, not one-time setup",
        "You catch architectural mismatches before code"
      ],
      "speakerNotes": {
        "talkingPoints": "Most engineers treat context gathering as a setup phase: 'Load the relevant files, then have the agent build.' This lesson changes that mental model. Grounding is a continuous activity. You load context, review what the agent proposes, let it execute incrementally, then validate. When something doesn't fit your mental model, you stop, clarify, and correct. This rhythm—load, review, execute, validate, iterate—is what transforms agents from code generators into reliable tools.",
        "timing": "2 minutes",
        "discussion": "Ask: 'When was the last time an agent's plan surprised you—in a good or bad way?' Discuss what signals should have warned you earlier.",
        "context": "This is the mental model shift. Many engineers treat agents like search engines: ask once, get answer. Agentic coding is iterative partnership.",
        "transition": "Let's look at specific grounding techniques that keep agents anchored in your actual codebase."
      }
    },
    {
      "type": "codeComparison",
      "title": "Grounding: Abstract vs Concrete",
      "leftCode": {
        "label": "Abstract (Generic)",
        "language": "text",
        "code": "Add rate limiting middleware"
      },
      "rightCode": {
        "label": "Concrete (Grounded)",
        "language": "text",
        "code": "Search existing middleware patterns,\nespecially authentication.\nCheck our Redis configuration.\nThen propose rate limiting that follows:\n- Same error handling\n- Same export structure\n- Same Redis client usage"
      },
      "speakerNotes": {
        "talkingPoints": "The right side isn't longer because it's verbose—it's longer because it's specific. The left prompt forces the agent to guess at your patterns. The right prompt forces discovery of your actual conventions. Concrete beats abstract every time. When you give specific examples of existing patterns, the agent doesn't invent—it discovers.",
        "timing": "3-4 minutes",
        "discussion": "Ask students: 'Have you seen agents generate 'plausible' code that doesn't match your style?' That's pattern completion from training data, not codebase discovery. This is how you fix it.",
        "context": "In production, vague prompts lead to 5-10 iteration cycles. Specific prompts lead to 1-2. The cost of writing better prompts is trivial compared to the cost of rework.",
        "transition": "Now let's look at a specific technique: questions as context engineering tools."
      }
    },
    {
      "type": "concept",
      "title": "Questions Load Context (They Don't Test)",
      "content": [
        "Question triggers: search → read → synthesize",
        "Result lives in context window for next steps",
        "More efficient than packing everything upfront",
        "Safe to execute autonomously (read-only)"
      ],
      "speakerNotes": {
        "talkingPoints": "When you ask 'How does our authentication work?', you're not testing the agent's knowledge. You're triggering a specific sequence: search for auth files, read middleware implementations, analyze patterns, synthesize findings. That synthesis now lives in the context window. The next prompt—'Add rate limiting following the same pattern'—has all the context it needs without searching again. This is different from stuffing one massive prompt with everything. Questions are a deliberate context engineering technique.",
        "timing": "2-3 minutes",
        "discussion": "Ask: 'Why is this safer than having the agent search autonomously?' Because search is unpredictable. Guided questions put the agent in read-only mode and ensure the right context is loaded.",
        "context": "This is about workflow efficiency. A sequence of exploratory questions creates a grounded context window much more reliably than hoping the agent searches for the right things.",
        "transition": "Questions are safe, but what forces the agent to actually ground in your code instead of guessing?"
      }
    },
    {
      "type": "codeComparison",
      "title": "Evidence Requirements Force Grounding",
      "leftCode": {
        "label": "Without Evidence",
        "language": "text",
        "code": "Why is this API slow?"
      },
      "rightCode": {
        "label": "With Evidence Required",
        "language": "text",
        "code": "Why is this API slow?\nProvide evidence:\n- File paths and line numbers\n- Actual values from logs/config\n- Specific identifiers (function names)\n- Full error messages and stack traces"
      },
      "speakerNotes": {
        "talkingPoints": "Without evidence requirement, the agent can pattern-match: 'Probably a database timeout or n+1 query.' That's statistical completion, not analysis. Requiring evidence forces the agent to read your code, trace execution, cite specifics. It cannot provide 'src/api/auth.ts:67' without opening that file. It cannot cite 'user.profile is null' without finding that exact error. Evidence requirements convert hallucinations into grounded analysis.",
        "timing": "3-4 minutes",
        "discussion": "Ask students to share debugging experiences where 'probably' answers cost them time. Evidence requirements eliminate that.",
        "context": "This is the single most effective grounding technique. One sentence in your prompt—'Provide evidence from the actual code'—changes everything.",
        "transition": "Evidence forces research. But you still need to validate the agent's reasoning. Let's talk about catching logic errors."
      }
    },
    {
      "type": "concept",
      "title": "LLMs Complete Patterns, Not Logic",
      "content": [
        "Agents are excellent at syntax and boilerplate",
        "Agents complete patterns from training data",
        "Your engineering judgment still required",
        "Challenge inconsistencies with your mental model"
      ],
      "speakerNotes": {
        "talkingPoints": "This is where your experience matters most. LLMs are probabilistic text completion engines—they're statistically likely to complete patterns correctly, but they don't 'understand' logic the way you do. If the agent says 'port 3000' but logs show 8080, that's inconsistent. Your mental model catches it. Point it out: 'You said port 3000, but logs show 8080. Reconcile this with evidence.' The agent will re-examine, search more carefully, and correct itself. Use your engineering skills to validate reasoning.",
        "timing": "2-3 minutes",
        "discussion": "Ask: 'Have you ever caught an agent's logical error that looked plausible?' That's pattern completion, not reasoning.",
        "context": "This is the partnership model: agents handle syntax, you handle logic and architecture.",
        "transition": "So you understand how to keep agents grounded. Now, before you let them execute, you need to review their plans. Let's look at what to check."
      }
    },
    {
      "type": "concept",
      "title": "Plan Review: Before Autonomous Execution",
      "content": [
        "Review strategy and reasoning, not just output",
        "Was grounding thorough? (Read relevant files?)",
        "Did it miss security, performance, edge cases?",
        "Does the approach fit your architecture?"
      ],
      "speakerNotes": {
        "talkingPoints": "This is high-level architectural validation, not line-by-line code review. When the agent proposes an approach, ask yourself: Did it ground this in my codebase? Did it read existing patterns? Did it consider alternatives? Does the reasoning hold up? If the agent proposes caching with 24-hour TTL, did it check your existing session implementation? Did it consider GDPR implications? Plan review catches architectural mismatches before code is written—much faster than rewriting generated code.",
        "timing": "3 minutes",
        "discussion": "Walk through an example: Agent proposes adding validation using a new library, but you already have Zod schemas. This is a grounding failure—caught at plan review stage, before code generation.",
        "context": "Plan review is where Lesson 4's constraint principles meet execution. You're validating the agent understands your constraints before it acts.",
        "transition": "Plan review also reveals a critical pattern: when agents invent instead of reuse."
      }
    },
    {
      "type": "comparison",
      "title": "Watch for: Invention vs Reuse",
      "left": {
        "label": "Red Flags (Invention)",
        "content": [
          "Create a new utility function for...",
          "Implement a helper to handle...",
          "Build error handling logic...",
          "Add validation for..."
        ]
      },
      "right": {
        "label": "Investigation (Reuse)",
        "content": [
          "Search for existing utilities first",
          "Does this helper already exist?",
          "What's our error handling pattern?",
          "Which validation schema applies?"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "Research shows AI-generated code has 8x more duplicated blocks than human-written code. Agents default to inventing plausible solutions from training patterns instead of discovering what already exists. When you see phrases like 'create new utility' or 'implement helper,' that's your signal: the agent wasn't grounded in existing code. Before execution, force discovery: 'Search for existing utilities that handle X. If none exist, then propose new ones.' This simple redirect prevents duplication.",
        "timing": "3-4 minutes",
        "discussion": "Ask: 'How many of you have seen generated code that reimplements something already in your codebase?' This is endemic. The agent isn't lazy—it's just pattern completing from training data.",
        "context": "This is the DRY principle enforced during planning. Catching invention at plan review is faster than refactoring generated code.",
        "transition": "Once your plan is solid and grounded, you can execute. But you need a safety net in case things go wrong."
      }
    },
    {
      "type": "concept",
      "title": "Checkpointing: Your Safety Net",
      "content": [
        "Agents make mistakes frequently",
        "Checkpoint before risky operations",
        "Commit after successful validation",
        "Make rollback seamless and instant"
      ],
      "speakerNotes": {
        "talkingPoints": "Agentic coding is probabilistic. Even with good prompts and planning, agents will occasionally generate code that doesn't compile or misses requirements. The key is making experimentation safe. Create restore points before risky operations, let the agent execute, validate results, then keep or revert. Modern tools (Claude Code, Cursor, VS Code Copilot) include checkpointing—use it aggressively. If your tool lacks it, commit far more frequently than traditional development: after each successful increment, before risky operations, when changing direction. Each commit is a known-good state you can return to instantly.",
        "timing": "2 minutes",
        "discussion": "Ask: 'How many of you have wished you could undo the last 10 minutes of agent execution?' That's exactly what checkpointing enables.",
        "context": "Claude Code: Press ESC twice to create a checkpoint. Other tools have similar features.",
        "transition": "With solid planning, grounding, and a safety net, you're ready for autonomous execution. For complex features, parallel workflows accelerate development."
      }
    },
    {
      "type": "code",
      "title": "Git Worktrees: Enable True Parallelization",
      "language": "bash",
      "code": "git worktree add ../auth-feature auth-task\ngit worktree add ../cache-feature cache-task\ngit worktree list\n\n# Run 3 agents on different branches simultaneously\n# No conflicts, isolated contexts",
      "caption": "Multiple working directories, separate branches, concurrent agent execution",
      "speakerNotes": {
        "talkingPoints": "Git worktrees are game-changers for multi-agent workflows. They let you check out different branches in different directories without stashing or switching. Why? You can run three agent instances on different features simultaneously. Agent 1 works on auth in ../auth-feature. Agent 2 works on caching in ../cache-feature. Agent 3 works on validation in ../validation-feature. Zero interference, isolated contexts, true parallelization. Each agent has its own working directory and branch.",
        "timing": "3-4 minutes",
        "discussion": "Ask: 'Who's tried running agents in series because they worried about conflicts?' Worktrees eliminate that entirely. You can parallelize ruthlessly.",
        "context": "This is infrastructure that unlocks scale. For solo engineers, one or two agents parallel is typical. For teams, worktrees enable running agents on many features simultaneously.",
        "transition": "Parallelization requires managing multiple concurrent sessions. Modern terminals make this feasible."
      }
    },
    {
      "type": "concept",
      "title": "Terminal Infrastructure Matters",
      "content": [
        "Modern terminals: Ghostty, Kitty, WezTerm, Alacritty",
        "Session management and rapid context switching",
        "GPU acceleration and visual indicators",
        "Invest as much as your IDE"
      ],
      "speakerNotes": {
        "talkingPoints": "When you're running three agent instances in parallel, your terminal becomes as important as your IDE. You're managing multiple sessions, switching between contexts, monitoring long-running processes. Modern terminals offer IDE-level features: GPU acceleration, programmable layouts, notification systems, extensive customization. If you're using a stock terminal, upgrade. Ghostty, Kitty, and WezTerm each offer different customization approaches. Pick one that fits your workflow and invest time configuring it. The payoff compounds across every development session.",
        "timing": "2-3 minutes",
        "discussion": "Ask: 'How much time do you spend in your terminal?' If it's hours daily, customization is worth it.",
        "context": "This is pragmatic infrastructure investment. Terminals remain under-invested by many engineers.",
        "transition": "Modern CLI tools complement parallel workflows. Let's look at which tools matter for multi-agent development."
      }
    },
    {
      "type": "concept",
      "title": "Modern CLI Tools for Multi-Agent Workflows",
      "content": [
        "micro - Quick edits without IDE switching",
        "eza - Modern ls with file type colors",
        "fzf - Rapid file and history search",
        "lazygit - Visual git across worktrees"
      ],
      "speakerNotes": {
        "talkingPoints": "These tools reduce friction when juggling multiple worktrees and agent sessions. Micro is a terminal editor that feels like VS Code—Ctrl+S, Ctrl+Q. Perfect for one-line edits without switching to your IDE. Eza replaces ls with better formatting and git integration—scan directories faster. Fzf lets you find files or recall commands from history instantly. Lazygit gives you a visual git interface—branch management, interactive staging, multi-worktree navigation. Install these once, benefit in every workflow. They're not fancy—they're efficient.",
        "timing": "2 minutes",
        "discussion": "Ask which tools students already use. There's often overlap. The combination across multiple workflows is what matters.",
        "context": "These are tested tools used across production teams. Not bleeding-edge—proven efficient.",
        "transition": "You can ask agents to help you set up and use these tools. Let's look at how."
      }
    },
    {
      "type": "code",
      "title": "Agents Help with CLI Setup",
      "language": "text",
      "code": "Help me set up git worktrees for parallel\nagent workflows.\n\nI want:\n- 3 independent working directories\n- Separate branches (auth, cache, validation)\n- Clean directory structure\n\nGenerate git worktree add commands\nfor my specific repo structure.",
      "caption": "Ground agent in external tools via documentation research",
      "speakerNotes": {
        "talkingPoints": "You don't need to memorize git worktree syntax or fzf keybindings. Ask your agent. Ground with research first: 'Research best practices for git worktree workflows. Then generate the specific commands I need.' The agent researches, proposes a clean layout, generates exact git commands. This is faster than reading documentation manually and ensures commands match your context.",
        "timing": "1-2 minutes",
        "discussion": "This bridges the gap: agents are useful for CLI setup too, not just code generation.",
        "context": "Agents can help you learn tools you haven't mastered yet.",
        "transition": "The final principle: use the right tool for each task. Don't be dogmatic."
      }
    },
    {
      "type": "comparison",
      "title": "Mix CLI and UI: Use What Works",
      "left": {
        "label": "IDE Strengths",
        "content": [
          "Symbol search and go-to-definition",
          "Call hierarchies and refactoring",
          "Viewing large files and complexity",
          "Best for code exploration"
        ]
      },
      "right": {
        "label": "CLI Strengths",
        "content": [
          "Quick edits in agent context",
          "Git operations across worktrees",
          "Managing parallel sessions",
          "Best for rapid iteration"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "Don't fall into the trap of 'terminal-only' or 'GUI-only' thinking. IDEs remain superior for code navigation—symbol search, go-to-definition, call hierarchies are hard to match in terminal. But CLI excels at quick edits (no context switch cost), git operations across multiple worktrees, and managing parallel sessions. Use the best tool for each task. Reading a large file? IDE. One-line edit during agent execution? Terminal. Managing three branches? CLI. This pragmatism beats ideology every time.",
        "timing": "2 minutes",
        "discussion": "Ask which tools students prefer for different tasks. You'll see consensus around IDE for navigation, CLI for rapid iteration.",
        "context": "This is about efficiency. The best workflow combines tools deliberately, not religiously.",
        "transition": "Let's consolidate the key takeaways from this lesson."
      }
    },
    {
      "type": "takeaway",
      "title": "Key Takeaways",
      "content": [
        "Questions are context engineering tools—not knowledge tests",
        "Evidence requirements force grounding in actual code",
        "Review plans for logic and architectural fit before execution",
        "Git worktrees enable true parallel agent workflows"
      ],
      "speakerNotes": {
        "talkingPoints": "These four principles transform how you work with agents. Questions deliberately load context. Evidence requirements convert guesses into grounded analysis. Plan review catches architectural mismatches early. Worktrees let you parallelize complex features across multiple agents. Together, they create reliable, efficient agentic workflows.",
        "timing": "2-3 minutes",
        "discussion": "Ask students which principle they want to implement first in their own workflows. Commitment to action matters.",
        "context": "These aren't theoretical—they're tactical techniques used in production.",
        "transition": "Next lesson: Tests as Guardrails. We'll cover how to validate agent-generated code and catch bugs before they ship."
      }
    }
  ]
}