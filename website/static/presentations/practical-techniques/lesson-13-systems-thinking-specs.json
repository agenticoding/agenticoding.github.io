{
  "metadata": {
    "title": "Systems Thinking for Specs",
    "lessonId": "lesson-13-systems-thinking-specs",
    "estimatedDuration": "35-45 minutes",
    "learningObjectives": [
      "Write precise iterative specs",
      "Model architecture and boundaries",
      "Choose state modeling strategies",
      "Converge through spec-code loops"
    ]
  },
  "slides": [
    {
      "type": "title",
      "title": "Systems Thinking for Specs",
      "subtitle": "Oscillate between architecture and implementation until convergence",
      "content": [],
      "speakerNotes": {
        "talkingPoints": "Lesson 12 established specs as temporary scaffolding. This lesson answers: what makes a spec good enough to produce quality code? We'll cover the zoom lens metaphor, iterative precision, architecture sections, state modeling, and convergence criteria.",
        "timing": "1 minute",
        "discussion": "Ask: When you write a spec, how do you know when it's 'done enough' to start coding?",
        "context": "This builds directly on Lesson 12's SDD principles. Students should already understand specs as disposable scaffolding.",
        "transition": "Let's start with the fundamental metaphor for how specs work."
      }
    },
    {
      "type": "visual",
      "title": "The Spec as a Zoom Lens",
      "component": "SpecCodeZoomDiagram",
      "caption": "Specs sharpen through oscillation between architecture and implementation.",
      "speakerNotes": {
        "talkingPoints": "Think of a spec as a zoom lens. Zoomed out, you see architecture—modules, boundaries, invariants. Zoomed in, you see implementation—edge cases, error handling, concurrency. You oscillate between views, and the spec sharpens through contact with implementation. Start with Architecture, Interfaces, and State. Generate a first pass. The spec is a hypothesis. The code is an experiment.",
        "timing": "2-3 minutes",
        "discussion": "How many passes through spec→code do you typically do before you feel confident? One? Five?",
        "context": "This is Lesson 3's four-phase cycle applied fractally. At the spec level: research, plan, write, validate. At the code level: research, plan, execute, validate. Each zoom transition is itself a Research→Plan→Execute→Validate loop.",
        "transition": "Let's see what precision actually looks like in a spec—and how you discover it through iteration."
      }
    },
    {
      "type": "codeComparison",
      "title": "Precision Through Iteration",
      "leftCode": {
        "label": "Vague",
        "language": "text",
        "code": "Handle webhook authentication\n\nStore payment data"
      },
      "rightCode": {
        "label": "Precise",
        "language": "text",
        "code": "C-001: NEVER process unsigned webhook\n— Signature validation on line 1 of handler\n\nI-001: SUM(transactions) = account.balance\n— Verified by: generate 1K transactions,\n  check sum after each batch"
      },
      "speakerNotes": {
        "talkingPoints": "Vague specs produce vague code. Precision narrows the solution space. But precision isn't achieved through contemplation alone—it's discovered through iteration. Each pass through implementation reveals constraints the spec missed: a state transition you didn't anticipate, a concurrency edge case, an unrealistic performance budget.",
        "timing": "3-4 minutes",
        "discussion": "Think of a recent spec you wrote. Was it closer to the left or the right? What would the precise version look like?",
        "context": "The bottleneck has shifted from 'production' to 'orchestration + verification'. You orchestrate what gets built and verify it matches intent. When implementation diverges from intent, ask: is the architecture sound? If yes, fix the code. If no, fix the spec and regenerate.",
        "transition": "Now let's break down what goes into the architecture section of a spec."
      }
    },
    {
      "type": "concept",
      "title": "Architecture: Modules, Boundaries, Contracts",
      "content": [
        "Modules: single responsibility, not categories",
        "Boundaries: what a module CANNOT import",
        "Contracts: preconditions and postconditions",
        "Integration points: where traffic crosses boundaries"
      ],
      "speakerNotes": {
        "talkingPoints": "Every system has internal structure. The architecture section forces you to make it explicit. A module isn't 'handles payments'—that's a category. It's 'processes Stripe webhook events and updates payment state'—that's a responsibility. Boundaries define coupling constraints: webhook-handler NEVER imports from notification. Contracts define what the caller provides and what the callee guarantees.",
        "timing": "3-4 minutes",
        "discussion": "Can you articulate what each module in your current project does in one sentence? If not, it's probably doing too much.",
        "context": "Integration points are the doors in the boundary wall. Direction matters: inbound needs validation and rate limiting; internal pub/sub needs delivery guarantees.",
        "transition": "Boundaries and contracts prevent coupling—but there's a hidden driver of architectural decisions we often overlook."
      }
    },
    {
      "type": "concept",
      "title": "Third-Party Assumptions Drive Design",
      "content": [
        "Assumptions capture behavioral guarantees you depend on",
        "Each assumption drives specific spec elements",
        "Traceability: assumption → constraint → code",
        "Provider migration scoped to changed assumptions"
      ],
      "speakerNotes": {
        "talkingPoints": "Integration points tell you WHERE external services connect. Third-party assumptions capture WHAT you believe about those services. Example: 'Webhooks deliver at-least-once, not exactly-once' drives idempotency checks, Redis locks, and the event-driven state model. The Drives column creates traceability—when an assumption changes (migrating from Stripe to Adyen), you know exactly which constraints to revisit. Without it, a provider migration becomes an audit of the entire spec.",
        "timing": "3-4 minutes",
        "discussion": "What third-party assumptions does your current system silently depend on? What would break if Stripe changed delivery semantics tomorrow?",
        "context": "For the Stripe webhook system: at-least-once delivery drives idempotency; out-of-order delivery drives the state machine; HMAC-SHA256 signing drives signature validation; ~99.99% availability drives the circuit breaker.",
        "transition": "Architecture defines the skeleton. Now let's tackle where bugs actually hide—state."
      }
    },
    {
      "type": "comparison",
      "title": "State Modeling Strategies",
      "neutral": true,
      "left": {
        "label": "Declarative",
        "content": [
          "Declare desired end state",
          "Reconciler diffs and converges",
          "React, Terraform, SQL, GitOps",
          "Simple reasoning; need a reconciler"
        ]
      },
      "right": {
        "label": "Event-Driven",
        "content": [
          "Append-only log of what happened",
          "Full audit trail and replay",
          "Webhooks, messaging, event sourcing",
          "Eventual consistency complexity"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "State is where bugs hide. Choose one model per entity. Declarative is increasingly the default: desired_state + reconciliation_loop. Event-driven gives full audit trail but adds ordering complexity. State machines (not shown separately) make illegal transitions impossible—payment lifecycle uses pending→processing→succeeded/failed. The model shapes generated code: state machines produce switch/case, event-driven produces handlers and projections, declarative produces diff-and-patch reconcilers.",
        "timing": "3-4 minutes",
        "discussion": "For a payment lifecycle, which model would you choose? What about webhook ingestion? Why different models for different entities?",
        "context": "State machines are the third option: every edge must be enumerated upfront, but illegal transitions become impossible. Payment lifecycle = state machine. Webhook ingestion = event-driven. Account balance = declarative (SUM(transactions) must converge to account.balance).",
        "transition": "State modeling also means accounting for what happens when things go wrong."
      }
    },
    {
      "type": "concept",
      "title": "Error States and Crash Recovery",
      "content": [
        "Errors are part of your data model, not exceptions",
        "Each error state has an explicit recovery path",
        "Startup ordering determines restart safety",
        "Non-idempotent startup steps corrupt state on crash"
      ],
      "speakerNotes": {
        "talkingPoints": "When you model error states explicitly, recovery paths become obvious. PAYMENT_PENDING → retry webhook check. PAYMENT_FAILED → notify user, allow retry. WEBHOOK_DUPLICATE → return 200, skip processing. For crash recovery, specify startup ordering: database first, then cache, then HTTP server. Each component has a 'ready when' condition and an 'on fail' action. If any startup step is not idempotent, a crash-and-restart can corrupt state.",
        "timing": "2-3 minutes",
        "discussion": "Does your system handle restart gracefully? What happens if it crashes between database migration and cache warm-up?",
        "context": "Ephemeral state (like Redis processing locks) disappears on crash. Your system must handle that—this is why the entity table distinguishes persistent from ephemeral storage.",
        "transition": "We've covered what's inside the system. Now let's flip perspective—what does the system look like from the outside?"
      }
    },
    {
      "type": "visual",
      "title": "The System Boundary",
      "component": "SystemBoundaryDiagram",
      "caption": "Architecture lives inside the boundary; interfaces cross it.",
      "speakerNotes": {
        "talkingPoints": "The dashed line is the key. Everything inside it is architecture: modules connected by contracts. Everything crossing it is an interface: data entering (inputs) or leaving (outputs) the system. Integration points are the doors in the wall. This visual distinction helps you think about what needs validation (anything crossing the boundary) versus what you can trust (internal contracts).",
        "timing": "2-3 minutes",
        "discussion": "Draw your current system's boundary. What crosses it? Are all crossing points validated?",
        "context": "Inputs need format parsing, validation, and rate limiting. Outputs need SLAs. Inputs without all three are bugs waiting to happen.",
        "transition": "Let's look at the specific inputs and outputs that cross this boundary."
      }
    },
    {
      "type": "concept",
      "title": "Interfaces: Inputs and Outputs",
      "content": [
        "Every input: source, format, validation, rate limit",
        "Every output: destination, format, SLA",
        "Inputs without all three properties are latent bugs",
        "Deployment strategy affects code structure"
      ],
      "speakerNotes": {
        "talkingPoints": "While architecture describes internal structure, interfaces describe the external surface. For inputs: the Format column is what you parse, the Validation column is what you reject, the Rate Limit column is what you throttle. For outputs: define destination and SLA. Deployment strategy matters too—canary deployments require feature flags; rolling updates require backward-compatible APIs. These operational decisions affect code structure.",
        "timing": "2-3 minutes",
        "discussion": "Pick one endpoint in your system. Can you name its format, validation rules, and rate limit? If not, that's a gap in your spec.",
        "context": "For production-critical systems, also specify observability: structured logging with correlation IDs, SLOs with burn-rate alerts, and distributed tracing with sampling strategy.",
        "transition": "We've covered what goes into a spec. Now the critical question: when are you done iterating?"
      }
    },
    {
      "type": "codeExecution",
      "title": "The Convergence Loop",
      "steps": [
        {
          "line": "Start with Architecture, Interfaces, State",
          "highlightType": "human",
          "annotation": "Three sections are enough for a first pass"
        },
        {
          "line": "Generate first implementation pass",
          "highlightType": "execution",
          "annotation": "The spec is a hypothesis; the code is an experiment"
        },
        {
          "line": "Ask: Is the architecture sound?",
          "highlightType": "prediction",
          "annotation": "The single decision point each iteration"
        },
        {
          "line": "YES → Fix the code (mechanical error)",
          "highlightType": "execution",
          "annotation": "Patch the implementation, boundaries are correct"
        },
        {
          "line": "NO → Fix the spec and regenerate",
          "highlightType": "human",
          "annotation": "Don't patch around flawed boundaries"
        },
        {
          "line": "Code reveals new gaps → add sections",
          "highlightType": "feedback",
          "annotation": "Concurrency constraints, performance budgets emerge"
        },
        {
          "line": "Done when loop produces no new gaps",
          "highlightType": "summary",
          "annotation": "Testable termination: code passes, spec accounts for all constraints"
        }
      ],
      "speakerNotes": {
        "talkingPoints": "Always start with three sections. Generate a first pass. Then ask one question: is the architecture sound? Yes → fix the code. No → fix the spec and regenerate. Each loop reveals what the spec missed. The first pass might expose concurrency constraints—add Constraints. The second might surface a performance bottleneck—add a Performance Budget. The code pulls depth from you; you don't push depth onto it.",
        "timing": "3-4 minutes",
        "discussion": "How do you currently decide between 'fix the code' and 'fix the design'? Is it a conscious decision or gut feel?",
        "context": "A simple feature converges in one loop. A complex architectural change might take five. You discover which you're dealing with by running the loop, not by predicting it. The engineer who runs ten hypothesis→experiment→verify loops per day outperforms the one who runs two with a more thorough upfront spec.",
        "transition": "Let's look at how extension points prevent rewrites when known variations arrive."
      }
    },
    {
      "type": "comparison",
      "title": "Extension Points: YAGNI Gate",
      "left": {
        "label": "Without Extension Points",
        "content": [
          "Hardcoded Stripe client",
          "PayPal arrives in Q3 → full rewrite",
          "USD-only → multi-currency migration",
          "Abstractions added reactively under pressure"
        ]
      },
      "right": {
        "label": "With Extension Points",
        "content": [
          "PaymentGateway interface declared now",
          "PayPal in Q3 → implement interface, extend",
          "Only committed variations earn abstractions",
          "YAGNI gates which variations make the spec"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "Protected Variation from Cockburn and Larman: identify points of predicted variation and create a stable interface around them. But only for committed business needs—funded, scheduled, required by a known deadline. Multi-currency that's 'not committed' stays out. Declaring the interface now costs one abstraction; omitting it costs a migration later. The YAGNI gate ensures you don't over-abstract.",
        "timing": "2-3 minutes",
        "discussion": "What committed variations does your current project have? Are there stable interfaces ready for them?",
        "context": "Without this, agents build the simplest correct implementation. When PayPal arrives, that's a rewrite, not an extension. The key distinction: predicted AND committed = declare the interface. Predicted but not committed = YAGNI, skip it.",
        "transition": "Let's bring it all together with the key principles to take away."
      }
    },
    {
      "type": "concept",
      "title": "Iteration Speed is the Multiplier",
      "content": [
        "Code generation is approaching post-scarcity",
        "Scarce resource: your judgment about what to build",
        "10 loops/day beats 2 with thorough upfront specs",
        "Same insight that made Agile outperform Waterfall"
      ],
      "speakerNotes": {
        "talkingPoints": "Code is cheap, judgment is scarce. Maximize hypothesis→experiment→verify loops per day, not spec thoroughness per loop. This is the same insight that made Agile outperform Waterfall, compressed from weeks-per-iteration to minutes. Use exploration planning and ArguSeek to research before each loop. For system-level work, start from the full template. Validate through the SDD workflow—gap-analyze, implement, then delete the spec.",
        "timing": "2-3 minutes",
        "discussion": "How long does one spec→code→validate loop currently take you? What would double your iteration speed?",
        "context": "Research found iterative prompting ~10x faster than specification-driven development. LLMs misperceive specification quality, requiring iterative alignment. Plans fail on contact with implementation complexity.",
        "transition": "Let's capture the key takeaways from this lesson."
      }
    },
    {
      "type": "takeaway",
      "title": "Key Takeaways",
      "content": [
        "Specs are hypotheses, not blueprints",
        "Precision discovered through iteration",
        "Architecture makes structure explicit",
        "Fix specs or fix code",
        "Maximize loops, not spec depth"
      ],
      "speakerNotes": {
        "talkingPoints": "Specs are a zoom lens—oscillate between architecture and implementation. Each spec↔code pass reveals gaps the previous spec missed. Architecture sections force explicit structure: modules, boundaries, contracts. When implementation diverges, ask if the architecture is sound—fix the spec for design flaws, fix the code for mechanical errors. And remember: iteration speed is the multiplier. Delete the spec when done—code is the source of truth.",
        "timing": "2 minutes",
        "discussion": "Which of these principles challenges your current workflow the most? What's the first thing you'll change?",
        "context": "This connects back to Lesson 12: specs as scaffolding. The difference is now students know what goes INTO the scaffolding and when to stop building it.",
        "transition": "In the next lesson, we'll explore how to apply these principles at scale across larger systems and teams."
      }
    }
  ]
}