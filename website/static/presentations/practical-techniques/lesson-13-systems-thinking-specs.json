{
  "metadata": {
    "title": "Systems Thinking for Specs",
    "lessonId": "lesson-13-systems-thinking-specs",
    "estimatedDuration": "35-45 minutes",
    "learningObjectives": [
      "Iterate specs through code",
      "Define modules and boundaries",
      "Model state and transitions",
      "Converge on architectural soundness"
    ]
  },
  "slides": [
    {
      "type": "title",
      "title": "Systems Thinking for Specs",
      "subtitle": "Spec = Hypothesis, Code = Experiment",
      "content": [],
      "speakerNotes": {
        "talkingPoints": "Lesson 12 established that specs are scaffolding—temporary thinking tools deleted after implementation. This lesson answers: what makes a spec good enough to produce quality code? Think of a spec as a zoom lens. Zoomed out, you see architecture—modules, boundaries, invariants. Zoomed in, you see implementation—edge cases, error handling, concurrency. You oscillate between views, and the spec sharpens through contact with implementation.",
        "timing": "1 minute",
        "discussion": "How do you currently decide when a spec is 'done enough' to start coding?",
        "context": "This builds directly on Lesson 12's ephemeral spec model and Lesson 3's four-phase cycle. The key shift: specs aren't perfected upfront—they converge through iteration.",
        "transition": "Let's start with why precision matters and how you actually achieve it."
      }
    },
    {
      "type": "codeComparison",
      "title": "Precision Through Iteration",
      "leftCode": {
        "label": "Vague",
        "language": "text",
        "code": "Handle webhook authentication\n\nStore payment data"
      },
      "rightCode": {
        "label": "Precise",
        "language": "text",
        "code": "C-001: NEVER process unsigned webhook\n— Signature validation on line 1 of handler\n\nI-001: SUM(transactions) = account.balance\n— Verified by: generate 1K transactions,\n  check sum after each batch"
      },
      "speakerNotes": {
        "talkingPoints": "Vague specs produce vague code. Precision narrows the solution space. But precision isn't achieved through contemplation alone—it's discovered through iteration. Each pass through implementation reveals constraints the spec missed: a state transition you didn't anticipate, a concurrency edge case, an unrealistic performance budget. The bottleneck has shifted from production to orchestration and verification.",
        "timing": "2-3 minutes",
        "discussion": "Think about a spec you wrote recently. How many constraints were discovered during implementation rather than upfront?",
        "context": "When implementation diverges from intent, ask: is the architecture sound? If yes, fix the code. If the model or boundaries are wrong, fix the spec and regenerate.",
        "transition": "Let's visualize how this iterative workflow actually operates."
      }
    },
    {
      "type": "visual",
      "title": "The Iterative Spec-Code Workflow",
      "component": "SpecCodeZoomDiagram",
      "caption": "Specs sharpen through repeated contact with implementation.",
      "speakerNotes": {
        "talkingPoints": "Start with three sections: Architecture, Interfaces, and State—enough to generate a first pass. The spec is a hypothesis. The code is an experiment. Implementation reveals what the spec missed. Zoom out—extract updated understanding from code. Fix the architecture. Zoom back in—regenerate. Repeat until convergence, then delete the spec. This is Lesson 3's four-phase cycle applied fractally. At the spec level: research domain, plan architecture, write spec, validate completeness. At the code level: research codebase, plan changes, execute, validate tests.",
        "timing": "3-4 minutes",
        "discussion": "How many spec-code loops does a typical feature take in your experience? What about a complex architectural change?",
        "context": "The depth of iteration scales with complexity: a simple feature converges in one pass; a complex architectural change might take five. You discover which you're dealing with by running the loop, not by predicting it.",
        "transition": "Now let's walk through each section the loop surfaces, starting with architecture."
      }
    },
    {
      "type": "concept",
      "title": "Architecture: Modules, Boundaries, Contracts",
      "content": [
        "Modules: single responsibility, not category",
        "Boundaries: what a module CANNOT import",
        "Contracts: preconditions and postconditions",
        "Integration points: where traffic crosses boundaries"
      ],
      "speakerNotes": {
        "talkingPoints": "Every system has internal structure. The architecture section forces you to make it explicit. A module has a single responsibility—not 'handles payments' (that's a category), but 'Processes Stripe webhook events and updates payment state.' Boundaries define what a module cannot import—the coupling constraint. Contracts define how modules communicate with preconditions and postconditions. Integration points are the doors in the boundary wall—where traffic crosses from external to internal.\n\nModule table for reference:\n| Module | Responsibility | Boundary |\n| webhook-handler | Process Stripe webhooks, update payment state | src/payment/webhooks/ |\n| notification | Send emails on payment events | src/notification/ |\n\nContract table:\n| Provider | Consumer | Contract |\n| webhook-handler | payment | processEvent(stripeEventId): PaymentIntent — precondition: event not yet processed |\n| payment | notification | PaymentEvent { type, paymentId, amount, timestamp } — postcondition: immutable once published |",
        "timing": "3-4 minutes",
        "discussion": "Can you articulate what each module in your current system does in one sentence? If not, it's probably doing too much.",
        "context": "Boundaries prevent changes in one module from rippling through the system. For example: webhook-handler NEVER imports from notification or order. It publishes events to a queue; consumers decide action.",
        "transition": "Architecture also includes something most specs miss: third-party assumptions."
      }
    },
    {
      "type": "concept",
      "title": "Third-Party Assumptions Drive Design",
      "content": [
        "At-least-once delivery → idempotency checks",
        "Out-of-order webhooks → state machine transitions",
        "HMAC-SHA256 signing → signature validation",
        "~99.99% availability → circuit breaker + retry"
      ],
      "speakerNotes": {
        "talkingPoints": "Integration points tell you where external services connect. Third-party assumptions capture what you believe about those services—behavioral guarantees your design silently depends on. When you don't make them explicit, design decisions appear arbitrary: an agent sees a constraint like idempotency check but not the delivery semantic that demands it.\n\nFull assumption table:\n| Assumption | Source | Drives |\n| Webhooks deliver at-least-once, not exactly-once | Stripe docs | C-001 (idempotency), Redis lock, event-driven state model |\n| Webhooks may arrive out of order | Stripe docs | State machine with explicit transitions |\n| Payloads signed with HMAC-SHA256 | Stripe docs | C-002 (signature validation) |\n| API availability ~99.99% | Stripe SLA | Circuit breaker, retry queue, manual fallback |\n\nThe Drives column creates traceability from assumption to spec element—so when a provider changes (Stripe to Adyen), you know exactly which constraints to revisit.",
        "timing": "3-4 minutes",
        "discussion": "What assumptions does your system make about external services? Are they documented anywhere? What happens when those assumptions break?",
        "context": "Without the Drives column, a provider migration becomes an audit of the entire spec. With it, the audit is scoped to the rows whose assumptions no longer hold.",
        "transition": "Architecture also includes extension points—but only for committed variations. Let's move to state modeling."
      }
    },
    {
      "type": "comparison",
      "title": "State Models Shape Generated Code",
      "neutral": true,
      "left": {
        "label": "Declarative",
        "content": [
          "Declare desired end state",
          "Reconciler diffs and converges",
          "React, Terraform, SQL, GitOps",
          "Simple to reason about"
        ]
      },
      "right": {
        "label": "State Machine",
        "content": [
          "Enumerate legal transitions",
          "Illegal transitions impossible",
          "Payment lifecycles, approval chains",
          "Every edge enumerated upfront"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "State is where bugs hide. The state section forces you to account for what the system remembers. How you model state determines how you think about transitions. Choose one model per entity.\n\nThree models:\n| Model | Use When | Key Question |\n| Declarative | UI, infrastructure, schema convergence | What should the end state be? |\n| Event-Driven | Webhooks, messaging, event sourcing | What happened, and in what order? |\n| State Machine | Payment lifecycles, order flows | What transitions are legal from this state? |\n\nThe model shapes the code agents generate: state machines produce switch/case with explicit transitions, event-driven produces handlers and projections, declarative produces diff-and-patch reconcilers. Also specify entities (what persists), error states (part of the data model, not exceptions), and initialization/crash recovery ordering.",
        "timing": "3-4 minutes",
        "discussion": "For each entity in your system, can you name which state model it uses? Is it the right one?",
        "context": "Event-driven is the third option not shown here—full audit trail and replay, eventual consistency. The slide focuses on declarative vs state machine as the most distinct paradigms. Declarative is increasingly the default: the core pattern is always desired_state + reconciliation_loop.",
        "transition": "Now let's flip the perspective from internal structure to the external surface."
      }
    },
    {
      "type": "visual",
      "title": "System Boundary: Inside vs Outside",
      "component": "SystemBoundaryDiagram",
      "caption": "Architecture is internal; interfaces are what crosses the boundary.",
      "speakerNotes": {
        "talkingPoints": "The dashed line is the key. Everything inside it is architecture: modules connected by contracts. Everything crossing it is an interface: data entering (inputs) or leaving (outputs) the system. Integration points are the doors in the wall. Every input needs three things: Format (what you parse), Validation (what you reject), Rate Limit (what you throttle). Inputs without all three are bugs waiting to happen.",
        "timing": "2-3 minutes",
        "discussion": "Draw the boundary of your current system. Where are the doors? What crosses them?",
        "context": "This visual separates architectural concerns (modules, boundaries, contracts) from interface concerns (inputs, outputs, SLAs). The distinction matters because architecture is about internal coupling while interfaces are about external contracts.",
        "transition": "With boundaries clear, let's look at how we constrain what happens inside them."
      }
    },
    {
      "type": "code",
      "title": "Constraints Migrate from Spec to Code",
      "language": "typescript",
      "code": "// C-001: NEVER process duplicate webhook\n// C-002: NEVER process unsigned webhook\nexport async function handleWebhook(req) {\n  verifySignature(req)  // C-002\n  if (await isDuplicate(req.body.id))\n    return new Response(null, { status: 200 })  // C-001\n  // ...\n}",
      "caption": "Constraint IDs inline in code become the authoritative source after implementation",
      "speakerNotes": {
        "talkingPoints": "Constraints are non-negotiable rules—violations mean bugs. Invariants are conditions that must always hold. Both need spec IDs that migrate into code comments. The spec table is the authoritative source during design. The code comments become the authoritative source after implementation. This is what makes deleting the spec safe—the constraints have migrated. Each behavioral scenario traces back to a constraint or invariant. Walk edge categories systematically: boundary values, null/empty, error propagation, concurrency, temporal.",
        "timing": "3-4 minutes",
        "discussion": "Do your code comments reference constraint IDs? If not, how do you trace requirements to implementation?",
        "context": "Invariants answer ALWAYS (SUM(transactions) = account.balance). Constraints answer NEVER (never process unsigned webhook). Behavioral scenarios answer WHAT SHOULD HAPPEN (when amount=0, when unknown intent, when Stripe returns 503). Together they form the verification surface.",
        "transition": "Beyond correctness, we also need to specify how good is good enough."
      }
    },
    {
      "type": "codeExecution",
      "title": "Performance Budget: Decomposing SLOs",
      "steps": [
        {
          "line": "SLO target: Latency p95 < 100ms",
          "highlightType": "human",
          "annotation": "Quality attribute defines the overall budget"
        },
        {
          "line": "Signature validation: 2ms (hot path)",
          "highlightType": "execution",
          "annotation": "Synchronous, blocking — tight budget"
        },
        {
          "line": "Idempotency check (Redis): 5ms (hot path)",
          "highlightType": "execution",
          "annotation": "Rules out full-table scan — must use indexed lookup"
        },
        {
          "line": "Parse + validate payload: 3ms (hot path)",
          "highlightType": "execution",
          "annotation": "Parsing overhead, schema validation"
        },
        {
          "line": "Update payment state (DB): 15ms (hot path)",
          "highlightType": "execution",
          "annotation": "Largest single operation on the critical path"
        },
        {
          "line": "Publish event (queue): 5ms (cold path)",
          "highlightType": "execution",
          "annotation": "Async — tolerates more latency"
        },
        {
          "line": "Total: 30ms — Headroom: 70ms",
          "highlightType": "summary",
          "annotation": "70ms reserved for future operations on this path"
        }
      ],
      "speakerNotes": {
        "talkingPoints": "Quality attributes define measurable thresholds across three tiers: target (normal), degraded (alerting), failure (paging). But saying 'p95 < 100ms' for the whole webhook flow isn't actionable. The budget forces two decisions agents can't make alone. First, hot vs cold path: signature validation is synchronous and blocking—tight budget. Event publishing is async—tolerates more. Second, headroom: total is 30ms against 100ms SLO, leaving 70ms for future operations. Per-operation budgets also surface algorithmic constraints—if idempotency check must complete in 5ms, that rules out a full-table scan.",
        "timing": "3-4 minutes",
        "discussion": "Do you decompose your SLOs into per-operation budgets? How do you decide which operations get how much of the budget?",
        "context": "Without decomposition, an agent might spend the entire budget on a single unoptimized query. The budget makes implicit performance expectations explicit and actionable.",
        "transition": "Let's see how execution flows trace through the system."
      }
    },
    {
      "type": "visual",
      "title": "Flows: Tracing Execution",
      "component": "SystemFlowDiagram",
      "caption": "Each step defines success and failure paths—not just happy path.",
      "speakerNotes": {
        "talkingPoints": "Flows trace execution from trigger to completion, revealing integration points and error handling gaps. Each step has three parts: what happens, what happens on success, what happens on failure. Flows force you to think through the actual execution path, not an idealized happy-path abstraction. Security, observability, deployment strategy, and integration dependencies are also part of the spec—they're system properties that emerge from correct boundaries and instrumentation.",
        "timing": "2-3 minutes",
        "discussion": "Pick a critical flow in your system. Can you trace every step and name what happens on failure at each point?",
        "context": "Security asks: where does trust end? What can an attacker control? Observability asks: how do you know it's working? Deployment asks: how does it get to production? These aren't bolted on—they emerge from the architecture.",
        "transition": "Now let's tie everything together with the convergence loop."
      }
    },
    {
      "type": "codeExecution",
      "title": "Converge, Don't Count Passes",
      "steps": [
        {
          "line": "Start with: Architecture + Interfaces + State",
          "highlightType": "human",
          "annotation": "Three sections — enough to generate a first pass"
        },
        {
          "line": "Generate first implementation pass",
          "highlightType": "execution",
          "annotation": "The spec is a hypothesis, the code is an experiment"
        },
        {
          "line": "Ask: is the architecture sound?",
          "highlightType": "prediction",
          "annotation": "The single diagnostic question after each pass"
        },
        {
          "line": "YES → fix the code (mechanical error)",
          "highlightType": "execution",
          "annotation": "Patch the implementation, don't touch the spec"
        },
        {
          "line": "NO → fix the spec and regenerate",
          "highlightType": "human",
          "annotation": "Don't patch around flawed boundaries"
        },
        {
          "line": "Loop reveals missing sections (Constraints, Perf Budget...)",
          "highlightType": "feedback",
          "annotation": "Code pulls depth from you — you don't push it"
        },
        {
          "line": "Done when loop produces no new gaps",
          "highlightType": "summary",
          "annotation": "Testable termination: code passes scenarios, spec accounts for all constraints"
        }
      ],
      "speakerNotes": {
        "talkingPoints": "The convergence loop is the core workflow. Always start with Architecture, Interfaces, and State. Generate a first pass. Then ask one question: is the architecture sound? Yes means fix the code—the agent made a mechanical error. No means fix the spec and regenerate—don't patch around flawed boundaries. Each loop reveals what the spec missed. The first pass might expose concurrency constraints. The second might surface a performance bottleneck. The code pulls depth from you; you don't push depth onto it by categorizing complexity upfront.",
        "timing": "3-4 minutes",
        "discussion": "How do you currently decide whether a bug is in the code or in the design? What's your diagnostic process?",
        "context": "You're done when the loop produces no new gaps: code passes all behavioral scenarios, spec accounts for all constraints the code revealed, and the last pass surfaces nothing new. A simple feature converges in one loop. A complex change might take five. Iteration speed is the multiplier—ten hypothesis-experiment-verify loops per day outperforms two with a more thorough upfront spec.",
        "transition": "Let's summarize the key insights from systems thinking for specs."
      }
    },
    {
      "type": "takeaway",
      "title": "Key Takeaways",
      "content": [
        "Specs converge through code contact",
        "Architecture sound? Fix accordingly",
        "Third-party assumptions drive design",
        "Iteration speed is the multiplier",
        "Delete specs when converged"
      ],
      "speakerNotes": {
        "talkingPoints": "Five critical insights: First, specs are a zoom lens—oscillate between architecture and implementation detail. Precision is discovered through iteration, not specified upfront. Second, the single diagnostic question: is the architecture sound? Yes means patch code. No means fix spec and regenerate. Third, make third-party assumptions explicit so agents know which decisions to revisit when providers change. Fourth, code generation is approaching post-scarcity—maximize hypothesis-experiment-verify loops per day, not spec thoroughness per loop. Fifth, delete the spec when done. Code is the source of truth. What survives deletion: constraint IDs inlined in code and the small WHY residual committed as decision records.",
        "timing": "2 minutes",
        "discussion": "What's one section from this lesson you'll add to your next spec? Which part of the convergence loop do you think your team struggles with most?",
        "context": "This lesson completes the spec-driven development arc. Lesson 12 established specs as scaffolding. Lesson 13 showed how to make that scaffolding precise enough to generate quality code through iterative convergence.",
        "transition": "Practice this by writing a spec for a system you know well and running it through the convergence loop."
      }
    }
  ]
}