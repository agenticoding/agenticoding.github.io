{
  "metadata": {
    "title": "How LLMs Work",
    "lessonId": "lesson-1-how-llms-work",
    "estimatedDuration": "30-40 minutes",
    "learningObjectives": [
      "Understand LLM token prediction",
      "Distinguish LLM from agent",
      "Avoid anthropomorphizing AI tools",
      "Apply operator mindset principles"
    ]
  },
  "slides": [
    {
      "type": "title",
      "title": "How LLMs Work",
      "subtitle": "Understanding the machinery before operating it",
      "content": [],
      "speakerNotes": {
        "talkingPoints": "Welcome to the first lesson. Before we can effectively operate AI agents, we need to understand what they actually are—not the marketing version, but the technical reality. This foundational knowledge prevents the most common mistakes engineers make.",
        "timing": "1 minute",
        "discussion": "Quick pulse check: How many of you have used ChatGPT or Copilot? How many have used a CLI agent like Claude Code or Cursor?",
        "context": "This lesson establishes the mental model that everything else builds on. Engineers who skip this tend to anthropomorphize agents and make predictable errors.",
        "transition": "Let's start with the paradigm shift that's happening in our industry right now."
      }
    },
    {
      "type": "comparison",
      "title": "The Paradigm Shift",
      "left": {
        "label": "Hand Tools Era",
        "content": [
          "Carpenters shaped every piece manually",
          "Engineers write code line-by-line",
          "Focus on syntax and implementation",
          "Craftsmanship through repetition"
        ]
      },
      "right": {
        "label": "Power Tools Era",
        "content": [
          "Craftsmen design and operate machines",
          "Engineers orchestrate AI agents",
          "Focus on architecture and verification",
          "Bandwidth through configuration"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "Software engineering is undergoing a transformation similar to how power tools revolutionized construction. We're not losing control—we're gaining bandwidth and precision. The key shift is from implementation focus to orchestration focus.",
        "timing": "2-3 minutes",
        "discussion": "What parallels do you see between power tools in construction and AI agents in software? What skills become more important, and which become less critical?",
        "context": "This framing helps engineers see AI agents as capability amplifiers rather than replacements. The construction analogy resonates because both industries saw massive productivity gains without eliminating the need for skilled practitioners.",
        "transition": "Now let's look under the hood at what these tools actually are—starting with the LLM itself."
      }
    },
    {
      "type": "concept",
      "title": "LLM = Brains (Token Prediction Engine)",
      "content": [
        "Predicts the next most probable token in a sequence",
        "Processes ~200K tokens of context (working memory)",
        "Samples from probability distributions learned from training",
        "Has zero consciousness, intent, or feelings"
      ],
      "speakerNotes": {
        "talkingPoints": "An LLM is a statistical pattern matcher built on transformer architecture. It predicts tokens—the atomic units of text processing. Think of it as incredibly sophisticated autocomplete that's read most of the internet.",
        "timing": "3 minutes",
        "discussion": "Why do you think understanding this 'just autocomplete' framing is important for how we use these tools?",
        "context": "A token averages 3-4 characters. Common words are single tokens, while longer words split into subwords. This matters for cost (billed per token), context limits (your working memory budget), and performance.",
        "transition": "Let's contrast what we say about LLMs versus what's actually happening technically."
      }
    },
    {
      "type": "marketingReality",
      "title": "Marketing vs Reality",
      "metaphor": {
        "label": "What We Say",
        "content": [
          "The agent thinks about the problem",
          "The agent understands the codebase",
          "The agent learns from mistakes",
          "The agent reasons through solutions"
        ]
      },
      "reality": {
        "label": "What's Actually Happening",
        "content": [
          "Token predictions through attention layers",
          "Pattern matching against training data",
          "Weights update during training only, not conversations",
          "Sequential predictions that build on each other"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "The language we use to describe AI is full of anthropomorphization. 'Thinks,' 'understands,' 'learns,' 'reasons'—these are metaphors, not descriptions. The LLM doesn't understand your code; it generates statistically probable continuations based on patterns.",
        "timing": "2-3 minutes",
        "discussion": "Which of these metaphors have you caught yourself using? How might that language shape your expectations of the tool?",
        "context": "This isn't pedantic—engineers who internalize that LLMs 'understand' things make systematic errors. They assume shared context that doesn't exist and skip verification because they trust the 'understanding.'",
        "transition": "The LLM is just the brains. Let's look at how agent software provides the body."
      }
    },
    {
      "type": "concept",
      "title": "Agent Software = Body (Execution Layer)",
      "content": [
        "File operations: Read, Write, Edit",
        "Command execution: Bash, git, npm, pytest",
        "Code search: Grep, Glob patterns",
        "API calls: Fetch docs, external resources"
      ],
      "speakerNotes": {
        "talkingPoints": "The LLM alone can only generate text. Agent software wraps the LLM to enable action through deterministic tool execution. The LLM is the brains; the agent framework is the body that can actually do things.",
        "timing": "2 minutes",
        "discussion": "What's the distinction between what the LLM does and what the agent software does?",
        "context": "This separation is crucial. The LLM predicts text; the agent executes tools. When you see 'the agent edited the file,' understand that the LLM predicted the edit, and deterministic code executed it.",
        "transition": "Let's see this brains-and-body relationship in action with a concrete example."
      }
    },
    {
      "type": "codeExecution",
      "title": "Agent Execution: How It Actually Works",
      "steps": [
        {
          "line": "Engineer: 'Add authentication middleware'",
          "highlightType": "human",
          "annotation": "Human provides the task specification"
        },
        {
          "line": "LLM predicts: 'I should read existing auth middleware'",
          "highlightType": "prediction",
          "annotation": "Token prediction drives next action"
        },
        {
          "line": "Agent executes: Read(src/auth.ts)",
          "highlightType": "execution",
          "annotation": "Deterministic tool call"
        },
        {
          "line": "File content returned to context",
          "highlightType": "feedback",
          "annotation": "Result becomes input for next prediction"
        },
        {
          "line": "LLM predicts code changes based on patterns",
          "highlightType": "prediction",
          "annotation": "Prediction incorporates new context"
        },
        {
          "line": "Agent executes: Edit(file, old, new)",
          "highlightType": "execution",
          "annotation": "Code modification applied"
        },
        {
          "line": "LLM predicts: 'Run tests to verify'",
          "highlightType": "prediction",
          "annotation": "Validation step predicted"
        },
        {
          "line": "Agent executes: Bash('npm test')",
          "highlightType": "execution",
          "annotation": "Test execution"
        },
        {
          "line": "Loop continues until task complete",
          "highlightType": "summary",
          "annotation": "Iteration based on feedback"
        }
      ],
      "speakerNotes": {
        "talkingPoints": "When an agent 'implements a feature,' this is the actual flow. LLM predicts, agent executes, results feed back. No magic, no consciousness—just probability distributions driving tool execution in a loop.",
        "timing": "3-4 minutes",
        "discussion": "At which steps could things go wrong? Where would you want verification or human oversight?",
        "context": "Understanding this loop is fundamental. Every 'magical' thing an agent does is this cycle repeated. The LLM never 'sees' your filesystem directly—it only sees text representations that agent software provides.",
        "transition": "Now that we understand the machinery, let's look at the three critical errors this knowledge helps us avoid."
      }
    },
    {
      "type": "concept",
      "title": "Error 1: Assuming the Agent 'Knows' Things",
      "content": [
        "Reality: Agent only sees current context (~200K tokens)",
        "It doesn't remember previous conversations",
        "It can't access files it hasn't been shown",
        "Fix: Provide explicit context every time"
      ],
      "speakerNotes": {
        "talkingPoints": "The most common error is assuming shared knowledge. The agent has no persistent memory of your project beyond what's in the current context window. Every session starts fresh. Every file must be explicitly loaded.",
        "timing": "2 minutes",
        "discussion": "What happens when you assume the agent 'remembers' something from earlier? What failure modes does this create?",
        "context": "In production, this error manifests as agents making confident but wrong assumptions about code they haven't seen. They'll generate plausible-looking code that doesn't match your actual architecture.",
        "transition": "The second error is related but subtly different."
      }
    },
    {
      "type": "concept",
      "title": "Error 2: Expecting the Agent to 'Care'",
      "content": [
        "Reality: It executes your literal instruction to completion",
        "No sense of 'good enough' or 'this seems wrong'",
        "Will happily implement terrible architecture if asked",
        "Fix: Be precise and include explicit constraints"
      ],
      "speakerNotes": {
        "talkingPoints": "The agent has no stake in your project's success. It will implement exactly what you describe, even if that's a bad idea. It won't push back on poor architecture or question requirements that seem off.",
        "timing": "2 minutes",
        "discussion": "How is this different from working with a junior developer who might at least ask 'are you sure?' when something seems wrong?",
        "context": "Engineers who treat agents like teammates expect them to flag concerns. They don't. If you ask for a singleton when you need dependency injection, you'll get a perfect singleton.",
        "transition": "This leads us to the third and most important error."
      }
    },
    {
      "type": "concept",
      "title": "Error 3: Teammate vs Tool Mindset",
      "content": [
        "Reality: It's a precision instrument that speaks English",
        "No judgment, no initiative, no self-correction",
        "Requires verification systems, not trust",
        "Fix: Maintain operator mindset—you verify output"
      ],
      "speakerNotes": {
        "talkingPoints": "You don't blame a circular saw for a bad cut—you adjust your technique. Same with LLMs. They're tools that execute language-based instructions with impressive fluency but zero comprehension. Your job is to operate and verify.",
        "timing": "2-3 minutes",
        "discussion": "What verification systems do you currently have in place for human-written code? How might those need to adapt for AI-generated code?",
        "context": "The operator mindset is the key insight of this course. You're not managing a junior developer. You're operating a sophisticated code generation tool that needs architectural guardrails—tests, types, and lints that catch probabilistic errors.",
        "transition": "Let's crystallize this with the power tool analogy."
      }
    },
    {
      "type": "comparison",
      "title": "Power Tool Analogy",
      "left": {
        "label": "Circular Saw",
        "content": [
          "Doesn't 'understand' what you're building",
          "Executes based on how you operate it",
          "Bad cuts come from bad technique",
          "Requires safety equipment and verification"
        ]
      },
      "right": {
        "label": "LLM Agent",
        "content": [
          "Doesn't 'understand' your codebase",
          "Executes based on your instructions",
          "Bad code comes from bad prompts",
          "Requires tests, types, and verification"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "This analogy captures the operator mindset. Power tools are incredibly capable but require skill to operate well. The tool doesn't ensure quality—the operator does through technique and verification.",
        "timing": "2 minutes",
        "discussion": "What safety equipment does a carpenter use? What's the equivalent 'safety equipment' for operating AI agents?",
        "context": "Tests are your safety goggles. Type systems are your blade guards. Code review is your measurement verification. The agent doesn't ensure quality; your verification systems do.",
        "transition": "Let's wrap up with the key takeaways from this lesson."
      }
    },
    {
      "type": "takeaway",
      "title": "Key Takeaways",
      "content": [
        "LLMs predict tokens, not meaning",
        "Agents execute, LLMs predict",
        "Provide explicit context always",
        "Verify output like power tools"
      ],
      "speakerNotes": {
        "talkingPoints": "Four things to remember: LLMs are token prediction engines, not reasoning systems. Agent software provides the execution layer. Context must be explicit every time. And your job is verification, not trust.",
        "timing": "2 minutes",
        "discussion": "Which of these takeaways most challenges your current mental model of AI agents?",
        "context": "These principles will be reinforced throughout the course. Every technique we cover builds on this foundation of understanding what we're actually operating.",
        "transition": "Next lesson, we'll dive deeper into agent architecture and how your role as an engineer evolves into an operator role."
      }
    }
  ]
}