{
  "metadata": {
    "title": "Lesson 5: Grounding - Anchoring Agents in Reality",
    "lessonId": "lesson-5-grounding",
    "estimatedDuration": "40-50 minutes",
    "learningObjectives": [
      "Ground agents in external sources",
      "Choose tools by codebase scale",
      "Understand U-shaped attention curves",
      "Combine code and web grounding"
    ]
  },
  "slides": [
    {
      "type": "title",
      "title": "Lesson 5: Grounding",
      "subtitle": "Anchoring agents in your reality, not hallucinations",
      "content": [],
      "speakerNotes": {
        "talkingPoints": "We've covered the workflow (Plan, Execute, Validate) and communication patterns (effective prompting). Now we address a critical reality: agents hallucinate. They generate plausible solutions based on training data, not your actual codebase. This lesson teaches you how to ground agents in external information—your code, your architecture, current documentation—so they solve problems for YOUR system, not hypothetical ones.",
        "timing": "1 minute",
        "discussion": "Ask students: Who's had an agent confidently suggest a solution that doesn't exist in your codebase?",
        "context": "This is the bridge between communication and execution. Good prompts communicate intent. Grounding ensures the agent understands your context.",
        "transition": "Let's start with the fundamental problem: what happens when agents don't know your codebase exists."
      }
    },
    {
      "type": "concept",
      "title": "The Hallucination Problem",
      "content": [
        "Agent confidently generates JWT verification patterns",
        "Your codebase uses sessions, not JWTs",
        "Plausible solution based on training data, wrong for your system",
        "Root cause: agent has zero knowledge of your code"
      ],
      "speakerNotes": {
        "talkingPoints": "The agent's context window is its entire world. Everything outside it doesn't exist. When you ask 'fix the authentication bug,' the agent has no knowledge of whether you use JWT, sessions, OAuth, or anything else. It predicts tokens based on statistical patterns from training data—what works in many systems. For your system, this is dead wrong. The solution looks plausible, compiles cleanly, follows common patterns... and solves a problem you don't have.",
        "timing": "2-3 minutes",
        "discussion": "Real example from your production environment: What was the last time an agent's solution looked right but didn't fit your architecture?",
        "context": "This is the core problem that grounding solves. Without grounding, agents are operating in an information vacuum.",
        "transition": "The solution is grounding—injecting reality into the context window before the agent generates."
      }
    },
    {
      "type": "visual",
      "title": "Grounding: Injecting Reality",
      "component": "GroundingComparison",
      "caption": "Grounding bridges the gap between agent knowledge and system reality",
      "speakerNotes": {
        "talkingPoints": "This visual shows the problem-solution pair. Without grounding, agents operate in a vacuum—they don't know your patterns, constraints, or current state. With grounding, you feed external information into the context before generation: your codebase patterns, architectural decisions, current documentation. The agent now predicts tokens based on YOUR system, not generic training data.",
        "timing": "1-2 minutes",
        "discussion": "What sources of truth exist in your system? Code patterns? API contracts? Architecture docs?",
        "context": "Grounding is the bridge between generic AI capabilities and production-specific accuracy.",
        "transition": "There are three main sources for grounding: your codebase, web research, and documentation. Let's start with code discovery."
      }
    },
    {
      "type": "concept",
      "title": "Agentic Search: Agent-Driven Discovery",
      "content": [
        "Agent autonomously calls Grep, Read, Glob",
        "Works beautifully under 10,000 lines of code",
        "Two to three searches return 5-10 files",
        "Context stays clean for problem-solving"
      ],
      "speakerNotes": {
        "talkingPoints": "Agentic search means the agent decides what to search for, interprets results, and determines next steps. You don't need to manually run Grep—the agent does it. For small codebases, this is elegant: search for 'auth', get 8 files, read them (~15,000 tokens), the agent understands your authentication pattern and implements a fix. Context stays manageable.",
        "timing": "2-3 minutes",
        "discussion": "In your own projects, how much code typically needs to be read to understand a new feature area?",
        "context": "Agentic search is the default approach. It works until scale becomes a problem.",
        "transition": "But agentic search breaks down at larger scales. Let's talk about the context window problem."
      }
    },
    {
      "type": "concept",
      "title": "The Context Window Illusion",
      "content": [
        "Advertised: 200,000 tokens (Claude Sonnet 4.5)",
        "Realistic effective: 60,000-120,000 tokens",
        "That's 30-60% of advertised capacity",
        "Advertised numbers are marketing, not engineering reality"
      ],
      "speakerNotes": {
        "talkingPoints": "Claude Sonnet advertises 200,000 tokens. In practice, you can reliably count on 60,000-120,000 tokens of effective context. The rest is real but not reliable—not a bug in the model, it's transformer architecture under realistic constraints. You can stuff more tokens in, but attention quality degrades. This is the context window illusion: the advertised number sounds impressive but doesn't reflect what engineers should actually count on.",
        "timing": "2 minutes",
        "discussion": "Have you hit unexpected context limits in your work? When did you realize you were running low?",
        "context": "This is critical for understanding why sophisticated grounding strategies matter. You don't have unlimited tokens.",
        "transition": "The model's attention isn't uniform. It follows a specific pattern—the U-curve."
      }
    },
    {
      "type": "visual",
      "title": "U-Shaped Attention: How Transformers Work",
      "component": "UShapeAttentionCurve",
      "caption": "Beginning and end receive strong attention; middle gets skimmed or missed",
      "speakerNotes": {
        "talkingPoints": "This isn't a limitation of Claude specifically—it's how transformer attention mechanisms work under realistic constraints. The beginning of your context (system prompt, critical constraints) gets strong attention. The end of your context (current task) gets strong attention. Everything in the middle? Skimmed or missed. This means if you load 150,000 tokens into context, your critical constraints get pushed into the ignored middle. This is why sophisticated grounding strategies matter: you can't just dump your entire codebase into context and hope the agent finds what it needs.",
        "timing": "2-3 minutes",
        "discussion": "Where would your current task specification fall if you put 10,000 tokens of code discovery results before it?",
        "context": "Understanding this curve explains why sub-agents and semantic search are valuable—they keep the orchestrator's context clean.",
        "transition": "At scale, agentic search gets worse. Each search returns more results. Let's see how."
      }
    },
    {
      "type": "concept",
      "title": "Scale Problem: Agentic Search Breakdown",
      "content": [
        "Search 'authentication' in 100K-line project → 80+ files",
        "Reading them costs 60,000+ tokens",
        "Half your effective context consumed before reasoning starts",
        "Critical constraints pushed into ignored middle of U-curve"
      ],
      "speakerNotes": {
        "talkingPoints": "In a large codebase, agentic search returns overwhelming results. You ask the agent to find authentication code. It gets 80 files. Reading them all consumes 60,000 tokens—that's half your effective context window. Only then can the agent start reasoning about the actual problem. And your original constraints? Buried in the middle of the U-curve, being skimmed. This is why simple agentic search doesn't work at production scale.",
        "timing": "2 minutes",
        "discussion": "At what point does this become a problem in your own projects?",
        "context": "This sets up the need for semantic search and sub-agents.",
        "transition": "There are three solutions to this problem: semantic search, sub-agents, and careful context arrangement. Let's start with semantic search."
      }
    },
    {
      "type": "concept",
      "title": "Solution 1: Semantic Search",
      "content": [
        "Query by meaning instead of keywords",
        "'Authentication middleware' finds code without exact word match",
        "Built-in for IDE agents (Cursor, Windsurf)",
        "For CLI agents: ChunkHound, Claude Context via MCP"
      ],
      "speakerNotes": {
        "talkingPoints": "Semantic search uses vector embeddings to find code by concept, not text matching. You ask 'find middleware that validates user credentials' and get relevant code even if it never uses those exact words. 'Auth middleware', 'login verification', and 'JWT validation' are semantically near each other in vector space. The benefit: fewer false positives, better discovery at scale. IDE assistants include this automatically. CLI agents (including Claude Code) need MCP servers to add it.",
        "timing": "2-3 minutes",
        "discussion": "How often do you search your codebase for concepts vs exact keywords?",
        "context": "Semantic search is valuable but still fills your context. The real scaling solution is sub-agents.",
        "transition": "Semantic search helps, but it still consumes context tokens. The next solution—sub-agents—isolates that consumption entirely."
      }
    },
    {
      "type": "concept",
      "title": "Solution 2: Sub-Agents",
      "content": [
        "Sub-agent runs in isolated context: discovers, reads, analyzes",
        "Returns synthesis (2,000-5,000 tokens) not raw results",
        "Costs more total tokens but delivers accuracy",
        "Two architectures: autonomous (flexible) vs structured (reliable at scale)"
      ],
      "speakerNotes": {
        "talkingPoints": "A sub-agent is an agent invoked by another agent. You tell it: 'Research our JWT implementation—find all code, explain the pattern.' It runs in its own context window, executing 50-150K tokens of searches and file reads. When complete, it returns a synthesis: 'JWT implemented at src/auth/jwt.ts using Passport.js with 24-hour expiration.' That synthesis loads into your orchestrator's context—typically 2,000-5,000 tokens instead of raw search results. You pay for processing tokens in both contexts, increasing total cost, but your orchestrator maintains a clean context throughout. First-iteration accuracy typically saves tokens compared to correction cycles from polluted context.",
        "timing": "3 minutes",
        "discussion": "When would you prefer accuracy on the first attempt over cheaper but less accurate results?",
        "context": "Sub-agents are the scaling solution for large codebases. Claude Code includes an Explore sub-agent built-in.",
        "transition": "There are two different sub-agent architectures. The one you choose depends on scale."
      }
    },
    {
      "type": "concept",
      "title": "Sub-Agent Architectures",
      "content": [
        "Autonomous: Agent decides search strategy (Explore agent)",
        "Structured: Deterministic algorithm, LLM ranks relevance",
        "Autonomous: simpler, cheaper, flexible, degrades at scale",
        "Structured: complex, expensive, consistent, reliable at 100K+ LOC"
      ],
      "speakerNotes": {
        "talkingPoints": "Autonomous agents (like Claude Code's Explore) decide their own exploration strategy. You give them a question and they autonomously pick tools, sequences, then synthesize. Simpler to implement, lower cost per call, flexible across different research questions. The downside: they make suboptimal exploration choices in large codebases. Structured agents use deterministic algorithms (breadth-first traversal, multi-hop graphs) that you control. The LLM makes tactical decisions within your structure—'Should I expand this node?' but the overall exploration strategy is programmed. More complex to build, higher cost, but maintains consistency even at 1M lines of code. For 10K-100K codebases, autonomous often wins. For 100K+, structured becomes essential.",
        "timing": "2-3 minutes",
        "discussion": "What's more important to you: simpler implementation or guaranteed consistency at scale?",
        "context": "This explains the difference between Claude Code's Explore and ChunkHound.",
        "transition": "Now let's talk about how to choose the right grounding tools for your specific codebase size."
      }
    },
    {
      "type": "comparison",
      "title": "Codebase Scale: Tool Selection",
      "left": {
        "label": "Under 10K LOC",
        "content": [
          "Agentic search sufficient",
          "Grep returns manageable results",
          "Read 5-10 files, context clean",
          "Use Explore agent if available"
        ]
      },
      "right": {
        "label": "100K+ LOC",
        "content": [
          "Agentic search returns 50+ files",
          "Searches consume half context window",
          "Constraints pushed into ignored middle",
          "Use ChunkHound structured agent"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "Your codebase size determines grounding strategy. Under 10,000 lines, agentic search with Grep works beautifully. A few searches, a few files read, context stays clean. At 10,000-100,000 lines, add semantic search (Explore agent or ChunkHound) to reduce false positives. Over 100,000 lines, you need structured sub-agent architecture—anything less produces incomplete results. ChunkHound is currently the only MCP-based sub-agent option for CLI agents other than Claude Code (which includes Explore built-in).",
        "timing": "3 minutes",
        "discussion": "What's the LOC count of your primary codebase? Are your current grounding approaches appropriate for that scale?",
        "context": "This is a practical decision framework. Match your tools to your scale.",
        "transition": "Code grounding is half the story. The other half is web grounding—keeping current with ecosystem knowledge."
      }
    },
    {
      "type": "concept",
      "title": "Web Grounding: Same Pattern, Different Source",
      "content": [
        "Agents need ecosystem knowledge: API docs, best practices, advisories",
        "Simple web search works initially, hits context limits at scale",
        "Synthesis tools (Perplexity) compress results to 3-8K tokens",
        "ArguSeek isolation + state management for multi-source research"
      ],
      "speakerNotes": {
        "talkingPoints": "Code grounding anchors agents in your system. Web grounding gives them current ecosystem knowledge. Most assistants include basic web search—this works for simple queries. Same problem emerges: search results flood context, pushing constraints into ignored middle. Synthesis tools like Perplexity search, fetch, and compress before returning—reducing 30,000 tokens to 5,000 tokens per query. The limitation: after 3-5 queries you're context-limited and can't build on previous research. ArguSeek is a sub-agent for web research with semantic state management: it processes 12-30 sources per call while maintaining understanding of what you've already researched. You can make tens of calls, scanning 100+ sources total, keeping your orchestrator context clean.",
        "timing": "3 minutes",
        "discussion": "How many times have you needed to re-search topics because the tool didn't remember what you'd already learned?",
        "context": "Web grounding follows the same progression as code grounding: simple → synthesis → sub-agents.",
        "transition": "In production, you combine code grounding with web grounding for the most reliable solutions."
      }
    },
    {
      "type": "code",
      "title": "Production Pattern: Multi-Source Grounding",
      "language": "typescript",
      "code": "// Agent task: Implement caching\n// Step 1: Ground in code\n//   Search: existing cache patterns\n//   Result: Redis wrapper at src/cache.ts\n\n// Step 2: Ground in web\n//   Search: 'Redis caching patterns 2024'\n//   Result: current best practices\n\n// Agent now implements solution that:\n//   - Fits your architecture\n//   - Uses current best practices",
      "caption": "Combine code + web grounding for production accuracy",
      "speakerNotes": {
        "talkingPoints": "In production systems, grounding from code alone risks outdated patterns. 'We've always done caching this way' might not be how you should do it now. Web grounding from best practices alone doesn't fit your architecture. Combining both: ground in your codebase patterns, ground in current best practices, the agent implements a solution that works for YOUR system using current standards. This prevents two failure modes: hallucinations from missing context, and solutions that work in theory but don't fit your production constraints.",
        "timing": "2-3 minutes",
        "discussion": "When was the last time you found current best practices conflicted with your existing patterns? How did you resolve it?",
        "context": "This is the professional engineering approach to grounding.",
        "transition": "Let's summarize the key insights from this lesson."
      }
    },
    {
      "type": "takeaway",
      "title": "Key Takeaways",
      "content": [
        "Agents hallucinate without grounding—ground in code, docs, and web research",
        "Agentic search works under 10K LOC; semantic search extends to 100K+; sub-agents scale beyond",
        "Context window illusion: advertised 200K, reliable 60K-120K with U-shaped attention",
        "Production systems combine code grounding (your patterns) + web grounding (best practices)"
      ],
      "speakerNotes": {
        "talkingPoints": "The core lesson: agents operate in an information vacuum without grounding. Without knowledge of your codebase, architecture, current documentation, they predict based on statistical patterns from training data. This produces plausible solutions that are often wrong for your system. Grounding—injecting external information into context—solves this. The specific tools depend on scale: small projects need simple agentic search; larger projects need semantic search; massive codebases need structured sub-agents. The context window is your constraint: 60K-120K reliable tokens with U-shaped attention. Smart grounding keeps your orchestrator's context clean while delegating discovery to sub-agents. Production systems ground in both code (for architectural fit) and web research (for current best practices). With these patterns, agents become reliable tools for production engineering.",
        "timing": "2 minutes",
        "discussion": "Which grounding pattern will you apply first to your own projects?",
        "context": "This completes the methodology module. You now have Plan, Execute, Validate, Prompting, and Grounding.",
        "transition": "You now have the complete toolkit: workflows, communication, and context management. You're ready to operate AI agents at scale in production."
      }
    }
  ]
}
