{
  "metadata": {
    "title": "Lesson 5: Grounding - Anchoring Agents in Reality",
    "lessonId": "lesson-5-grounding",
    "estimatedDuration": "40-50 minutes",
    "learningObjectives": [
      "Inject external reality into context",
      "Scale discovery beyond simple search",
      "Manage attention in massive codebases",
      "Combine code and web grounding"
    ]
  },
  "slides": [
    {
      "type": "title",
      "title": "Lesson 5: Grounding",
      "subtitle": "Anchoring agents in your actual system",
      "content": [],
      "speakerNotes": {
        "talkingPoints": "This lesson tackles a fundamental problem: agents don't know your codebase exists. Without grounding—injecting real code and documentation into the context—agents confidently generate hallucinated solutions based on generic training patterns. We'll cover how to discover your actual architecture, scale discovery beyond simple search, and combine code and web grounding for production-ready solutions.",
        "timing": "1 minute",
        "discussion": "Open with a relatable question: 'Has an AI ever confidently suggested a pattern that doesn't exist in your codebase?' This sets up the problem we're solving.",
        "context": "This lesson builds on Lesson 2 (context window as agent's entire world) and Lesson 4 (prompting structure). Students should understand that everything outside the context window is invisible to the agent.",
        "transition": "Let's start with a concrete problem: what happens when agents hallucinate solutions?"
      }
    },
    {
      "type": "concept",
      "title": "The Hallucination Problem",
      "content": [
        "Agent generates JWT validation that doesn't match your session architecture",
        "Confidently wrong: statistically plausible but functionally incompatible",
        "Agent has no knowledge your codebase exists",
        "Works from frozen training patterns, not current reality",
        "Without grounding: generic patterns > actual implementation"
      ],
      "speakerNotes": {
        "talkingPoints": "Start with the scenario: you ask an agent to fix an authentication bug. It generates a pristine JWT implementation... but your system uses sessions, not JWTs. The solution compiles, passes basic tests, and looks correct. But it's hallucinated—built from statistical patterns in training data, not your actual architecture. The agent doesn't know your codebase exists.",
        "timing": "2-3 minutes",
        "discussion": "Ask: 'Why doesn't the agent just know what's in your repo?' Guide them to understand: context window = agent's entire universe. Everything else is statistically invisible.",
        "context": "This connects directly to Lesson 2's core insight: the context window is the agent's complete world. If your code isn't in the context, the agent builds solutions from generic patterns.",
        "transition": "So how do we inject reality? We need to understand how agents actually discover information—and why simple search breaks down at scale."
      }
    },
    {
      "type": "concept",
      "title": "Agentic Search: Autonomous Discovery",
      "content": [
        "Agent calls Glob, Grep, Read autonomously to explore codebase",
        "Works beautifully under 10,000 LOC: 2-3 searches find relevant files",
        "Breaks at scale: search 'authentication' in 100K LOC = 80+ files",
        "Context flood: 60,000+ tokens of search results consumed before discovery ends",
        "Your constraints pushed into ignored middle of context window"
      ],
      "speakerNotes": {
        "talkingPoints": "Agentic search is powerful for small projects. The agent decides what to search, interprets results, makes decisions. No pre-planning required. But scale breaks it: a single grep in a large codebase returns dozens or hundreds of files. Reading them consumes half your effective context before the agent finishes reconnaissance. Remember the U-curve from earlier: your critical constraints at the start get buried in the middle under piles of search results.",
        "timing": "3-4 minutes",
        "discussion": "Walk through a specific example: 'Search authentication in a 100K line codebase. You probably get 80+ files. Each file reads 500-2000 tokens. You're at 40,000-60,000 tokens just in search results, before the agent even starts solving the actual problem.' Ask: 'Where are your constraints now? Buried in the middle, being ignored.'",
        "context": "This builds on Lesson 2's U-shaped attention curve concept. Establish the connection: scale doesn't just add search results, it pushes critical information into the ignored middle.",
        "transition": "This is the core scaling problem. How do we stay in the sweet spot where discovery is accurate but doesn't pollute context? Three approaches: semantic search to be smarter about what we find, sub-agents to isolate discovery in separate contexts, and sophisticated code research pipelines."
      }
    },
    {
      "type": "visual",
      "title": "The Context Window Illusion",
      "component": "UShapeAttentionCurve",
      "caption": "Beginning and end receive attention; middle gets skimmed.",
      "speakerNotes": {
        "talkingPoints": "Claude advertises 200K tokens, but effective attention spans 60K-120K tokens—that's the context window illusion. This isn't a limitation of Claude specifically; it's transformer architecture. The U-shaped attention curve shows strong attention at the start (your initial constraints), strong attention at the end (the current task), but weak attention in the middle. Fill your context with search results and your constraints disappear into the weaker zone.",
        "timing": "2-3 minutes",
        "discussion": "Ask: 'If I put my constraints at the start and my task at the end, what happens to 50,000 tokens of search results in the middle?' Let students predict before confirming: they get skimmed or missed entirely.",
        "context": "This is the fundamental transformer limitation that drives all the architectural decisions in this lesson. Without understanding the U-curve, students won't understand why semantic search and sub-agents matter.",
        "transition": "Understanding the U-curve explains why naive agentic search fails at scale. Now let's look at solutions: how to be smarter about what we search."
      }
    },
    {
      "type": "concept",
      "title": "Solution 1: Semantic Search",
      "content": [
        "Query by meaning, not keywords: find 'auth middleware' even without exact matches",
        "Vector embeddings capture semantic relationships (768-1536 dimensions)",
        "Cosine similarity clusters related concepts in vector space",
        "Hybrid approach: semantic discovery + keyword precision",
        "Extends scale to 100,000+ LOC with fewer false positives"
      ],
      "speakerNotes": {
        "talkingPoints": "Semantic search solves keyword blindness. Instead of searching for the string 'authentication', you search for the concept 'code that validates user credentials'. Vector embeddings convert code to semantic representation, and cosine similarity finds related chunks even if they use different terminology. Available built-in in IDE assistants (Cursor, Windsurf). For CLI agents, you need MCP servers like ChunkHound or Claude Context.",
        "timing": "3 minutes",
        "discussion": "Ask: 'What if your auth code is labeled 'verify user identity' instead of 'authentication'? Grep misses it. Semantic search finds it.' This highlights the advantage over keyword-only approaches.",
        "context": "Semantic search is an evolution of agentic search, not a replacement. Agents still call search tools autonomously, but they search by concept rather than keywords. The scaling advantage comes from getting relevant results faster with fewer false positives.",
        "transition": "Semantic search extends our scale, but we still hit context limits if we try to read 100 files in one agent. That's where sub-agents come in—delegating research to separate contexts."
      }
    },
    {
      "type": "concept",
      "title": "Solution 2: Sub-Agents",
      "content": [
        "Orchestrator delegates research to sub-agent with isolated context",
        "Sub-agent processes 50K-150K tokens internally (search, read, analyze)",
        "Returns concise synthesis: 2K-5K tokens to orchestrator",
        "Trade-off: higher token cost for first-iteration accuracy",
        "Two architectures: autonomous (flexible, Explore agent) or structured (ChunkHound)"
      ],
      "speakerNotes": {
        "talkingPoints": "Think of sub-agents like function calls, but for agents. You write a prompt: 'Find all authentication code and explain current patterns.' The sub-agent executes in its own context with search tools, processes massive amounts internally, then returns a synthesis. The orchestrator gets the summary without ever seeing the 80 files or 100K search tokens. Cost is higher (you pay for processing in two contexts), but your orchestrator context stays clean, which usually reduces total iterations and saves tokens overall.",
        "timing": "3-4 minutes",
        "discussion": "Compare to function calls: 'Imagine you had to read the entire implementation of every function you call. That's what happens without sub-agents. With them, you call the function, get the result back, context stays clean.'",
        "context": "This is a powerful architectural pattern that applies beyond code research. Any time you need discovery or processing that would pollute your main context, consider delegating to a sub-agent.",
        "transition": "Sub-agents come in two flavors: autonomous and structured. Let's compare when each makes sense."
      }
    },
    {
      "type": "comparison",
      "title": "Sub-Agent Architectures",
      "left": {
        "label": "Autonomous",
        "content": [
          "Agent decides search strategy independently",
          "Flexible across different research tasks",
          "Simpler to build and cheaper to run",
          "Examples: Claude Code's Explore agent",
          "Works well for small-medium codebases"
        ]
      },
      "right": {
        "label": "Structured",
        "content": [
          "Deterministic control plane defines search algorithm",
          "LLM makes tactical decisions within your framework",
          "Higher token cost, consistent at extreme scale",
          "Examples: ChunkHound multi-hop pipeline",
          "Essential for 100K+ LOC with complex architecture"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "Autonomous agents are flexible and cheap. Give them a goal, they figure out how to achieve it. Works great for varied research tasks. Structured agents follow a prescribed algorithm—like breadth-first traversal with reranking at decision points. More expensive to build and run, but scale predictably to massive codebases where autonomous agents start making suboptimal exploration choices.",
        "timing": "2-3 minutes",
        "discussion": "Ask: 'If I'm researching a small codebase, do I want structured multi-hop traversal or just let an agent explore?' Students should recognize that autonomous is better for simple tasks, but structured wins at scale.",
        "context": "This trade-off appears throughout engineering: flexibility vs. predictability. Autonomous agents are more flexible, structured agents are more predictable. Choose based on scale and consistency requirements.",
        "transition": "Now that we understand the scaling solutions, let's be practical: how do you choose the right tool for your specific codebase size?"
      }
    },
    {
      "type": "concept",
      "title": "Code Grounding by Scale",
      "content": [
        "Under 10K LOC: Agentic search works reliably",
        "10K-100K LOC: Add semantic search or Explore sub-agent",
        "100K+ LOC: Structured code research (ChunkHound) becomes valuable",
        "Scale determines not if you need grounding, but which tools work",
        "Tool selection is empirical: measure LOC, profile context usage, iterate"
      ],
      "speakerNotes": {
        "talkingPoints": "This is practical guidance for choosing tools. Use cloc to measure your codebase. Under 10K lines, simple agentic search with Grep and Read works beautifully—no overhead needed. At 10K-100K, you hit the inflection point where semantic search or a simple sub-agent becomes valuable if you're repeatedly connecting components across your codebase. At 100K+, the complexity of independent components means autonomous agents miss connections; structured approaches like ChunkHound become necessary. At 1M+ lines, sub-agents are non-negotiable.",
        "timing": "3-4 minutes",
        "discussion": "Ask students about their codebases: 'How many lines of code are you working with?' Discuss why their specific scale suggests certain tools. Make it concrete: 'If you're under 10K, you probably don't need ChunkHound yet. If you're at 100K, simple search is costing you iterations.'",
        "context": "This connects to the architecture-first principle in the course: understand your constraints before choosing tools. Codebase size is a hard constraint that determines which grounding strategy works.",
        "transition": "Code grounding handles your codebase. But agents also need current external knowledge—docs, best practices, security advisories. That's web grounding, and it follows the same progression."
      }
    },
    {
      "type": "concept",
      "title": "Web Grounding: Same Problem, Different Source",
      "content": [
        "Built-in search works for simple queries but floods context",
        "Synthesis tools (Perplexity) compress results: 15K tokens → 3K-8K tokens",
        "ArguSeek isolates research context and maintains semantic state",
        "Multi-query decomposition scans 100+ sources while keeping orchestrator clean",
        "Same progression: agentic search → semantic search → sub-agents"
      ],
      "speakerNotes": {
        "talkingPoints": "Web grounding mirrors code grounding architecturally. Simple web search returns results that flood your context (same U-curve problem). Synthesis tools like Perplexity compress pages into summaries (improvement, but still limited). ArguSeek goes further: it decomposes your research question into multiple concurrent queries (docs + community + security), scans 100+ sources total, maintains semantic state so follow-ups don't repeat basics, and returns a clean synthesis. You can make many ArguSeek calls while keeping your orchestrator context manageable.",
        "timing": "3-4 minutes",
        "discussion": "Ask: 'If you're researching 'best practices for caching strategies', how many sources would you ideally read? 10? 30? 100?' Then explain: 'ArguSeek lets you examine 100+ sources while your orchestrator context stays clean. That's the advantage.'",
        "context": "Web grounding is often underestimated. Students sometimes think 'just use web search' without realizing the context pollution problem. This slide establishes that external knowledge has the same scaling challenges as codebase knowledge.",
        "transition": "Separately, code grounding and web grounding both solve important problems. But production systems combine them—and that's where the real power emerges."
      }
    },
    {
      "type": "codeExecution",
      "title": "Multi-Source Grounding in Production",
      "steps": [
        {
          "line": "Engineer specifies: 'Implement caching layer for user queries'",
          "highlightType": "human",
          "annotation": "Task definition with implicit constraints"
        },
        {
          "line": "Orchestrator delegates to code research sub-agent",
          "highlightType": "prediction",
          "annotation": "LLM decides: need to understand current architecture"
        },
        {
          "line": "Sub-agent discovers: Redis pattern in src/cache/redis.ts, TTL handling in src/models/query.ts",
          "highlightType": "execution",
          "annotation": "Semantic search + file exploration"
        },
        {
          "line": "Code synthesis returns: 'Architecture uses Redis with 300s TTL, query invalidation via pub/sub'",
          "highlightType": "feedback",
          "annotation": "Compressed architectural knowledge"
        },
        {
          "line": "Orchestrator delegates to web research sub-agent",
          "highlightType": "prediction",
          "annotation": "LLM decides: need current best practices"
        },
        {
          "line": "Web research scans docs, community posts, security advisories",
          "highlightType": "execution",
          "annotation": "Multi-query decomposition across 30+ sources"
        },
        {
          "line": "Web synthesis returns: 'Redis 7.2+ supports keyspace notifications, cache-aside pattern preferred for consistency'",
          "highlightType": "feedback",
          "annotation": "Current ecosystem knowledge"
        },
        {
          "line": "Orchestrator now has actual architecture + current best practices in context",
          "highlightType": "summary",
          "annotation": "Clean context with grounded, current information"
        }
      ],
      "speakerNotes": {
        "talkingPoints": "Walk through a complete production scenario. The engineer asks for a caching layer. The agent doesn't guess—it researches. Code research discovers your actual patterns (Redis, TTL handling, pub/sub). Web research finds current best practices (Redis 7.2 features, cache-aside pattern). Then the agent solves the problem informed by both: your actual architecture AND current standards. No hallucinations, no outdated patterns, solution that integrates cleanly.",
        "timing": "4-5 minutes",
        "discussion": "Compare to the hallucination scenario from slide 2: 'Without grounding, the agent guesses. With code grounding alone, it matches your architecture but might use old patterns. With web grounding alone, it gives best practices that don't fit your system. Combined, it's both correct and current.'",
        "context": "This is the payoff for everything so far. All the complexity of semantic search, sub-agents, and web research serves this outcome: agents that solve problems correctly the first time.",
        "transition": "Let's wrap up with the key insights that will guide your use of grounding in production."
      }
    },
    {
      "type": "takeaway",
      "title": "Key Takeaways",
      "content": [
        "Agents hallucinate without grounding: context window is their entire universe",
        "Scale discovery with semantic search and sub-agents, not just keyword search",
        "U-shaped attention means your constraints get ignored if buried under search results",
        "Combine code grounding (architecture) with web grounding (current standards)",
        "Choose tools empirically by codebase size: measure LOC, profile context, iterate"
      ],
      "speakerNotes": {
        "talkingPoints": "Reinforce the core principle: agents work from context. Everything outside is statistically invisible. That's not a limitation—it's the insight that drives all grounding strategies. Agentic search is powerful but scales poorly. Semantic search extends scale by querying by meaning. Sub-agents isolate discovery in separate contexts. Web grounding prevents outdated patterns. All of these techniques serve one goal: making agents work within real constraints instead of hallucinating plausible solutions.",
        "timing": "3-4 minutes",
        "discussion": "Open discussion: 'Which of these insights changes how you think about using AI agents in your projects? Where do you see opportunities to apply grounding in your own work?' Give students time to connect concepts to their reality.",
        "context": "This lesson completes the methodology module. Together, Lessons 3-5 form a complete framework: Plan > Execute > Validate (lesson 3), Prompting patterns (lesson 4), and Grounding strategies (lesson 5). Students now have the core production workflows.",
        "transition": "The methodology module is complete. You have planning, prompting, and grounding. Next module moves to practical techniques: project onboarding, debugging workflows, testing patterns, and code review at scale."
      }
    }
  ]
}
