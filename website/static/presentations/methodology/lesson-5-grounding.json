{
  "metadata": {
    "title": "Lesson 5: Grounding - Anchoring Agents in Reality",
    "lessonId": "lesson-5-grounding",
    "estimatedDuration": "40-50 minutes",
    "learningObjectives": [
      "Ground agents in codebase",
      "Understand context window limits",
      "Scale discovery with sub-agents",
      "Combine code and web research"
    ]
  },
  "slides": [
    {
      "type": "title",
      "title": "Lesson 5: Grounding",
      "subtitle": "Anchoring Agents in Reality",
      "content": [],
      "speakerNotes": {
        "talkingPoints": "This lesson covers grounding—the core technique for preventing hallucinations. Without grounding, agents generate statistically plausible solutions based on training data. With grounding, you inject reality (your codebase, current documentation, actual constraints) into the context window so agents build solutions that work for your specific system.",
        "timing": "1 minute",
        "discussion": "Ask students: When have you seen an AI generate confident but wrong code? Usually missing context about your actual system.",
        "context": "Grounding is why production AI coding works. It's the difference between demo code and production code.",
        "transition": "Let's start with the problem: what happens when agents don't know your system exists."
      }
    },
    {
      "type": "concept",
      "title": "The Hallucination Problem",
      "content": [
        "Agent confidently generates JWT solution",
        "Your codebase actually uses sessions",
        "Agent doesn't know your architecture exists",
        "Context window is agent's entire world"
      ],
      "speakerNotes": {
        "talkingPoints": "When you ask an agent to 'fix the authentication bug,' it starts with zero knowledge of your system. It generates plausible solutions based on common patterns in training data—JWT, OAuth, etc. The agent doesn't know whether you use sessions or tokens, what libraries you've chosen, or what constraints exist in your architecture.",
        "timing": "2 minutes",
        "discussion": "Ask: What information does the agent need to generate correct solutions? (Your actual patterns, current docs, existing constraints.)",
        "context": "This is why Lesson 2 emphasized context window as the agent's entire world. What's not in context doesn't exist to the agent.",
        "transition": "Grounding is how you inject that reality into the context window. Let's look at two approaches: discovery and synthesis."
      }
    },
    {
      "type": "visual",
      "title": "Without vs With Grounding",
      "component": "GroundingComparison",
      "caption": "Grounding prevents hallucinations by injecting reality.",
      "speakerNotes": {
        "talkingPoints": "This visualization contrasts two scenarios. Without grounding, the agent works from frozen training data, guesses your architecture, and generates confident but wrong code. With grounding, you provide actual code patterns, current documentation, and real constraints before the agent generates.",
        "timing": "2 minutes",
        "discussion": "Which scenario leads to production-ready code? Why? (Grounding ensures solutions match your actual system.)",
        "context": "This is why every production AI coding workflow includes grounding. It's not optional—it's the bridge between generic AI and your specific system.",
        "transition": "Now let's look at how agents discover your codebase: agentic search."
      }
    },
    {
      "type": "concept",
      "title": "Agentic Search: Discovery Phase",
      "content": [
        "Agent autonomously calls Grep, Read, Glob",
        "Decides what to search based on task",
        "Interprets results and determines next steps",
        "Works beautifully under 10,000 lines of code"
      ],
      "speakerNotes": {
        "talkingPoints": "Agentic search is how agents discover your codebase without explicit instruction. You say 'fix the JWT bug,' the agent autonomously decides to grep for JWT code, reads the relevant files, and builds a mental model. The agent acts like a developer exploring unfamiliar code.",
        "timing": "2 minutes",
        "discussion": "In your experience, how do developers explore unfamiliar codebases? (Search for keywords, read relevant files, ask questions.) Agents do the same but autonomously.",
        "context": "This works great for small projects. Two grep searches return 5-10 files, 15,000 tokens total. Agent reads them and solves the problem.",
        "transition": "But what happens as codebases grow? Let's look at the scale problem."
      }
    },
    {
      "type": "code",
      "title": "Agentic Search in Action",
      "language": "text",
      "code": "Task: \"Fix JWT expiration validation\"\n\nAgent: I'll search for JWT-related code\n→ Grep(\"JWT\", \"**/*.ts\")\n\nTool result: Found in:\n  - src/auth/jwt.ts:45: export function verifyToken(token: string)\n  - src/middleware/auth.ts:12: import { verifyToken } from '../auth/jwt'\n  - tests/auth.test.ts:8: describe('JWT verification')\n\nAgent: Let me read the verification logic\n→ Read(\"src/auth/jwt.ts\")\n\nTool result: [verification logic shows expiration check missing]\n\nAgent: Found it - the verifyToken function doesn't check exp claim",
      "caption": "Agent autonomously discovers codebase by searching and reading.",
      "speakerNotes": {
        "talkingPoints": "This example shows how agentic search works in practice. The agent initiates searches based on the task, interprets results, and decides what to read next. No explicit instruction—the agent is driving its own discovery.",
        "timing": "2 minutes",
        "discussion": "Compare this to how you'd explore unfamiliar code. What's similar? (Both search, both read, both iterate.) What's different? (Agent is doing it autonomously.)",
        "context": "This pattern works perfectly in codebases under 10,000 lines. For larger codebases, it hits scaling problems.",
        "transition": "That brings us to the context window challenge. Even with autonomous discovery, large codebases create problems."
      }
    },
    {
      "type": "visual",
      "title": "Context Window Limits: The U-Curve",
      "component": "UShapeAttentionCurve",
      "caption": "Transformers attend strongly at beginning and end only.",
      "speakerNotes": {
        "talkingPoints": "This is the U-shaped attention curve. Transformers have strong attention at the beginning of context (your initial constraints) and the end (your task). The middle gets skimmed or missed. Claude Sonnet 4.5 advertises 200,000 tokens but reliably processes 60,000-120,000 tokens effectively—30-60% of advertised capacity.",
        "timing": "3 minutes",
        "discussion": "What happens to critical information you put in the middle of a long context? (It gets ignored.) What's the implication for agentic search? (Search results push your constraints into the ignored middle.)",
        "context": "This is transformer architecture, not a limitation we can overcome. It's fundamental to how attention mechanisms work. Understanding this changes how you structure agent workflows.",
        "transition": "As codebases grow, agentic search amplifies this problem. Let's look at why."
      }
    },
    {
      "type": "concept",
      "title": "Why Scale Breaks Agentic Search",
      "content": [
        "Search 'authentication' in 100K-line project returns 80+ files",
        "Reading files consumes 60,000+ tokens before discovery finishes",
        "Critical constraints pushed into ignored middle",
        "Agent generates solutions based on partial understanding"
      ],
      "speakerNotes": {
        "talkingPoints": "The math of scale. Search returns 80+ files. Each file averages 800 tokens. You're at 64,000 tokens just from search results. Add reading 5 files (22,000 tokens) and you're at 86,000 tokens with incomplete discovery. Your original constraints that were at the start are now in the ignored middle where the agent can't see them.",
        "timing": "2 minutes",
        "discussion": "How would you solve this? (Pre-retrieve relevant code, use semantic search for better results, isolate research in separate contexts.)",
        "context": "This is why tool selection matters. Different codebases need different grounding approaches.",
        "transition": "There are two main solutions: semantic search for better relevance, and sub-agents for context isolation. Let's start with semantic search."
      }
    },
    {
      "type": "concept",
      "title": "Solution 1: Semantic Search",
      "content": [
        "Query by meaning, not keywords",
        "Code converted to vectors capturing semantic meaning",
        "Cosine similarity finds related concepts",
        "Extends effective scale to 100,000+ lines of code"
      ],
      "speakerNotes": {
        "talkingPoints": "Semantic search uses embedding models to convert code into vectors (768-1536 dimensions) that capture meaning. 'Auth middleware', 'login verification', and 'JWT validation' map to nearby vectors. The system understands they're related even without keyword matches. This dramatically reduces search false positives and noise.",
        "timing": "2 minutes",
        "discussion": "What's the difference between keyword search and semantic search? (Keyword searches for exact text; semantic understands meaning.) When is semantic search valuable? (Finding conceptually related code without exact keyword matches.)",
        "context": "IDE assistants often include semantic search built-in. CLI agents need MCP servers like ChunkHound to add this capability.",
        "transition": "Semantic search improves discovery quality. But even with perfect search, filling your context with results is still a problem. That's where sub-agents come in."
      }
    },
    {
      "type": "concept",
      "title": "Solution 2: Sub-Agents for Context Isolation",
      "content": [
        "Delegate research to sub-agent with isolated context",
        "Sub-agent processes 50,000-150,000 tokens internally",
        "Returns concise synthesis: 2,000-5,000 tokens",
        "Orchestrator maintains clean context throughout"
      ],
      "speakerNotes": {
        "talkingPoints": "A sub-agent is an agent spawned by another agent with its own isolated context. Your orchestrator (with your task and constraints) delegates: 'Research JWT authentication patterns.' The sub-agent runs searches, reads files, and synthesizes findings. When complete, it returns a brief summary to your orchestrator—maybe 3,000 tokens instead of 50,000 tokens of raw search results.",
        "timing": "2-3 minutes",
        "discussion": "What's the trade-off? (You pay to process tokens in both contexts—higher cost. But your orchestrator stays clean—first-iteration accuracy.)",
        "context": "This is why Claude Code includes the Explore agent built-in. For other CLI agents, you need MCP servers like ChunkHound.",
        "transition": "There are two main sub-agent architectures. Let's compare them."
      }
    },
    {
      "type": "comparison",
      "title": "Sub-Agent Architectures",
      "neutral": true,
      "left": {
        "label": "Autonomous",
        "content": [
          "Agent decides search strategy",
          "Simpler to build and maintain",
          "Works well for varied tasks",
          "Degrades on very large codebases"
        ]
      },
      "right": {
        "label": "Structured",
        "content": [
          "Deterministic control plane defines algorithm",
          "Scales reliably to millions of LOC",
          "LLM makes tactical decisions within structure",
          "Higher cost and complexity to build"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "Two valid architectural approaches. Autonomous agents (like Claude Code's Explore) decide what to search and in what order. Structured agents (like ChunkHound) follow a fixed algorithm with decision points. Autonomous is cheaper and more flexible. Structured scales better but costs more.",
        "timing": "2 minutes",
        "discussion": "Which would you choose for a 50K-line codebase? (Autonomous is cheaper.) For 2M lines? (Structured is necessary.)",
        "context": "Your codebase scale determines which approach works best for you.",
        "transition": "Now let's look at the practical tool selection matrix. Which tools should you use at different scales?"
      }
    },
    {
      "type": "code",
      "title": "Tool Selection by Codebase Scale",
      "language": "markdown",
      "code": "Under 10,000 LOC:\nRECOMMENDED: Agentic search (Grep, Read, Glob)\nWHY: Two searches return 15,000 tokens total\n\n10,000-100,000 LOC:\nRECOMMENDED: Semantic search OR Explore sub-agent\nWHY: Extends scale, reduces false positives\nBREAKING POINT: Searches returning 50+ files\n\n100,000+ LOC:\nRECOMMENDED: ChunkHound (structured sub-agent)\nWHY: Multi-hop traversal, architectural relationships\nBREAKING POINT: Autonomous agents miss connections\n\n1,000,000+ LOC:\nREQUIRED: Structured sub-agent with state\nWHY: Only approach with progressive aggregation",
      "caption": "Select grounding tools based on your codebase size.",
      "speakerNotes": {
        "talkingPoints": "This is the practical decision tree. Measure your codebase with 'cloc .' and use that to choose tools. The breaking points are where tools stop being effective and you need the next level of sophistication.",
        "timing": "2 minutes",
        "discussion": "What's your codebase size? Which tools would you use? What problems have you seen from using the wrong tool at your scale?",
        "context": "These guidelines come from real production experience. Autonomous agents handling massive codebases consistently miss architectural connections.",
        "transition": "That's code grounding. Web grounding follows the same pattern: simple tools that break at scale, then more sophisticated solutions."
      }
    },
    {
      "type": "concept",
      "title": "Web Grounding: Same Pattern, Different Sources",
      "content": [
        "Need current docs, best practices, security advisories",
        "Built-in search works for simple queries",
        "Context pollution problem applies here too",
        "Sub-agents isolate research, maintain orchestrator context"
      ],
      "speakerNotes": {
        "talkingPoints": "Web grounding (API docs, security advisories, best practices) faces the same scaling problem as code grounding. Each search consumes 8,000-15,000 tokens. Each page fetch adds 3,000-10,000 tokens. The U-curve still applies—your constraints get buried in the middle.",
        "timing": "2 minutes",
        "discussion": "How is web research different from code research? (Different sources, but same context management problem.)",
        "context": "This is why web synthesis tools (like ArguSeek) exist. They process multiple sources and return synthesis instead of raw results.",
        "transition": "ArguSeek is the web-grounding equivalent of ChunkHound for code. Let's look at how it works."
      }
    },
    {
      "type": "concept",
      "title": "ArguSeek: Web Research Sub-Agent",
      "content": [
        "Processes 12-30 sources per call, 100+ total per task",
        "Google Search API for quality over Bing alternatives",
        "Semantic subtraction skips duplicate content in follow-ups",
        "Keeps orchestrator context clean while researching extensively"
      ],
      "speakerNotes": {
        "talkingPoints": "ArguSeek is structured for web research the way ChunkHound is for code. It combines Google Search, query decomposition (3 concurrent query variations), and semantic subtraction (tracking what's been covered to avoid re-explaining). You can make dozens of research calls while your orchestrator stays clean.",
        "timing": "2 minutes",
        "discussion": "What's semantic subtraction? (Tracking what's been covered so follow-up queries skip duplicate content and advance research.) Why does that matter? (Avoids re-explaining basics; makes research progress faster.)",
        "context": "This is currently the only web research tool with Google Search API quality plus semantic state management.",
        "transition": "Now let's see how code and web grounding work together in production."
      }
    },
    {
      "type": "code",
      "title": "Production Pattern: Multi-Source Grounding",
      "language": "text",
      "code": "Task: \"Implement OAuth2 client credentials flow for our API\"\n\n1. Code research: How does existing authentication work? (ChunkHound)\n   → Returns: Current session-based auth architecture, middleware patterns,\n              where auth config lives (3,200 tokens)\n\n2. Web research: What are current OAuth2 best practices and known CVEs? (ArguSeek)\n   → Returns: RFC 6749 implementation guidance, security considerations,\n              recent vulnerabilities in popular libraries (4,800 tokens)\n\n3. Implementation: Synthesize both sources\n   → Follows your existing architecture patterns (code-grounded)\n   → Uses 2025 security standards (web-grounded)\n   → Avoids known vulnerabilities (web-grounded)\n   → Integrates cleanly with your middleware (code-grounded)",
      "caption": "Combine code and web research for solutions that work.",
      "speakerNotes": {
        "talkingPoints": "This is the complete production pattern. Code-only grounding prevents hallucinations but risks outdated patterns. Web-only grounding gives current best practices but doesn't match your architecture. Combining both—code grounding for 'how we do it' and web grounding for 'what's current'—prevents failure modes in both directions.",
        "timing": "2-3 minutes",
        "discussion": "What would happen if you only used code grounding? (Works for your system but might follow outdated patterns.) Only web grounding? (Current best practices but doesn't fit your architecture.)",
        "context": "This is why production AI coding workflows involve both. Neither alone is sufficient.",
        "transition": "Let's summarize the key insights from this lesson."
      }
    },
    {
      "type": "takeaway",
      "title": "Key Takeaways",
      "content": [
        "Context window is agent's reality",
        "Sub-agents isolate research cleanly",
        "Ground in code and web",
        "Scale determines your tool choice"
      ],
      "speakerNotes": {
        "talkingPoints": "These are the fundamental insights. First: without information in context, the agent can't use it—it hallucinates instead. Second: at scale, research floods your context; sub-agents isolate that in separate contexts. Third: production systems need both code (your patterns) and web (current standards) grounding. Fourth: your codebase size determines which tools work.",
        "timing": "2 minutes",
        "discussion": "How would you apply these insights to your current project? What's your codebase size and which grounding tools would you start with?",
        "context": "These principles apply across all AI coding work—whether you're using Claude Code, IDE assistants, or building custom agents.",
        "transition": "You now have the complete methodology: Plan, Execute, Validate (Lesson 3), Prompting 101 (Lesson 4), and Grounding (Lesson 5). You have the fundamentals to operate AI agents effectively in production."
      }
    }
  ]
}
