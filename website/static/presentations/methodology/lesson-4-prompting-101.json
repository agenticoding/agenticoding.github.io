{
  "metadata": {
    "title": "Lesson 4: Prompting 101",
    "lessonId": "lesson-4-prompting-101",
    "estimatedDuration": "35-45 minutes",
    "learningObjectives": [
      "Prompting as pattern completion",
      "Structure drives model attention",
      "Specificity eliminates ambiguity",
      "CoT controls execution paths"
    ]
  },
  "slides": [
    {
      "type": "title",
      "title": "Prompting 101",
      "subtitle": "Pattern Completion, Not Conversation",
      "content": [],
      "speakerNotes": {
        "talkingPoints": "This lesson fundamentally shifts how engineers think about prompting. We're not having conversations with AI models—we're initializing pattern completion engines. The mental model change is critical: instead of asking politely, we're drawing the beginning of a pattern and letting the model predict what naturally comes next based on training data.",
        "timing": "1 minute",
        "discussion": "Ask students: Have you ever been surprised by what an AI generated? Often it's because you started a pattern the model completed differently than expected.",
        "context": "This mental model explains why pleasantries waste tokens, why specificity matters, and why structure directs attention. It's the foundation for everything else in this lesson.",
        "transition": "Let's start with the core principle: prompting as pattern completion."
      }
    },
    {
      "type": "concept",
      "title": "The Pattern Completion Model",
      "content": [
        "Prompting starts a sequence pattern",
        "Model predicts what naturally follows",
        "Your prompt is the beginning, not a request",
        "Specificity constrains the completion space",
        "Structure makes patterns scannable"
      ],
      "speakerNotes": {
        "talkingPoints": "Think about how you write code. You start with a function signature, the model completes the body. You specify types and constraints, the model respects them. This isn't the AI understanding your request—it's statistical pattern completion. The more precisely you draw the pattern start, the more constrained (and correct) the completion.",
        "timing": "3-4 minutes",
        "discussion": "Have students compare: 'Write a function' vs 'Write a TypeScript function that validates email addresses per RFC 5322.' Why does the second get better results?",
        "context": "In production, vague patterns lead to multiple iterations. Precise patterns reduce iterations from 5+ to 1-2. This saves engineering time significantly.",
        "transition": "Let's see this in action with imperative commands."
      }
    },
    {
      "type": "codeComparison",
      "title": "Imperative Commands: Pattern Start",
      "leftCode": {
        "label": "Vague Pattern",
        "language": "text",
        "code": "Could you help me write a\nfunction to validate email\naddresses? Thanks in advance!"
      },
      "rightCode": {
        "label": "Precise Pattern",
        "language": "text",
        "code": "Write a TypeScript function\nthat validates email addresses\nper RFC 5322.\nHandle edge cases:\n- Multiple @ symbols (invalid)\n- Missing domain (invalid)\n- Plus addressing (valid)\n\nReturn { valid: boolean,\nreason?: string }"
      },
      "speakerNotes": {
        "talkingPoints": "The vague version gives the model no constraints—it must guess language, standard, edge cases, return type. The precise version draws a complete pattern: TypeScript, RFC 5322, explicit edge cases, specific return shape. The model simply completes what naturally follows.",
        "timing": "2-3 minutes",
        "discussion": "Point out the tokens wasted: 'Could you help me' and 'Thanks in advance' add nothing. The concrete specification (RFC 5322, specific edge cases, return type) is what drives quality completion.",
        "context": "This is especially important in production when you're prompt-chaining: output from one step becomes input to the next. Vague patterns compound errors across steps.",
        "transition": "Now let's look at how action verbs sharpen the pattern."
      }
    },
    {
      "type": "concept",
      "title": "Action Verbs: Establish Patterns",
      "content": [
        "\"Write\" vs \"Make\" → Code pattern, not draft",
        "\"Debug X in File.ts:47\" → Pinpoints scope exactly",
        "\"Add JSDoc to exported functions\" → Defines boundaries",
        "\"Optimize for indexed columns\" → Implementation, not vague improvement"
      ],
      "speakerNotes": {
        "talkingPoints": "Weak verbs like 'make' or 'fix' start generic patterns. Strong verbs establish domain-specific patterns. 'Write' starts a code block. 'Debug' starts a diagnostic pattern. 'Add JSDoc' starts a documentation pattern. The verb you choose determines what completion patterns the model retrieves from training data.",
        "timing": "2-3 minutes",
        "discussion": "Ask: Why doesn't 'Improve performance' work well? Because it's too generic—the model fills gaps. Compare with 'Optimize the query to use indexed columns'—now there's one clear pattern to complete.",
        "context": "In security reviews, 'check for issues' is useless. 'Review for SQL injection vulnerabilities' triggers security-specific patterns. Same principle across domains.",
        "transition": "Strong verbs work because they constrain completion space. Constraints are essential—let's see why."
      }
    },
    {
      "type": "codeComparison",
      "title": "Constraints: Define Boundaries",
      "leftCode": {
        "label": "Unconstrained",
        "language": "text",
        "code": "Add authentication to the API"
      },
      "rightCode": {
        "label": "Constrained",
        "language": "text",
        "code": "Add authentication middleware to\nthe Express API in middleware/auth.ts.\nUse JWT tokens with RS256 signing.\nSet token expiration to 1 hour.\nAdd refresh token rotation.\nReturn { token, refreshToken,\nexpiresIn }"
      },
      "speakerNotes": {
        "talkingPoints": "Without constraints, the model guesses: Which authentication method? JWT, OAuth, sessions? Which endpoints? How long are tokens valid? These gaps compound into code that doesn't fit your architecture. Constraints eliminate guessing by explicitly defining the problem space.",
        "timing": "2-3 minutes",
        "discussion": "Walk through the unconstrained version: Ask students what they'd expect. Most will realize the model could go in 10 different directions. The constrained version allows only one reasonable completion.",
        "context": "Production systems fail silently when authentication doesn't match your token infrastructure. Constraints prevent this entirely.",
        "transition": "Constraints work. Now, when do personas help—and when do they waste tokens?"
      }
    },
    {
      "type": "concept",
      "title": "Personas: Vocabulary Retrieval",
      "content": [
        "Personas bias vocabulary distribution",
        "\"Security engineer\" triggers threat-focused terms",
        "\"Performance engineer\" retrieves optimization patterns",
        "Skip personas for straightforward tasks—tokens wasted"
      ],
      "speakerNotes": {
        "talkingPoints": "A persona like 'You are a security engineer' doesn't add knowledge—it changes which knowledge gets retrieved. During attention, the persona shifts vocabulary weights. 'Threat model,' 'attack surface,' 'least privilege' become more likely tokens. This is semantically equivalent to listing those terms explicitly, but more efficient because it triggers a cluster of related security concepts.",
        "timing": "3-4 minutes",
        "discussion": "Compare: 'Review this code for issues' vs 'You are a security engineer. Review this code and identify vulnerabilities.' First one is generic advice. Second one retrieves specific threat patterns. Vocabulary is the control interface for semantic retrieval.",
        "context": "This principle applies everywhere: ChunkHound queries, vector databases, web search. 'Authentication middleware patterns' retrieves different results than 'login code.' Choose terms that retrieve patterns you need.",
        "transition": "Personas shape vocabulary. Structure organizes information. Now let's talk about explicit step-by-step control with Chain-of-Thought."
      }
    },
    {
      "type": "codeComparison",
      "title": "Chain-of-Thought: Explicit Steps",
      "leftCode": {
        "label": "No CoT",
        "language": "text",
        "code": "Review this SQL query for N+1\nproblems and optimize it"
      },
      "rightCode": {
        "label": "With CoT",
        "language": "text",
        "code": "Review this SQL query for N+1\nproblems.\nStep 1: Identify the main query\nStep 2: Find join patterns\nStep 3: Check for loops in calling\ncode\nStep 4: Propose indexed solutions\nStep 5: Test the optimized query\nReturn analysis at each step"
      },
      "speakerNotes": {
        "talkingPoints": "Without CoT, the model completes in one pass—it might skip steps or take shortcuts. With CoT, you dictate the execution path. Each step must complete before the next. This makes accuracy transparent and errors surface early rather than compounding through multiple logical steps.",
        "timing": "3-4 minutes",
        "discussion": "Ask: When have you gotten a half-baked answer from an AI? Often it's because the model took shortcuts. CoT prevents this by enforcing the path you need.",
        "context": "CoT is essential for QA workflows, code reviews, and complex diagnostics (5+ steps). Simple tasks don't need it; multi-step operations require it for production reliability. See Lesson 8 for QA workflow examples.",
        "transition": "CoT gives you control. Structure gives the model clarity. Let's see why format matters."
      }
    },
    {
      "type": "concept",
      "title": "Structure: Information Density",
      "content": [
        "Markdown, JSON, XML are information-dense",
        "Headings make requirements scannable",
        "Lists organize alternatives clearly",
        "Code blocks contain complexity visually"
      ],
      "speakerNotes": {
        "talkingPoints": "Different formats have different information density. Markdown is highly information-dense: headings, lists, code blocks are well-represented in training data. A markdown spec with clear sections conveys more meaning per token than prose. This matters for both clarity and grounding—well-structured prompts help the model parse intent and respond with matching structure.",
        "timing": "2-3 minutes",
        "discussion": "Compare: A paragraph describing requirements vs a markdown spec with sections like '## What to Build,' '## How to Test,' '## What to Avoid.' Which one does the model likely complete better?",
        "context": "In production, structured prompts reduce ambiguity. The model responds with matching structure because it's completing the pattern you started.",
        "transition": "We've covered the right way to prompt. Now let's look at common failure modes to avoid."
      }
    },
    {
      "type": "codeComparison",
      "title": "Avoid Negation: State Positives",
      "leftCode": {
        "label": "Risky Negation",
        "language": "text",
        "code": "Do NOT store passwords in\nplain text. Validate user input.\nDo NOT use eval()."
      },
      "rightCode": {
        "label": "Explicit Positive",
        "language": "text",
        "code": "Do NOT store passwords in\nplain text.\nInstead, hash passwords with bcrypt\nusing 10+ salt rounds.\nValidate all user input with\nwhitelist rules.\nNever use eval()—use JSON.parse()\ninstead."
      },
      "speakerNotes": {
        "talkingPoints": "LLMs struggle with negation because attention mechanisms treat 'NOT' as just another token. When attention is distributed across 'passwords,' 'plain text,' and 'NOT,' the focus lands on the concepts rather than the negation. This is affirmation bias—the model leans toward positive selection. Solve this by stating what you want explicitly after negation.",
        "timing": "3-4 minutes",
        "discussion": "Share a concrete example: 'Don't use hardcoded credentials' often generates code with hardcoded credentials because the model focuses on 'hardcoded' and 'credentials' while missing 'Don't.' The right pattern: negate, then provide the positive alternative with implementation details.",
        "context": "Security failures often stem from negation being missed. In production, you must explicitly state the correct approach after negation.",
        "transition": "Negation is a linguistic limitation. Now let's talk about a fundamental mathematical limitation."
      }
    },
    {
      "type": "codeComparison",
      "title": "Avoid Math: Use Code Instead",
      "leftCode": {
        "label": "Don't Ask for Math",
        "language": "text",
        "code": "Calculate optimal cache size for\n1M users with 2KB objects and\n80% hit ratio"
      },
      "rightCode": {
        "label": "Ask for Code",
        "language": "text",
        "code": "Write a function that calculates\noptimal cache size.\nInputs: userCount, objectSize,\nhitRatio\nOutputs: cacheSizeInMB\nUse BigInt for accuracy\nConsider memory constraints"
      },
      "speakerNotes": {
        "talkingPoints": "LLMs are probabilistic text predictors, not calculators. They'll generate plausible-sounding numbers that may be completely wrong. Don't ask for calculations—ask for code that performs calculations. This delegates the math to deterministic execution rather than token prediction.",
        "timing": "2-3 minutes",
        "discussion": "Give a concrete example: Ask an AI for '5 + 3' and it usually gets it right. Ask for a complex calculation like cache sizing and it often hallucinates. The model is predicting tokens, not computing.",
        "context": "In production, this is critical. Never trust AI-generated numbers for calculations. Always ask for code that computes the values deterministically.",
        "transition": "We've covered what to do and what to avoid. Let's summarize the core principles."
      }
    },
    {
      "type": "takeaway",
      "title": "Key Takeaways",
      "content": [
        "Prompting is pattern completion—draw the beginning, let model complete",
        "Specificity constrains completion space—constraints eliminate guessing",
        "Structure and vocabulary are control surfaces—use them intentionally",
        "CoT gives you control over execution paths—essential for multi-step tasks",
        "Avoid negation and math—state positives and ask for deterministic code"
      ],
      "speakerNotes": {
        "talkingPoints": "These principles scale across all prompting scenarios: pair programming, code reviews, complex analysis. Effective prompting is precision engineering. You're not having a conversation—you're initializing a pattern completion engine with maximum specificity and structure.",
        "timing": "3 minutes",
        "discussion": "Ask students: Which principle will change your prompting most? Most will say specificity or constraints. Point out that all five work together—structure enables specificity, CoT enforces precision, vocabulary guides retrieval.",
        "context": "In production, these principles compound. A vague prompt with no CoT wastes 5+ iterations and hours of engineering time. A precise prompt with CoT and constraints often works first try.",
        "transition": "Next lesson: Grounding. We'll talk about how to feed the right context to ground the model's predictions in your actual codebase and domain."
      }
    }
  ]
}
