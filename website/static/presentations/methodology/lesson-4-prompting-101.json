{
  "metadata": {
    "title": "Lesson 4: Prompting 101",
    "lessonId": "lesson-4-prompting-101",
    "estimatedDuration": "35-45 minutes",
    "learningObjectives": [
      "Understand prompting as pattern completion, not conversation",
      "Master clear instruction-based prompting with imperatives and specificity",
      "Apply Chain-of-Thought for multi-step tasks",
      "Use structure (Markdown/JSON) to direct model attention",
      "Recognize and avoid common failure modes (negation, math)"
    ]
  },
  "slides": [
    {
      "type": "title",
      "title": "Lesson 4: Prompting 101",
      "subtitle": "Precision Engineering, Not Conversation",
      "content": [],
      "speakerNotes": {
        "talkingPoints": "Welcome to Prompting 101. Today we're fundamentally reframing how you think about AI coding assistants. They're not conversational partners—they're sophisticated pattern completion engines. Understanding this distinction is the foundation of effective prompting in production environments.",
        "timing": "1 minute",
        "discussion": "Before we start, think about your last interaction with an AI coding assistant. Did you frame it as a conversation or as initializing a pattern completion engine?",
        "context": "This lesson targets senior engineers who have experimented with AI assistants but may still use conversational framing. Moving from 'chat' to 'precision pattern initiation' requires a mental model shift.",
        "transition": "Let's start with the core concept: what prompting actually is."
      }
    },
    {
      "type": "concept",
      "title": "Prompting as Pattern Completion",
      "content": [
        "Prompts initialize pattern completion, not requests",
        "Model predicts what naturally follows based on training data",
        "More specific pattern start = more constrained completion",
        "Tokens matter: every word affects statistical prediction",
        "Skip pleasantries—'please' and 'thank you' dilute signal"
      ],
      "speakerNotes": {
        "talkingPoints": "Think of prompting like drawing the beginning of a pattern. You're not asking a question or having a conversation. You're starting a sequence the model will statistically complete. The model's job is to predict what comes next based on similar patterns it learned during training. This is fundamentally different from conversational interaction—there's no understanding happening, just pattern completion.",
        "timing": "3-4 minutes",
        "discussion": "Why does framing prompting as pattern completion matter? How does this change how you structure your prompts compared to natural conversation?",
        "context": "In production, this shift means removing conversational overhead. Teams that transition from chat-based prompting to precision pattern initiation see 30-40% fewer iterations because they're not competing with token budget for politeness.",
        "transition": "Now let's look at concrete techniques. We'll start with imperative commands—the foundation of clear pattern initiation."
      }
    },
    {
      "type": "comparison",
      "title": "Imperative Commands: Ineffective vs Effective",
      "left": {
        "label": "Ineffective",
        "content": [
          "Would you please write a validation function?",
          "Thanks, could you add some checks?",
          "Let me know what you think about this code",
          "Conversational phrasing wastes tokens on pleasantries"
        ]
      },
      "right": {
        "label": "Effective",
        "content": [
          "Write a TypeScript function validating email addresses",
          "Validate email format (RFC 5322), length (max 254), and DNS records",
          "Return { valid: boolean, error?: string }",
          "Direct, action-oriented language draws precise pattern"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "The left side shows conversational framing—you're asking for help. The right side shows pattern initiation—you're drawing the beginning of a code pattern the model will complete. Notice the effective version includes type signature, validation rules, and return type. You're not asking a question; you're starting a TypeScript pattern the model recognizes from its training data.",
        "timing": "3-4 minutes",
        "discussion": "What's different about the effective prompt structure? How does each element constrain the completion space?",
        "context": "Production teams report that switching from 'Hey, could you...' to direct imperatives reduces iteration cycles from 3-5 attempts to 1-2. The model completes a specific pattern instead of guessing what you want.",
        "transition": "Action verbs are critical here. Let's look at how specific verbs drive precision."
      }
    },
    {
      "type": "concept",
      "title": "Action Verbs: Driving Specificity",
      "content": [
        "'Write' vs 'Make': More precise verb = tighter pattern match",
        "'Debug the null pointer exception in UserService.ts:47' vs 'Fix the bug': Exact problem location and type",
        "'Add JSDoc comments to exported functions in auth.ts' vs 'Update the docs': Specific scope and format",
        "'Optimize query to use indexed columns' vs 'Improve performance': Concrete technical constraint",
        "Each verb activates different pattern clusters in the model's statistical space"
      ],
      "speakerNotes": {
        "talkingPoints": "Weak verbs like 'make,' 'fix,' 'update,' and 'improve' are vague. Strong verbs like 'write,' 'debug,' 'add,' and 'optimize' are specific. When you use a strong verb with context (like 'Debug the null pointer exception in UserService.ts:47'), you're not just asking for help—you're starting a precise debug pattern. The model's attention mechanisms activate different pattern clusters based on the verb you choose.",
        "timing": "2-3 minutes",
        "discussion": "Why does verb choice matter for pattern completion? What verbs do you naturally use when prompting? How could you make them more specific?",
        "context": "In code review, this principle extends to all communication. Vague feedback ('improve this') triggers generic patterns. Specific feedback ('reduce cyclomatic complexity from 8 to 4 using guard clauses') triggers precise architectural patterns.",
        "transition": "Specificity compounds—add constraints and the completion space narrows even further."
      }
    },
    {
      "type": "concept",
      "title": "Constraints as Guardrails",
      "content": [
        "Without constraints, model fills gaps with assumptions",
        "Define boundaries explicitly: authentication type, endpoints, protocols",
        "Example: 'Add authentication' → ambiguous (JWT? OAuth? Sessions?)",
        "Constrained: 'Add JWT authentication with HS256, 15min expiry, refresh tokens'",
        "Constraints map directly to test cases—validation becomes measurable"
      ],
      "speakerNotes": {
        "talkingPoints": "Unconstrained prompts force the model to guess. 'Add authentication' could mean JWT, OAuth, session tokens, Kerberos—completely different implementations. When you say 'Add JWT authentication with HS256, 15-minute expiry, and refresh token rotation,' the completion space collapses to one clear pattern. The model doesn't need to decide—you've decided. This also means your constraints become your test specifications.",
        "timing": "2-3 minutes",
        "discussion": "What happens in production when you don't specify constraints? Have you seen authentication implementations that didn't match your actual requirements?",
        "context": "This is critical for agent workflows. Constrained prompts reduce hallucination rates from 35-40% down to 5-10% because the model doesn't need to invent details.",
        "transition": "Constraints work well for technical specifications. Now let's talk about another technique: personas, which bias vocabulary rather than behavior."
      }
    },
    {
      "type": "code",
      "title": "Personas: Biasing Vocabulary, Not Capability",
      "language": "markdown",
      "code": "# Generic Approach\nAnalyze this authentication code for issues.\n\n# Persona-Driven Approach\nYou are a security engineer. Analyze this authentication code for:\n- Threat model: external adversaries with zero network access\n- Attack surface: user input validation, token storage, session management\n- Vulnerabilities using OWASP Top 10 framework\n- Mitigations with implementation details",
      "caption": "Personas shift vocabulary distribution—security-specific terms like 'threat model' and 'attack surface' become statistically probable, triggering different knowledge retrieval patterns",
      "speakerNotes": {
        "talkingPoints": "Personas don't add new knowledge to the model. They change which knowledge gets retrieved. 'You are a security engineer' increases the probability of security vocabulary—terms like 'threat model,' 'attack surface,' 'least privilege,' 'timing attacks'—appearing in the response. These words act as semantic queries, retrieving different training patterns than generic terms like 'check for issues.' The persona is a vocabulary shortcut: instead of listing every security term explicitly, you activate the cluster associated with 'security engineer.'",
        "timing": "3-4 minutes",
        "discussion": "When would you use a persona? When might adding persona context actually waste tokens without improving results? In what domains is vocabulary bias most valuable?",
        "context": "This principle scales to all retrieval systems: ChunkHound (codebase search), ArguSeek (web research), vector databases. 'Authentication middleware patterns' retrieves different results than 'login code.' 'Rate limiting algorithms' finds different research than 'slow down requests.' Vocabulary is your retrieval control interface.",
        "transition": "Personas handle vocabulary. For complex multi-step tasks, we need a different technique: Chain-of-Thought."
      }
    },
    {
      "type": "codeExecution",
      "title": "Chain-of-Thought: Controlling Execution Path",
      "steps": [
        {
          "line": "Engineer specifies: 'Refactor UserService to separate concerns'",
          "highlightType": "human",
          "annotation": "Explicit task with desired outcome"
        },
        {
          "line": "CoT Step 1: Read existing UserService implementation",
          "highlightType": "execution",
          "annotation": "Model predicts first action: gather context"
        },
        {
          "line": "LLM analyzes: 'I see authentication, database queries, and validation mixed together'",
          "highlightType": "prediction",
          "annotation": "Analysis of current state"
        },
        {
          "line": "CoT Step 2: Extract authentication logic into AuthService",
          "highlightType": "execution",
          "annotation": "First refactoring step with specific target"
        },
        {
          "line": "CoT Step 3: Move database queries to DataService",
          "highlightType": "execution",
          "annotation": "Second refactoring step"
        },
        {
          "line": "CoT Step 4: Create ValidationService for validation rules",
          "highlightType": "execution",
          "annotation": "Third refactoring step"
        },
        {
          "line": "LLM generates implementation for each step",
          "highlightType": "prediction",
          "annotation": "Code generation follows predetermined path"
        },
        {
          "line": "Validation: Run tests after each step, verify separation",
          "highlightType": "feedback",
          "annotation": "Checkpoint ensures quality at each stage"
        }
      ],
      "speakerNotes": {
        "talkingPoints": "Chain-of-Thought dictates execution sequence. You're not asking the model to figure out the best approach—you're telling it exactly which steps to take in order. This gives you control over complex tasks and makes reasoning transparent. Without CoT, a refactoring task might skip steps or take unexpected shortcuts. With CoT, you control the path, catch errors early, and ensure accuracy.",
        "timing": "4-5 minutes",
        "discussion": "When do you need CoT? What happens if you skip it for a 7-step task? How does explicit step guidance improve error detection?",
        "context": "Production QA workflows rely heavily on CoT. Instead of 'write good tests,' you specify: 'Step 1: Identify happy paths. Step 2: Identify edge cases. Step 3: Write assertions for each. Step 4: Verify coverage.' This deterministic path prevents skipped test cases.",
        "transition": "CoT works for complex sequences. Now let's talk about structure—how formatting directs attention."
      }
    },
    {
      "type": "concept",
      "title": "Structure: Information Density & Attention",
      "content": [
        "Information density: how much meaning per token (Markdown >> prose)",
        "Markdown headings, lists, code blocks parse efficiently",
        "Structured prompts help model parse intent and respond with matching format",
        "JSON for machine parsing; Markdown for hierarchical organization",
        "Well-structured prompts improve token efficiency and grounding accuracy"
      ],
      "speakerNotes": {
        "talkingPoints": "Format matters for attention. Markdown is information-dense—headings, lists, code blocks provide semantic structure with minimal overhead. When you write 'Create an API endpoint with authentication, validation, and error handling,' you're forcing the model to infer structure. When you write: # API Endpoint - Authentication: JWT with HS256 - Validation: request body schema - Error handling: consistent HTTP status codes—you're telling the model exactly how to structure its response. The model's attention mechanisms work better with explicit structure.",
        "timing": "2-3 minutes",
        "discussion": "How does prose-based prompting differ from structured prompting? What formats work best for different tasks?",
        "context": "In production, structured prompts reduce parsing errors and improve consistency. Markdown structure is particularly effective because it's well-represented in training data—models see thousands of READMEs and documentation files with clear hierarchies.",
        "transition": "Structure helps the model succeed. Now let's look at failure modes—things that reliably break prompts."
      }
    },
    {
      "type": "comparison",
      "title": "Negation: A Dangerous Failure Mode",
      "left": {
        "label": "Risky Negation",
        "content": [
          "Do NOT store passwords in plain text",
          "Do NOT use weak hashing algorithms",
          "Avoid hardcoding secrets",
          "LLMs miss negation due to attention bias—model focuses on the concepts mentioned, not the NOT"
        ]
      },
      "right": {
        "label": "Explicit Positive Alternative",
        "content": [
          "Do NOT store passwords in plain text. Instead, always hash with bcrypt (cost factor 12, random salt)",
          "Do NOT use weak algorithms like MD5. Instead, use bcrypt or Argon2 for password hashing",
          "Always use environment variables and secret management (Vault, AWS Secrets Manager)",
          "Explicit negation + positive opposite prevents hallucination"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "LLMs struggle with pure negation. Attention mechanisms treat 'NOT' as just another token competing for weight. When attention is low on the negation token, the model focuses on the concepts mentioned ('passwords,' 'plain text') rather than their negation. This is called affirmation bias. The token generation fundamentally leans toward positive selection (what to include) rather than negative exclusion (what to avoid). The right approach: explicit negation first ('Do NOT store passwords in plain text'), then immediately provide the positive opposite ('Instead, always hash with bcrypt'). This pattern works because you're stating the constraint twice—once as prohibition, once as positive requirement.",
        "timing": "3-4 minutes",
        "discussion": "Have you seen this failure mode in practice? What happened when you relied on pure negation? How did explicit positive alternatives change the result?",
        "context": "In code reviews, this principle applies universally. 'Don't use magic numbers' is weak. 'Don't use magic numbers; instead, define named constants with semantic meaning' is strong. Pure negation invites hallucination; positive alternatives ground the model.",
        "transition": "Negation is one failure mode. Math is another—and it's fundamental."
      }
    },
    {
      "type": "comparison",
      "title": "Math: When LLMs Fail Predictably",
      "left": {
        "label": "LLM Math (Unreliable)",
        "content": [
          "Calculate optimal cache size: 1M users × 100KB avg × 3x redundancy",
          "LLMs are probabilistic text predictors, not calculators",
          "Model generates plausible-sounding numbers that are often completely wrong",
          "Don't rely on LLMs for arithmetic—they hallucinate calculations"
        ]
      },
      "right": {
        "label": "Write Code to Do Math",
        "content": [
          "Write TypeScript to calculate optimal cache size",
          "Input: users, avg_data_size_kb, redundancy_factor",
          "Output: total_cache_mb with human-readable breakdown",
          "Model writes executable code that produces correct results"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "This is critical: LLMs are terrible at arithmetic because they're text predictors, not calculators. They generate numbers that sound reasonable but are statistically likely to be wrong. Don't ask the model to calculate latency percentiles, memory requirements, or cost projections. Instead, have the model write code—TypeScript, Python, SQL—that does the math. The code is deterministic. Numbers are precise. You get verifiable results instead of confident hallucinations.",
        "timing": "2-3 minutes",
        "discussion": "When have you seen LLMs generate plausible-sounding but incorrect numbers? Why is 'write code to do this calculation' safer than 'calculate this'?",
        "context": "Production systems fail because teams ask LLMs to calculate cache sizing, memory requirements, or SQL query costs and trust the responses. Always shift to 'write code that calculates this'—it's more tokens, but it's correct.",
        "transition": "We've covered core techniques, failure modes, and guardrails. Let's tie this together with key takeaways."
      }
    },
    {
      "type": "takeaway",
      "title": "Key Takeaways: Precision Engineering",
      "content": [
        "Prompting is pattern completion, not conversation—initialize specific patterns, not requests",
        "Clear instructions: imperatives + specificity + constraints eliminate ambiguity",
        "Personas affect vocabulary, not capability—use them to bias semantic retrieval",
        "Chain-of-Thought dictates execution path for multi-step tasks; essential for accuracy",
        "Structure (Markdown/JSON) directs attention and improves token efficiency",
        "Avoid negation; state positive alternatives explicitly",
        "Never rely on LLMs for math—write code that calculates instead"
      ],
      "speakerNotes": {
        "talkingPoints": "Effective prompting is precision engineering. You're not having a conversation—you're initializing a pattern completion engine. Every technique we covered today serves one goal: reducing ambiguity so the model completes the exact pattern you need. Specificity, constraints, structure, explicit positive alternatives—these aren't optional flourishes. They're the difference between one-shot success and five-iteration failure.",
        "timing": "2-3 minutes",
        "discussion": "Which of these techniques will you apply first in your work? What patterns have you been using that contradict what we covered today?",
        "context": "Teams that adopt these principles see dramatic improvements: iteration cycles drop from 5+ attempts to 1-2, code quality improves because ambiguity is eliminated, and AI-assisted workflows become predictable and reliable.",
        "transition": "Next lesson: Lesson 5 covers grounding—how to connect prompts to your codebase and external knowledge. Prompting precision + grounding + feedback loops form the foundation of production AI-assisted development."
      }
    }
  ]
}