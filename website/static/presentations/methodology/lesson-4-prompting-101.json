{
  "metadata": {
    "title": "Lesson 4: Prompting 101",
    "lessonId": "lesson-4-prompting-101",
    "estimatedDuration": "35-45 minutes",
    "learningObjectives": [
      "Prompting is pattern completion",
      "Structure directs attention and clarity",
      "Personas bias vocabulary retrieval",
      "Avoid negation and math errors"
    ]
  },
  "slides": [
    {
      "type": "title",
      "title": "Prompting 101",
      "subtitle": "Pattern Completion, Not Conversation",
      "content": [],
      "speakerNotes": {
        "talkingPoints": "Welcome to Lesson 4. We're going to fundamentally shift how you think about prompting. Most engineers treat AI like a conversational partner. That's wrong. Understanding prompting as pattern completion changes everything about how you structure your instructions.",
        "timing": "1 minute",
        "discussion": "Ask: What's your mental model for how prompts work? Do you think about it as conversation or something else?",
        "context": "This lesson is the foundation for effective AI-assisted development. Everything that follows—grounding, agent workflows, QA strategies—builds on these principles.",
        "transition": "Let's start with the fundamental mental model."
      }
    },
    {
      "type": "concept",
      "title": "Prompting as Pattern Completion",
      "content": [
        "You're not asking a question—you're starting a sequence",
        "Model predicts what naturally follows based on training patterns",
        "More specific pattern start = more constrained completions",
        "Tokens aren't requests; they're sequence initialization"
      ],
      "speakerNotes": {
        "talkingPoints": "Think of a prompt as drawing the beginning of a pattern. When you write 'Write a TypeScript function that validates...', you're initializing a sequence the model will statistically complete. The model has seen millions of similar patterns during training—your job is to draw a clear enough beginning that it completes the pattern correctly. This is fundamentally different from conversation.",
        "timing": "3 minutes",
        "discussion": "Compare: 'Can you help me write a function?' vs 'Write a TypeScript function'. Which one draws a clearer pattern start?",
        "context": "Production impact: Vague patterns lead to 5+ iteration cycles. Clear patterns reduce this to 1-2.",
        "transition": "Let's see what clear pattern initialization looks like in practice."
      }
    },
    {
      "type": "codeComparison",
      "title": "Imperative Commands: Pattern Start Matters",
      "leftCode": {
        "label": "Weak Pattern Start",
        "language": "text",
        "code": "Could you help me write a function to validate\n  email addresses?\nThanks in advance!"
      },
      "rightCode": {
        "label": "Strong Pattern Start",
        "language": "text",
        "code": "Write a TypeScript function that\n  validates email addresses per RFC 5322.\nHandle edge cases:\n- Multiple @ symbols (invalid)\n- Missing domain (invalid)\n- Plus addressing (valid)\nReturn { valid: boolean, reason?: string }"
      },
      "speakerNotes": {
        "talkingPoints": "The left side asks conversationally. The model has to guess: What framework? What validation rules? What should it return? The right side draws a complete pattern—language, standard, edge cases, return type. Notice what's absent: 'please' and 'thank you'. These tokens consume precious context without adding clarity.",
        "timing": "3-4 minutes",
        "discussion": "Which starting pattern is less ambiguous? Why does specificity reduce iterations? Have students identify what's missing from the weak pattern.",
        "context": "In production, the weak pattern returns generic validation. The strong pattern returns code matching your requirements immediately.",
        "transition": "Action verbs are particularly powerful for establishing clear patterns."
      }
    },
    {
      "type": "comparison",
      "title": "Action Verbs: Weak vs Strong",
      "left": {
        "label": "Weak Verbs",
        "content": [
          "Make a function",
          "Fix the bug",
          "Update the docs",
          "Improve performance"
        ]
      },
      "right": {
        "label": "Strong Verbs",
        "content": [
          "Write a TypeScript function",
          "Debug null pointer in UserService.ts:47",
          "Add JSDoc to exported functions",
          "Optimize query for indexed columns"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "Strong verbs establish concrete patterns. 'Write' signals a code pattern start. 'Debug' with a file:line signals a specific context. Generic verbs like 'fix' or 'improve' require the model to fill in too many gaps. Each gap is an opportunity for the model to guess wrong.",
        "timing": "2-3 minutes",
        "discussion": "Which prompt would you rather receive as an engineer? Why are specificity and location (file:line) so powerful for debugging?",
        "context": "File:line references are the difference between 'figure out where the problem is' and 'fix exactly this problem'.",
        "transition": "Now let's look at how personas work—and when to use them."
      }
    },
    {
      "type": "concept",
      "title": "Personas: Vocabulary Control, Not Knowledge",
      "content": [
        "Personas bias vocabulary distribution toward domain terms",
        "Use when domain-specific terminology matters",
        "Skip when task is straightforward",
        "Persona is a semantic shortcut—shifts which patterns are retrieved"
      ],
      "speakerNotes": {
        "talkingPoints": "A persona doesn't give the model new knowledge. It changes which knowledge gets retrieved. Writing 'You are a security engineer' increases probability of security-specific terms like 'threat model' and 'attack surface' appearing. These terms act as semantic queries during attention, pulling different training patterns than generic words. The persona is a vocabulary shortcut instead of listing every security term explicitly.",
        "timing": "3-4 minutes",
        "discussion": "Why doesn't a persona add knowledge if it shifts vocabulary? What's the practical difference between generic and domain-focused prompts?",
        "context": "This principle applies everywhere: ChunkHound searches, ArguSeek research, vector databases, agent instructions. Choose vocabulary that retrieves the patterns you need.",
        "transition": "Let's see this principle in action with a security example."
      }
    },
    {
      "type": "codeComparison",
      "title": "Persona Example: Generic vs Security-Focused",
      "leftCode": {
        "label": "Generic Prompt",
        "language": "text",
        "code": "Review this authentication code for issues."
      },
      "rightCode": {
        "label": "Security-Focused Persona",
        "language": "text",
        "code": "You are a security engineer.\n  Review this authentication code.\nIdentify:\n- Threat model violations\n- Attack surface expansion\n- Privilege escalation paths\n- Cryptographic weaknesses"
      },
      "speakerNotes": {
        "talkingPoints": "The generic prompt gets generic advice like 'check for proper validation'. The security-focused prompt shifts vocabulary toward threat modeling and attack analysis. Notice the persona doesn't teach security—it retrieves security-specific patterns from training data using domain vocabulary as a semantic query.",
        "timing": "2-3 minutes",
        "discussion": "What vocabulary difference do you expect between these two responses? Why is 'threat model' more precise than 'security issues'?",
        "context": "Production security reviews depend on this. Generic reviews miss attack vectors that domain-focused vocabularies naturally surface.",
        "transition": "When complexity increases, you need more than just personas—you need Chain-of-Thought."
      }
    },
    {
      "type": "concept",
      "title": "Chain-of-Thought: Paving the Execution Path",
      "content": [
        "Explicit step-by-step instructions control the route",
        "Model must complete each step before proceeding",
        "Critical for multi-step operations (5+ steps)",
        "Validation surfaces errors early, not at the end"
      ],
      "speakerNotes": {
        "talkingPoints": "Chain-of-Thought (CoT) doesn't ask the model to show its thinking—it dictates the path. You're giving turn-by-turn directions instead of just the destination. This is essential for complex operations where you need deterministic execution and can't tolerate shortcutting or step-skipping.",
        "timing": "3-4 minutes",
        "discussion": "Why does defining each step give you control? What happens if the model can take shortcuts or reorder steps?",
        "context": "QA workflows (Lesson 8) rely heavily on CoT to methodically execute test strategies. Multi-step refactoring also requires this.",
        "transition": "Let's see the difference between CoT and unguided execution."
      }
    },
    {
      "type": "codeComparison",
      "title": "Chain-of-Thought: Without vs With",
      "leftCode": {
        "label": "Without CoT (Unguided)",
        "language": "text",
        "code": "Refactor this class to use dependency injection."
      },
      "rightCode": {
        "label": "With CoT (Guided Steps)",
        "language": "text",
        "code": "Refactor this class to use dependency injection.\nFollow these steps in order:\n1. Identify all external dependencies\n2. Create constructor parameters for each\n3. Update all instantiation calls\n4. Verify tests pass\n5. Remove hardcoded imports"
      },
      "speakerNotes": {
        "talkingPoints": "Without CoT, the model might refactor partially, skip testing, or leave hardcoded imports. With CoT, you control the sequence: identify → parameterize → update → verify → clean. Each step must complete before the next, and errors surface early at validation rather than downstream.",
        "timing": "3-4 minutes",
        "discussion": "What could go wrong if the model reorders these steps? Why is 'verify tests pass' critical to place where it is?",
        "context": "In production, unguided refactoring leaves dead code or introduces subtle bugs. Explicit steps catch these.",
        "transition": "How you structure the prompt itself matters too—information density affects clarity."
      }
    },
    {
      "type": "concept",
      "title": "Structure Directs Attention",
      "content": [
        "Markdown, JSON, XML are information-dense formats",
        "Well-represented in training data",
        "Structure makes intent scannable and unambiguous",
        "Matching structure directs response format"
      ],
      "speakerNotes": {
        "talkingPoints": "Structure isn't decoration—it's a control interface. Markdown headings, lists, and code blocks are information-dense: they convey meaning with minimal overhead. These formats are well-represented in training data, so models recognize and respect structure naturally. When you structure your prompt clearly, you also constrain the response structure.",
        "timing": "2-3 minutes",
        "discussion": "Why is a list more scannable than a paragraph? How does structure help the model understand your intent?",
        "context": "Well-structured prompts reduce parsing errors and response iterations. Structure is precision engineering.",
        "transition": "Let's look at what to avoid—predictable failure modes."
      }
    },
    {
      "type": "concept",
      "title": "Negation: The Affirmation Bias Problem",
      "content": [
        "Attention mechanisms treat NOT as a weak token",
        "Model focuses on mentioned concepts, ignores negation",
        "Plain text passwords storage: model generates exactly that",
        "Solution: State negation, then provide positive opposite"
      ],
      "speakerNotes": {
        "talkingPoints": "LLMs struggle with negation because attention treats 'NOT' as just another token competing for weight. When attention is low on NOT, the model focuses on the concepts mentioned ('passwords', 'plain text') while the negation is missed. This is called affirmation bias. The model's token generation fundamentally leans toward positive selection (what to include) rather than negative exclusion (what to avoid).",
        "timing": "3-4 minutes",
        "discussion": "Why is negation harder for attention mechanisms than positive statements? Can you think of other examples where affirmation bias causes problems?",
        "context": "Security prompts are particularly vulnerable. 'Don't store passwords in plain text' might still generate plain text storage because the model attended to passwords and plain text.",
        "transition": "Here's how to fix negation vulnerabilities."
      }
    },
    {
      "type": "codeComparison",
      "title": "Negation: Risky vs Better Approach",
      "leftCode": {
        "label": "Risky (Relies on Negation)",
        "language": "text",
        "code": "Do NOT store passwords in plain text.\nDo NOT log sensitive data."
      },
      "rightCode": {
        "label": "Better (Negation + Positive)",
        "language": "text",
        "code": "Do NOT store passwords in plain text.\nInstead,\n  always store passwords as hashed values using bcrypt\n  with 12 salt rounds.\n\nDo NOT log sensitive data.\nInstead, log only non-sensitive metadata:\n  user ID, action timestamp, operation type."
      },
      "speakerNotes": {
        "talkingPoints": "The risky approach relies on attention catching the negation. The better approach states what NOT to do, then immediately provides the positive pattern. The concrete implementation details (bcrypt, 12 salt rounds, what to log instead) reinforce the correct pattern through positive examples rather than betting on negation being detected.",
        "timing": "3-4 minutes",
        "discussion": "Why does the positive opposite help? How does providing concrete implementation (bcrypt, 12 rounds) improve the pattern?",
        "context": "Production security depends on this. Many vulnerabilities trace back to 'don't do X' instructions being missed during attention.",
        "transition": "There's one more predictable failure mode: math."
      }
    },
    {
      "type": "concept",
      "title": "Math: A Critical Limitation",
      "content": [
        "LLMs are text predictors, not calculators",
        "Arithmetic produces plausible-sounding wrong answers",
        "Never ask models to do math directly",
        "Instead: Have models write code that does the math"
      ],
      "speakerNotes": {
        "talkingPoints": "This is straightforward but critical. Models predict plausible tokens. For math, 'plausible' doesn't mean 'correct'—it means statistically likely given patterns in training data. A model might confidently return 6 * 7 = 49 because that's a plausible number even though it's wrong. In production, this causes silently incorrect calculations.",
        "timing": "2 minutes",
        "discussion": "Why does a model generate confident but wrong answers? What makes code different from direct calculation?",
        "context": "Caching calculations, rate limiting algorithms, performance tuning—all require delegating math to code, not the LLM.",
        "transition": "Let's summarize the core principles."
      }
    },
    {
      "type": "takeaway",
      "title": "Key Takeaways",
      "content": [
        "Prompting is pattern completion—draw clear pattern starts, not conversational requests",
        "Structure and specificity direct attention—personas control vocabulary, verbs establish scope",
        "Chain-of-Thought controls execution paths—essential for multi-step operations requiring deterministic ordering",
        "Avoid predictable failures—use positive statements instead of negation, delegate math to code"
      ],
      "speakerNotes": {
        "talkingPoints": "These principles form the foundation for everything that follows. From grounding strategies to agent workflows to QA guardrails, effective prompting is about precision engineering. You're not conversing with the model—you're initializing a pattern completion engine. Be specific, be structured, be explicit.",
        "timing": "2-3 minutes",
        "discussion": "Which of these principles will most change how you write prompts? What's the biggest takeaway for your current work?",
        "context": "Apply these principles to your next agent interaction. Notice how specificity reduces iteration cycles and structure improves accuracy.",
        "transition": "Next lesson: Grounding. We'll learn how to anchor prompts to your codebase and domain knowledge."
      }
    }
  ]
}