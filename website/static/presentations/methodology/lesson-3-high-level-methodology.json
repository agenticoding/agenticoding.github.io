{
  "metadata": {
    "title": "Lesson 3: High-Level Methodology",
    "lessonId": "lesson-3-high-level-methodology",
    "estimatedDuration": "45-60 minutes",
    "learningObjectives": [
      "Shift from craftsman to operator",
      "Ground agents with research",
      "Plan before executing",
      "Validate without reading every line"
    ]
  },
  "slides": [
    {
      "type": "title",
      "title": "Lesson 3: High-Level Methodology",
      "subtitle": "From Craftsman to Operator",
      "content": [],
      "speakerNotes": {
        "talkingPoints": "This lesson is about fundamentally reframing your role as an engineer when working with AI agents. You're no longer writing implementation code—you're orchestrating systems. This isn't a productivity hack; it's a different job entirely. We'll walk through the systematic workflow that makes this shift practical and sustainable.",
        "timing": "1-2 minutes",
        "discussion": "Ask: How many of you feel pressure to read and verify every line of generated code? What do you think that's costing you?",
        "context": "Most senior engineers initially resist this shift because their value has been built on deep code ownership. That resistance is normal—it took me six months to stop feeling like I was 'cheating' by delegating implementation. But the engineers who make this shift earliest are already shipping 5-10x more features than their peers.",
        "transition": "Let's start with the hardest part: the psychological shift required."
      }
    },
    {
      "type": "concept",
      "title": "The Mindset Shift",
      "content": [
        "Craftsman: Own every line, understand deeply",
        "Operator: Own results, direct systems",
        "From syntax to structure",
        "From implementer to orchestrator"
      ],
      "speakerNotes": {
        "talkingPoints": "For 20+ years, your value came from code quality and deep understanding. AI agents force a fundamental shift: you can't understand thousands of lines the way you understood hundreds. This isn't failure—it's recognizing that your value moves up the stack. You shift from writing code to directing systems, from syntax to architecture.",
        "timing": "3-4 minutes",
        "discussion": "Ask students: What does 'owning results' mean differently than 'owning code'? Is your accountability different?",
        "context": "In production environments, I've seen engineers try to maintain the craftsman mindset with agents—they read every generated file, second-guess decisions, rewrite implementations. They burn out or become bottlenecks. The engineers who shift to operator mindset after 2-3 weeks are 10x more productive because they trust the framework, not the individual outputs.",
        "transition": "But letting go doesn't mean losing quality. It means ensuring quality differently."
      }
    },
    {
      "type": "comparison",
      "title": "Developer Workflow Comparison",
      "left": {
        "label": "Traditional",
        "content": [
          "Write implementation code",
          "Read and verify every line",
          "Debug syntax and logic errors",
          "Refactor and improve continuously"
        ]
      },
      "right": {
        "label": "Operator",
        "content": [
          "Understand system architecture",
          "Research patterns and context",
          "Plan architectural decisions",
          "Direct and validate selectively"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "Notice what disappears from the operator workflow: writing implementation code, reading every line, debugging syntax. The agent handles those. Your cognitive load shifts entirely to architectural thinking—understanding how pieces fit, what patterns guide them, what constraints matter.",
        "timing": "2-3 minutes",
        "discussion": "Which skills from the traditional workflow do you keep? Which are genuinely gone?",
        "context": "In practice, most senior engineers spend about 80% of their time thinking about architecture, data flows, and integration points. They spend 20% writing code. With agents, you're optimizing what you're already doing—outsourcing the 20% and amplifying the 80%.",
        "transition": "This shift is sustainable because properly-prompted agent code is often easier to read than hand-written code. You're not sacrificing quality—you're achieving consistency at scale."
      }
    },
    {
      "type": "concept",
      "title": "Why Agent Code Can Be Better",
      "content": [
        "LLMs follow patterns with mechanical precision",
        "Thousands of lines maintain consistency perfectly",
        "Your patterns are correct = output is correct",
        "Structural consistency > individual craftsmenship",
        "Your responsibility: validate patterns, not details"
      ],
      "speakerNotes": {
        "talkingPoints": "This is the counterintuitive insight that changes everything: AI-generated code, when properly prompted, is often easier to read than hand-written code. Not because AI is smarter, but because it never gets tired, never shortcuts, never says 'this function is complex but it works.' It replicates your patterns perfectly across thousands of lines.",
        "timing": "2 minutes",
        "discussion": "Have you seen hand-written code with inconsistent patterns? How much time does that waste when other engineers have to learn multiple conventions?",
        "context": "I've reviewed thousands of lines of Claude-generated code compared to hand-written from the same codebase. The generated code consistently has fewer pattern violations, fewer edge case bugs, and fewer 'clever' solutions that later confuse maintainers.",
        "transition": "But this only works if the patterns themselves are right. That's where the four-phase workflow comes in."
      }
    },
    {
      "type": "visual",
      "title": "The Four-Phase Workflow",
      "component": "WorkflowCircle",
      "caption": "Research, Plan, Execute, Validate—cycle until complete.",
      "speakerNotes": {
        "talkingPoints": "Every significant agent interaction should follow this pattern. Each phase has a distinct purpose. Skip any one, and your failure rate jumps dramatically. This isn't theory—this is the framework that separates 'agents are cool but unreliable' from 'agents are production-grade tools.'",
        "timing": "1 minute (introduction), 12-15 minutes (detailed walkthrough across next 4 slides)",
        "discussion": "Which phase do you think engineers most commonly skip?",
        "context": "The workflow is iterative. Validation often reveals gaps in research or planning. That's expected and normal. The value is having a systematic framework that catches issues before they compound.",
        "transition": "Let's walk through each phase. We'll start with Research—grounding your agent in context it needs to operate."
      }
    },
    {
      "type": "concept",
      "title": "Phase 1: Research (Grounding)",
      "content": [
        "Bridge general knowledge with real context",
        "Without grounding: hallucination and inconsistency",
        "ChunkHound: semantic code search",
        "ArguSeek: domain knowledge and documentation",
        "Cost: 2-5 minutes | Benefit: 20-30 min saved in rework"
      ],
      "speakerNotes": {
        "talkingPoints": "Grounding is the bridge between the general-purpose knowledge in the model and the specific context your agent needs to operate in. Think of it like reading documentation and Stack Overflow before starting a new project. Without it, agents invent patterns, hallucinate implementations, and miss your existing code. We cover these tools in detail in Lesson 5, but understand the principle: research both your codebase and domain knowledge before planning changes.",
        "timing": "2-3 minutes",
        "discussion": "What patterns have you seen agents invent when they didn't have enough context? How much time was wasted fixing those?",
        "context": "In production, the agents that fail are almost never failing because of reasoning capability—they're failing because of context gaps. A well-grounded agent that knows your patterns and constraints is orders of magnitude more reliable.",
        "transition": "Once you've researched context, you plan. But planning isn't one-size-fits-all."
      }
    },
    {
      "type": "visual",
      "title": "Phase 2: Plan—Two Strategies",
      "component": "PlanningStrategyComparison",
      "caption": "Exploration discovers best approach;\nexact planning executes known solutions.",
      "speakerNotes": {
        "talkingPoints": "Planning is a strategic choice based on whether you already know the solution. Exploration planning is for discovering approaches—you frame the problem, the agent researches alternatives, you iterate together. Exact planning is for known solutions—you're directive, specify patterns, constraints, and criteria. Both are valid; choose based on your architectural certainty.",
        "timing": "2-3 minutes",
        "discussion": "In your experience, when do you typically have architectural certainty vs. needing to explore? What determines that?",
        "context": "Most production work lands in exact planning—you've already designed the system, you just need reliable implementation. Exploration planning is for R&D, experimental features, or unfamiliar domains. Know which mode you're in before you start.",
        "transition": "With your plan defined, you execute. But execution has two modes with very different productivity profiles."
      }
    },
    {
      "type": "comparison",
      "title": "Execution Modes: Supervised vs Autonomous",
      "left": {
        "label": "Supervised Mode",
        "content": [
          "Watch each action in real time",
          "Steer when agent drifts",
          "Maximum control and precision",
          "You're blocked while it works",
          "Throughput: single task only"
        ]
      },
      "right": {
        "label": "Autonomous Mode",
        "content": [
          "Give task, let agent run",
          "You work on other things",
          "Requires excellent planning",
          "Massive parallelism possible",
          "Throughput: 3+ simultaneous projects"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "This is where the real productivity transformation happens. Supervised mode (babysitting) gives maximum control but tanks throughput—you're context-locked to one task. Autonomous mode (autopilot) is where you maintain parallel work, 8-hour productive stretches with minimal keyboard time. The difference isn't speed per task; it's the ability to work on multiple projects simultaneously while living your life.",
        "timing": "3-4 minutes",
        "discussion": "Most engineers start in supervised mode. When do you think you should shift toward autonomous? What has to be true about your planning for that to work?",
        "context": "Real 10x productivity isn't one task finishing 10x faster. It's three tasks running in parallel. You're in meetings, taking breaks, cooking dinner—and three agents are working. That senior engineer ships more code than the same engineer babysitting one agent, even if each individual task takes slightly longer.",
        "transition": "But autonomous mode only works if your grounding and planning are excellent. Garbage in, garbage out. Let's talk about validation—how to know if the output is right."
      }
    },
    {
      "type": "concept",
      "title": "Phase 4: Validate—Core Principle",
      "content": [
        "LLMs almost never produce 100% perfect output",
        "That's expected, not failure",
        "Decision: iterate or regenerate?",
        "Iterate: Foundation right, needs refinement",
        "Regenerate: Fundamental approach is wrong"
      ],
      "speakerNotes": {
        "talkingPoints": "Validation isn't about achieving perfection on first pass. It's about accurately identifying what's wrong and deciding the fastest path to ship. Here's the key insight: it's usually easier to fix your context (the prompt, grounding, constraints) than to fix the generated code. You're debugging your input, not the output.",
        "timing": "2-3 minutes",
        "discussion": "When was the last time you spent 2 hours patching bad code instead of 15 minutes regenerating with better context? What changed your decision-making?",
        "context": "In production, I've found that about 70% of agent iterations are actually context fixes (better examples, clearer constraints) rather than code fixes. This is the shift in thinking: the agent's output is a mirror of your input quality.",
        "transition": "But validation doesn't mean reading every line. You need a faster way to check correctness."
      }
    },
    {
      "type": "concept",
      "title": "Validate Without Reading Every Line",
      "content": [
        "Mental model: Does architecture match plan?",
        "Pattern check: Do implementations follow examples?",
        "Run your code: Test happy path and edges",
        "Use agent to review its own work",
        "Automated checks: Build, tests, linters pass?"
      ],
      "speakerNotes": {
        "talkingPoints": "You don't read every line of generated code—you validate against your mental model. Does the architecture match what you planned? Do the patterns align with your research examples? Does the behavior work as expected? If yes, ship it. If no, decide: is the problem context (regenerate) or refinement (iterate)?",
        "timing": "2-3 minutes",
        "discussion": "What's your mental model of a system you work in? How quickly can you validate whether generated code fits that model?",
        "context": "Five minutes of manual testing reveals more than an hour of code review. Run it, break it, test edge cases. Is performance acceptable? Does error handling work? That's your validation—not reading, running.",
        "transition": "This workflow is iterative. Validation often reveals gaps. That's normal. But it catches issues before they compound in production."
      }
    },
    {
      "type": "concept",
      "title": "The Mental Model Advantage",
      "content": [
        "Build understanding of system relationships",
        "How auth flows through middleware",
        "Where validation vs. logic separation happens",
        "Error propagation to client",
        "Performance bottlenecks and boundaries"
      ],
      "speakerNotes": {
        "talkingPoints": "Your mental model is your blueprint. As you research and plan, you're not memorizing code—you're understanding relationships. How does data flow? Where do decisions happen? What are the boundaries? This mental model is what allows you to validate quickly. When the agent completes, you check: Does this fit my model? If yes, ship it.",
        "timing": "2 minutes",
        "discussion": "Do you actually have a clear mental model of a system you maintain? What would make it clearer?",
        "context": "This is why experienced engineers onboard faster to new systems—they already know what relationships to look for. Apply that same pattern recognition to agent-generated code.",
        "transition": "Now that we've covered the workflow, let's bring it together: what does this mean for how you work day-to-day?"
      }
    },
    {
      "type": "takeaway",
      "title": "Key Takeaways",
      "content": [
        "Your value moves from syntax to structure",
        "Quality comes from context and validation, not code reading",
        "Autonomous mode is where real productivity lives",
        "Mental model validates faster than line-by-line review"
      ],
      "speakerNotes": {
        "talkingPoints": "The operator mindset unlocks everything else in this course. You're not trying to make agents 10% faster. You're fundamentally restructuring how you work to maintain quality while delegating implementation. Grounding, planning, and smart validation let you ship thousands of lines with confidence.",
        "timing": "2 minutes",
        "discussion": "Which takeaway challenges your current workflow most? How would you experiment with the operator mindset this week?",
        "context": "The engineers I know who've made this shift report two things: first, they stop feeling guilty about not writing code; second, they stop being bottlenecks for their teams. They become force multipliers instead of individual contributors.",
        "transition": "Next lesson, we dive into prompting fundamentals—the specific techniques for communicating with agents so they execute your plans reliably. See you then."
      }
    }
  ]
}
