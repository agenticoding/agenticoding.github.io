{
  "metadata": {
    "title": "Lesson 3: High-Level Methodology",
    "lessonId": "lesson-3-high-level-methodology",
    "estimatedDuration": "45-60 minutes",
    "learningObjectives": [
      "Understand the mindset shift from craftsman to operator when working with AI agents",
      "Learn the four-phase workflow: Research, Plan, Execute, Validate",
      "Identify when to use supervised vs autonomous execution modes",
      "Master the iteration decision: when to regenerate vs iterate on generated code"
    ]
  },
  "slides": [
    {
      "type": "title",
      "title": "Lesson 3: High-Level Methodology",
      "subtitle": "From Craftsman to Operator: Systematic Workflows for AI-Powered Development",
      "content": [],
      "speakerNotes": {
        "talkingPoints": "This lesson fundamentally challenges how we approach coding when working with AI agents. For decades, your value came from understanding every line you wrote. AI agents force a psychological shift: you're moving from implementer to orchestrator. We'll cover a systematic workflow that lets you maintain architectural control while delegating implementation, and we'll address the real source of 10x productivity gains.",
        "timing": "2 minutes",
        "discussion": "Ask: How many of you feel uncomfortable not reading every line of code your AI generates? That's the right instinct—but we'll show you how to validate quality differently.",
        "context": "This is the foundation for everything that follows. Senior engineers who master this workflow see 3-5x productivity gains immediately, with much higher gains possible as they build trust and move toward autonomous execution.",
        "transition": "Let's start with the psychological foundation: understanding your new role."
      }
    },
    {
      "type": "concept",
      "title": "The Operator Mindset",
      "content": [
        "You're not doing the same job faster—you're doing a different job",
        "Value moves from syntax to architecture, loops to logic, details to decisions",
        "You ensure quality differently: by validating patterns, not reading every line",
        "Your focus: the context you provide and prompts you craft",
        "Properly grounded AI code is often more readable than hand-written code"
      ],
      "speakerNotes": {
        "talkingPoints": "This is the mental shift. Your entire career, you've been measured on writing clean code and understanding every detail. With AI agents, that becomes your bottleneck. When an agent can generate 2,000 lines of consistent code following your patterns, reading every line becomes impossible and counterproductive. Instead, you shift focus to architecture and validation. You read selectively—when your mental model says something's risky. You validate structurally—does it follow our patterns? Does it handle boundaries? Does it integrate correctly? The counterintuitive truth: LLMs replicate patterns with mechanical precision. When you provide quality patterns, you get structural consistency at scale that individual craftsmanship can't match.",
        "timing": "4-5 minutes",
        "discussion": "Ask: What patterns in your codebase would be tedious to implement 50 times by hand? Those are perfect for agents. Where do inconsistencies creep in across your code? Those are what agents eliminate through strict pattern adherence.",
        "context": "In production, this shift is what separates engineers who see 2x productivity from those who see 10x. It's psychological before it's methodological.",
        "transition": "Now let's see how to systematize this. There's a workflow that makes this mindset practical."
      }
    },
    {
      "type": "concept",
      "title": "Traditional vs Operator Workflow",
      "content": [
        "Traditional: Write → Test → Review → Debug → Refactor",
        "Operator: Understand → Research → Plan → Direct → Validate",
        "What's removed: implementation coding, line-by-line debugging, syntax fixing",
        "What's added: architectural thinking, pattern validation, systematic validation"
      ],
      "speakerNotes": {
        "talkingPoints": "Notice what disappears: the agent writes implementation code. You don't write it. You don't read every line of it during review. You don't debug syntax errors—the agent doesn't make them. What replaces these activities is architectural thinking and pattern validation. You're spending your cognitive load on 'Does this fit our system architecture?' and 'Do the patterns match our codebase?' not 'Is this loop correct?' This is genuinely harder than coding, but it's where your expertise compounds. A great operator architect can direct agents more effectively than average engineers can code by hand.",
        "timing": "3 minutes",
        "discussion": "Ask: Which activities on the traditional list do you spend the most time on? Usually debugging and refactoring. Those disappear. What becomes your bottleneck instead?",
        "context": "This shift is why many engineers initially struggle with AI agents. The job skills don't directly transfer. You're not faster at the same job; you're different at a different job.",
        "transition": "Here's the systematic process that makes this work: the Four-Phase Workflow."
      }
    },
    {
      "type": "visual",
      "title": "The Four-Phase Workflow",
      "component": "WorkflowCircle",
      "caption": "Research → Plan → Execute → Validate. Each phase is distinct, and skipping any one dramatically increases failure rate.",
      "speakerNotes": {
        "talkingPoints": "This is your process. Every significant agent interaction follows this pattern. Research grounds the agent in your codebase and domain knowledge. Planning forces strategic thinking before implementation. Execution has two modes: supervised (for learning and critical code) and autonomous (for scale). Validation determines whether to iterate or regenerate. These aren't optional steps—they're where quality, speed, and reliability come from. A common mistake: engineers skip Phase 1 (research) and wonder why agents hallucinate. Or skip Phase 2 (planning) and expect good results. Or stay in Phase 3 (supervised) forever and never get productivity gains.",
        "timing": "2 minutes",
        "discussion": "Ask: Which phase feels most unfamiliar to your current process? That's where your biggest opportunity for improvement is.",
        "context": "This workflow is specific to AI agents. You wouldn't write hand-coded solutions without understanding the codebase first (Phase 1) or thinking through the approach first (Phase 2). The same principle applies to agents.",
        "transition": "Let's walk through each phase in detail, starting with Research."
      }
    },
    {
      "type": "concept",
      "title": "Phase 1: Research (Grounding)",
      "content": [
        "You wouldn't code in a new codebase without learning its architecture and patterns",
        "Grounding: bridge between general model knowledge and your real-world context",
        "Without grounding: agents hallucinate patterns, invent inconsistent APIs, miss existing implementations",
        "Code Research (ChunkHound): semantic search answering 'How does X work?'",
        "Domain Research (ArguSeek): pull docs, frameworks, algorithms, and best practices"
      ],
      "speakerNotes": {
        "talkingPoints": "Research is about giving your agent the context it needs to operate like a team member who understands your codebase, not a generic code generator. ChunkHound does semantic code research—it answers architectural questions like 'How is authentication handled?' Instead of keyword matching, it retrieves the patterns and implementations from your codebase. ArguSeek pulls domain knowledge from Google and documents directly into context. Think of this as opening the right tabs, reading Stack Overflow threads, pulling API docs, checking best practices—all automatically. Without this phase, agents will generate code that looks reasonable but doesn't match your patterns or integrate correctly.",
        "timing": "4 minutes",
        "discussion": "Ask: When you join a new team, how long before you can write code that matches their patterns and standards? That's how long grounding should take. A few hours of research saves days of rework.",
        "context": "In Lesson 5, we cover these tools in detail. For now, understand the principle: ground before you plan.",
        "transition": "With grounding complete, you're ready to plan. But there are two approaches, depending on whether you know the solution already."
      }
    },
    {
      "type": "comparison",
      "title": "Planning: Exploration vs Exact",
      "left": {
        "label": "Exploration Planning",
        "content": [
          "Solution space is unclear",
          "Frame problem, steer agent to research alternatives",
          "Discover approach together through iteration",
          "Higher cost, better solutions",
          "Catches architectural issues early"
        ]
      },
      "right": {
        "label": "Exact Planning",
        "content": [
          "You know the solution",
          "Be directive: specify task, integration points, constraints",
          "Agent executes predetermined path",
          "Faster and more cost-effective",
          "Requires upfront clarity and certainty"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "This is a strategic choice. Exploration planning is when you're genuinely unsure—maybe you're tackling a new problem, or you need the agent to explore your codebase to find the best approach. You frame the problem and steer the agent through research and iteration. This costs more tokens and takes longer, but it often discovers better solutions. Exact planning is when you've already done the architectural thinking and you know exactly what you need built. You specify the task precisely, define integration points, list constraints and edge cases, and the agent executes. This is faster and cheaper, but if your plan is wrong, the code will be wrong. Most senior engineers use exact planning 70% of the time because they've already done the architectural thinking. You use exploration planning 30% of the time when genuinely exploring new territory.",
        "timing": "3-4 minutes",
        "discussion": "Ask: Think about a feature you're about to build. Do you know exactly how it should be architected, or are you still figuring out the approach? That answer tells you which planning mode to use.",
        "context": "This choice compounds. One wrong architectural decision discovered in Phase 2 saves weeks of rework in Phase 3-4.",
        "transition": "With planning done, you execute. But execution has two very different modes."
      }
    },
    {
      "type": "concept",
      "title": "Phase 3: Execute—Supervised vs Autonomous",
      "content": [
        "Supervised: Watch each action, steer in real-time, catch issues immediately",
        "Cost: You're blocked while the agent works. Attention burned on implementation details",
        "Autonomous: Give task, let it run, check results when done",
        "Benefit: Work on multiple projects. 8-hour output with 2 hours at keyboard",
        "Real 10x gain: Parallel work, not just faster individual tasks"
      ],
      "speakerNotes": {
        "talkingPoints": "Here's where productivity myths get exposed. Most people think 10x comes from the agent being faster. Wrong. It comes from parallel work. In supervised mode, you watch the agent work. You catch issues immediately, you steer in real-time, you have maximum control. But you can only do one task at a time, and you're blocked while the agent works. In autonomous mode, you give the agent a task and walk away. You're cooking dinner, attending a meeting, working on a different project. You check occasionally to see if it's blocked, but mostly you're away. You can have three agents running simultaneously. Over an 8-hour day, you might spend 2 hours actually interacting with agents and 6 hours doing other things, while shipping 3-4x more code than you would have hand-coded. That's where the real productivity transformation happens.",
        "timing": "5 minutes",
        "discussion": "Ask: How many times have you been blocked waiting for code review or a deployment? Autonomous agents eliminate that. You're not blocked anymore. But how many of you would trust that right now? That's honest. Trust comes from mastering Research and Planning phases.",
        "context": "Most senior engineers start in supervised mode to build confidence. After 2-3 weeks of seeing consistent quality, they shift to 50% autonomous. After a month, they're 80% autonomous and never going back.",
        "transition": "The agent finished. Your code is generated. Now you validate."
      }
    },
    {
      "type": "concept",
      "title": "Phase 4: Validate & Iterate",
      "content": [
        "LLMs are probabilistic: almost never 100% perfect on first pass (this is expected)",
        "Validate against your mental model: Does it fit the architecture? Follow patterns? Handle boundaries?",
        "Iterate when: aligned with expectations but has gaps—missing edge cases, tech debt",
        "Regenerate when: fundamental misalignment—wrong architecture, misunderstood requirements",
        "Run your code: manually test happy path, edge cases, error handling"
      ],
      "speakerNotes": {
        "talkingPoints": "Validation isn't about achieving perfection. It's about accurately identifying what's wrong or incomplete, then making a critical decision: do you iterate (fix gaps) or regenerate (fix fundamental issues)? The rule of thumb: it's usually easier to fix your input (the prompt, examples, constraints) than to fix the output (the code). So regenerate when the agent misunderstood your requirements. Iterate when the foundation is right but incomplete. Don't patch fundamentally broken code—that's debugging the output instead of fixing your input. Validation also means running your code. Be the user. Test the happy path, try to break it, check edge cases. Five minutes of actual testing reveals more than an hour of code review.",
        "timing": "4 minutes",
        "discussion": "Ask: When you get code back from a junior engineer that's 'almost right,' what do you do? Usually, you fix it yourself because it's faster. With agents, you should regenerate instead. Why? Because regenerating teaches the agent better than patching teaches a junior.",
        "context": "This is where your mental model of the system becomes invaluable. You're not reviewing line by line. You're checking: 'Does this match how I thought about the problem?'",
        "transition": "Let's look at how these phases come together in a real scenario."
      }
    },
    {
      "type": "code",
      "title": "Mental Model: Your Validation Blueprint",
      "language": "plaintext",
      "code": "How authentication flows through middleware\nWhere data validation happens vs business logic\nHow errors propagate to the client\nWhere performance bottlenecks might appear\nWhat security boundaries exist",
      "caption": "During planning and validation, you're building and refining this mental model. This is what allows you to validate generated code quickly without reading every line.",
      "speakerNotes": {
        "talkingPoints": "Your mental model isn't about memorizing code. It's about understanding relationships and boundaries. How does a request flow through your system? Where does validation happen? How do errors propagate? What are the security boundaries? When you complete Phase 1 (Research), you should have a working mental model of your system. When the agent generates code, you don't read every line. You check: 'Does this match my mental model of how this system works?' If yes, probably correct. If no, either your model is wrong (update it) or the code is wrong (regenerate). This is how you validate at scale without drowning in details.",
        "timing": "2 minutes",
        "discussion": "Ask: Can you draw the authentication flow in your current system? If not, that's a gap. That gap is what causes agents to make mistakes.",
        "context": "Building this mental model is partly why Phase 1 (Research) exists. It's not just grounding the agent—it's grounding you.",
        "transition": "Now let's talk about the biggest mistake engineers make: staying in supervised mode too long."
      }
    },
    {
      "type": "concept",
      "title": "The 10x Productivity Paradigm",
      "content": [
        "Speed per task: agent might take same time or longer than hand-coding",
        "Parallel throughput: three agents running simultaneously while you work on other projects",
        "Continuous output: 8-hour productive output with 2 hours at the keyboard",
        "Real gain: autonomous agents aren't faster at individual tasks—they're force multipliers on your total output",
        "Prerequisite: excellent grounding (Phase 1) and planning (Phase 2)"
      ],
      "speakerNotes": {
        "talkingPoints": "This is the reality check. A lot of engineers expect agents to hand-code faster than they can. Sometimes true, but not the point. The real productivity transformation is parallel work. You have one brain, but you can have five agents running simultaneously, each working on different tasks, none of them asking questions. While you're in a meeting, your agents are shipping code. While you're eating lunch, your agents finished three pull requests. While you're reviewing design documents, your agents are building the implementation. A senior engineer with three autonomous agents running in parallel ships more code than the same engineer hand-coding without agents. This only works if your grounding and planning are excellent. If you skip those phases, agents become liabilities instead of multipliers.",
        "timing": "3-4 minutes",
        "discussion": "Ask: How many tasks are you juggling simultaneously right now? What if each one had an autonomous agent? How much of your time would you spend coding vs thinking?",
        "context": "This is the psychological barrier most senior engineers face. 'I don't trust the agent to work without my watching.' That's a grounding and planning problem, not an agent problem.",
        "transition": "Let's summarize the key principles that make this workflow effective."
      }
    },
    {
      "type": "takeaway",
      "title": "Key Takeaways",
      "content": [
        "The operator mindset: you're validating patterns and architecture, not reading every line",
        "Every agent interaction follows four phases: Research → Plan → Execute → Validate",
        "Your mental model is your validation blueprint—use it to spot-check generated code",
        "Supervised mode is for learning. Autonomous mode is for scale. You should spend 70% of your time in autonomous mode",
        "Regenerate when the approach is wrong. Iterate when the foundation is right but incomplete",
        "The real 10x gain comes from parallel work and continuous output, not faster individual tasks"
      ],
      "speakerNotes": {
        "talkingPoints": "These six principles form the foundation of everything that follows. The operator mindset is psychological—you're shifting from 'Did I write this correctly?' to 'Does this match my architectural vision?' The four-phase workflow is systematic—it catches issues before they compound. Your mental model is your tool—it's what lets you validate at scale. Execution modes are tactical—supervised for learning, autonomous for productivity. Iteration decisions are strategic—regenerate for fundamental issues, iterate for refinement. And the real productivity gain is paradigm-level—it's about doing genuinely different work, not just faster work.",
        "timing": "3 minutes",
        "discussion": "Ask: Which of these feels most different from how you currently work? That's where to focus your effort over the next week.",
        "context": "Everything in Lessons 4-9 builds on these principles. You'll learn specific techniques for prompting, grounding, planning, and validation. But they all depend on internalizing this foundational workflow.",
        "transition": "Next lesson: Prompting 101. We'll learn the specific techniques for crafting prompts that produce reliable results at every phase of this workflow."
      }
    }
  ]
}