{
  "metadata": {
    "title": "Lesson 3: High-Level Methodology",
    "lessonId": "lesson-3-high-level-methodology",
    "estimatedDuration": "35-45 minutes",
    "learningObjectives": [
      "Understand the mindset shift from craftsman to operator",
      "Master the four-phase workflow: Research, Plan, Execute, Validate",
      "Distinguish between supervised and autonomous execution modes",
      "Apply grounding techniques to reduce agent hallucination",
      "Make informed decisions about iteration vs. regeneration"
    ]
  },
  "slides": [
    {
      "type": "title",
      "title": "High-Level Methodology",
      "subtitle": "From Craftsman to Operator: Scaling Your Impact with AI Agents",
      "content": [],
      "speakerNotes": {
        "talkingPoints": "This lesson teaches the systematic workflow that allows you to maintain architectural control while delegating implementation to agents. The core insight: you're not speeding up the work you already do—you're changing what work you do entirely. Your focus shifts from writing code to directing systems.",
        "timing": "1 minute",
        "discussion": "As we go through this lesson, think about where in your current workflow you'd feel comfortable delegating to an agent and where you'd need to stay hands-on.",
        "context": "This is the strategic framework lesson. It sets up everything that follows in Lesson 4 (prompting) and beyond. These phases are non-negotiable—skipping any one of them dramatically increases failure rates.",
        "transition": "Let's start by understanding the fundamental mindset shift required."
      }
    },
    {
      "type": "concept",
      "title": "The Operator Mindset",
      "content": [
        "You can't read and own every line at agent scale",
        "Quality shifts from reading code to thinking systematically",
        "Focus moves from details to architecture and patterns",
        "Your value moves up the stack—syntax to structure to decisions",
        "Your responsibility doesn't change—you still own the results"
      ],
      "speakerNotes": {
        "talkingPoints": "For your entire career, your value has been in the details—clean code, spotting subtle bugs, understanding every line you ship. AI agents force you to operate differently. You can't read 2,000 lines of generated code the way you owned 200 lines you wrote. If you try, you become the bottleneck. Instead, you validate correctness by thinking systematically: Does this fit our architecture? Does it follow our patterns? Does it handle identified risks? Does behavior match my mental model?",
        "timing": "3-4 minutes",
        "discussion": "Ask: What's the hardest part of letting go of code you didn't write? What patterns do you naturally validate first when reviewing code?",
        "context": "This is psychological as much as methodological. Many senior engineers resist this shift because it feels like losing control or quality. In reality, properly grounded and planned agent-generated code often achieves consistency at scale individual craftsmanship can't match. The risk isn't lower quality—it's trusting the wrong implementation because you didn't properly research or plan.",
        "transition": "Let's compare the traditional developer workflow to the operator workflow."
      }
    },
    {
      "type": "comparison",
      "title": "Traditional Developer vs. Operator",
      "left": {
        "label": "Traditional Developer",
        "content": [
          "Write code → Test → Review → Debug → Refactor",
          "Responsible for implementation details",
          "Cognitive load: syntax, logic, patterns",
          "Bottlenecked by personal typing speed and time",
          "Quality validated by reading every line"
        ]
      },
      "right": {
        "label": "Operator",
        "content": [
          "Understand system → Research → Plan → Direct → Validate",
          "Responsible for architecture and guidance",
          "Cognitive load: structure, integration, requirements",
          "Parallelized through multiple simultaneous agent tasks",
          "Quality validated against mental model and patterns"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "The operator workflow removes you from the implementation machinery. You're not writing code, debugging syntax, or reading every line. You're ensuring the system architecture is sound, patterns are consistent, and the agent has clear direction. This isn't faster implementation of the same work—it's a completely different set of responsibilities.",
        "timing": "2 minutes",
        "discussion": "Which responsibilities do you prefer? Where do you feel most confident? Where would you need to build skills?",
        "context": "This workflow works only if you do the previous phases well. Skip grounding and planning, and the agent will hallucinate and drift. Do them properly, and you can confidently ship thousands of lines without reading them all.",
        "transition": "Now let's walk through each phase of this workflow in detail."
      }
    },
    {
      "type": "concept",
      "title": "Phase 1: Research (Grounding)",
      "content": [
        "You wouldn't code in a new codebase without learning architecture first",
        "Grounding bridges general knowledge with real-world context",
        "Without grounding, agents hallucinate patterns and miss implementations",
        "Two types of grounding needed: codebase context and domain knowledge"
      ],
      "speakerNotes": {
        "talkingPoints": "Grounding is foundational. Without it, agents invent inconsistent APIs, miss existing patterns, and generate code that doesn't integrate with your actual system. Think of it like reading documentation and Stack Overflow before starting work in a new domain. Your agent needs the same preparation.",
        "timing": "2 minutes",
        "discussion": "What happens when you start a new project without understanding existing patterns? How long does it take to realize you're heading the wrong direction?",
        "context": "This is where tools like ChunkHound (code research) and ArguSeek (domain research) come in. We'll cover them in detail in Lesson 5, but the principle is simple: give your agent the context it needs before asking it to build.",
        "transition": "Let's look at the specific tools that provide this grounding."
      }
    },
    {
      "type": "concept",
      "title": "Grounding Tools",
      "content": [
        "ChunkHound: Semantic code search answering architectural questions",
        "Retrieves patterns and implementations from your codebase",
        "ArguSeek: Pulls domain knowledge—APIs, frameworks, research papers",
        "No manual tab-switching or context reconstruction needed",
        "Both tools covered in detail in Lesson 5"
      ],
      "speakerNotes": {
        "talkingPoints": "ChunkHound performs deep code research—it answers 'How is authentication handled?' not just keyword matches. ArguSeek pulls from Google, docs, GitHub issues, and research papers directly into your context. Together, they eliminate the most common source of agent hallucination: missing context. The agent knows what patterns exist in your codebase and what the best practices are in your domain.",
        "timing": "2 minutes",
        "discussion": "What questions would you ask ChunkHound about a new codebase? What domain knowledge would you pull with ArguSeek for your current project?",
        "context": "These tools are not nice-to-have. They're foundational. Without them, grounding is incomplete, and your agent will make decisions blind to your actual architecture. With them, grounding is systematic and reproducible.",
        "transition": "With grounding complete, we move to Phase 2: planning the change strategically."
      }
    },
    {
      "type": "visual",
      "title": "The Four-Phase Workflow",
      "component": "WorkflowCircle",
      "caption": "Systematic workflow maintains architectural control across agent delegation.",
      "speakerNotes": {
        "talkingPoints": "This is the core framework for every significant agent interaction. Research provides grounding. Plan defines direction. Execute delegates implementation. Validate ensures quality. Each phase has a distinct purpose. Skip any one, and your failure rate skyrockets. It's not linear—validation often reveals gaps that loop back to research or planning.",
        "timing": "1-2 minutes",
        "discussion": "Which phase do you most want to skip in your work? Why is that tempting? What happens when you actually skip it?",
        "context": "This workflow applies whether you're building a small feature, refactoring a module, or implementing a major architectural change. The scale changes the depth of each phase, but the phases themselves are consistent.",
        "transition": "Phase 2 is where strategy matters. Let's talk about the two different planning approaches."
      }
    },
    {
      "type": "visual",
      "title": "Planning Strategies",
      "component": "PlanningStrategyComparison",
      "caption": "Choose exploration when solution is unclear, exact planning when you know the approach.",
      "speakerNotes": {
        "talkingPoints": "Exploration planning is for discovering the best approach. You frame the problem, guide the agent to research patterns and alternatives, iterate together through reasoning cycles. Higher cost, better solutions, builds your mental model. Exact planning is for when you've already done the architectural thinking—be directive, define specificity, list constraints, specify patterns. Faster, more cost-effective, requires architectural certainty. Your choice depends on whether you know the solution or need to discover it.",
        "timing": "3 minutes",
        "discussion": "For your current project, would you use exploration or exact planning? What information would you need for exact planning? What would exploration cost you in time and tokens?",
        "context": "Most engineers overestimate how much they know about the optimal solution. Exploration planning often finds better approaches than the 'obvious' path. But it's not free—it requires iteration, more tokens, and more time. Use exact planning only when you're genuinely certain about the direction.",
        "transition": "Once your plan is clear, you execute. But execution happens in two very different modes."
      }
    },
    {
      "type": "concept",
      "title": "Phase 3: Execute—Supervised Mode",
      "content": [
        "You actively monitor the agent as it works—each action, each output",
        "Maximum control and precision, catch issues immediately",
        "High cognitive cost: your throughput tanks, you're blocked",
        "Can't context-switch to other tasks during execution",
        "Use when learning, tackling critical security code, or exploring complex problems"
      ],
      "speakerNotes": {
        "talkingPoints": "Supervised mode is your training ground. You watch the agent work, see how it thinks, understand where it struggles, and develop intuition about what works. Yes, you're the bottleneck—but you're also building the mental models and trust that let you graduate to autonomous mode. Think of this as apprenticeship: intensive, high-touch, educational.",
        "timing": "2-3 minutes",
        "discussion": "When would you use supervised mode? Security-critical code, obviously—but where else? When learning a new system? When the requirements are fuzzy?",
        "context": "Many engineers stay in supervised mode too long because it feels safer. But staying here prevents you from reaching the real productivity transformation. The goal is to build skills and trust so you can move to autonomous mode.",
        "transition": "Autonomous mode is where the real productivity gain happens. This is the mode that changes how much you can ship."
      }
    },
    {
      "type": "concept",
      "title": "Phase 3: Execute—Autonomous Mode",
      "content": [
        "Give agent a well-defined task, let it run, check results when done",
        "You're doing other things—different projects, meetings, actual life",
        "Real productivity gain is parallel work, not per-task speed",
        "Depends entirely on excellent grounding and planning",
        "Three agents running simultaneously on different projects scales your impact"
      ],
      "speakerNotes": {
        "talkingPoints": "This is the actual game-changer. Yes, an agent might take 30 minutes on a task you'd hand-code in 20 minutes. But in autonomous mode, you're not blocked—you're cooking dinner. Meanwhile, three other agents are running on three other projects. You're maintaining 8-hour stretches of productive output while only being at your keyboard for 2 hours. This is genuine parallelization in software development for the first time in history. Even if per-task speed is the same, autonomous mode wins on throughput because you ship more simultaneously.",
        "timing": "3-4 minutes",
        "discussion": "How many simultaneous projects do you currently work on? How would your productivity change if you could have three agents running in parallel? What would you do with all that freed-up attention?",
        "context": "The counterintuitive insight: the real 10x isn't speed per task. It's parallel execution and continuous output. A senior engineer running three autonomous agents while attending meetings and cooking dinner ships more code than the same engineer babysitting one agent through a single task.",
        "transition": "Autonomous mode only works if your grounding and planning are solid. Let's move to Phase 4: validating what the agent produced."
      }
    },
    {
      "type": "concept",
      "title": "Phase 4: Validate",
      "content": [
        "LLMs are probabilistic—almost never 100% perfect on first pass",
        "Validation goal: identify what's wrong, then decide next step",
        "Run your code—be the user, test happy path and edge cases",
        "Use the agent itself to review its own work (Lesson 9 covers this)",
        "Automated checks: build, tests, linters provide clear signal"
      ],
      "speakerNotes": {
        "talkingPoints": "Validation isn't perfection verification. It's identifying what needs fixing and making a critical decision: iterate with fixes or regenerate from scratch? Don't get attached to the output—code generation is cheap. Run your implementation, test behavior, check errors. Use automated checks for signal. Then decide: foundation right but gaps exist? Iterate. Fundamentally wrong? Regenerate and fix your context, not the output.",
        "timing": "3 minutes",
        "discussion": "What's your threshold for 'it's good enough to ship'? How do you currently validate code? How would validation change if an agent generated it?",
        "context": "The key principle: it's usually easier to fix your context (the prompt, examples, constraints) than to fix the generated code. Think of yourself as debugging your input, not the output. If something's wrong, the problem often isn't the code—it's that the agent didn't have clear direction.",
        "transition": "How do you decide between iterate and regenerate? Here's the decision framework."
      }
    },
    {
      "type": "concept",
      "title": "Iteration vs. Regeneration",
      "content": [
        "Iterate when: output is aligned but has gaps (edge cases, polish, pattern consistency)",
        "Regenerate when: something fundamental is wrong (wrong architecture, misunderstood requirements)",
        "Fix your context, not the generated code—regenerate with better guidance",
        "Code generation is cheap; don't patch fundamentally broken output",
        "Key principle: debug your input, not the output"
      ],
      "speakerNotes": {
        "talkingPoints": "This is the decision that separates efficient workflows from ones that waste time. A general rule: if the foundation is right but execution is incomplete, iterate. Fix the missing edge cases, add error handling, improve patterns. But if the foundation itself is wrong—the architecture doesn't match your mental model, the approach is fundamentally flawed—don't patch it. Throw it out, fix your context (the prompt, examples, constraints), and regenerate. You'll get better code faster than hand-patching fundamentally broken generation.",
        "timing": "2-3 minutes",
        "discussion": "Think of a piece of generated code you've received. Would you iterate or regenerate? What would you change in the context to get better output next time?",
        "context": "Many engineers underestimate how much iteration is necessary and overestimate the cost of regeneration. It's actually more efficient to fix your context and regenerate than to manually patch poor generation. This flips the debugging paradigm: you're not debugging code—you're debugging instructions.",
        "transition": "This entire workflow ties together with a key insight about what's actually changed."
      }
    },
    {
      "type": "takeaway",
      "title": "Key Takeaways",
      "content": [
        "Shift from craftsman mindset (I write code) to operator mindset (I direct systems)",
        "Four-phase workflow is non-negotiable: Research → Plan → Execute → Validate",
        "Grounding eliminates hallucination; planning ensures correct direction",
        "Autonomous mode is the productivity multiplier—work in parallel, not serially",
        "Quality comes from clear context and systematic validation, not reading every line"
      ],
      "speakerNotes": {
        "talkingPoints": "The workflow isn't linear—it's iterative. Validation reveals gaps that loop back to research or planning. That's expected and healthy. The value isn't executing each phase perfectly the first time. It's having a systematic framework that catches issues before they compound. Everything that follows in this course—prompting techniques, tool integration, self-review patterns, test generation—builds on this foundational workflow.",
        "timing": "2 minutes",
        "discussion": "Which phase feels most important to you? Where would the workflow break down if you skipped it? How will this change how you work?",
        "context": "Remember: this workflow tells you what to do. Lesson 4 (Prompting 101) tells you how to communicate effectively with agents within this framework. Strategy without execution is just planning. These pieces fit together.",
        "transition": "Next lesson: Lesson 4: Prompting 101 - Learn the specific prompt engineering techniques that make this workflow effective."
      }
    }
  ]
}